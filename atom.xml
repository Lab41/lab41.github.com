<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[{ blog }]]></title>
  <link href="https://va-vsrv-github.a.internal/atom.xml" rel="self"/>
  <link href="https://va-vsrv-github.a.internal/"/>
  <updated>2013-09-11T17:06:11-07:00</updated>
  <id>https://va-vsrv-github.a.internal/</id>
  <author>
    <name><![CDATA[Lab41]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Stochastic Kronecker natural graphs]]></title>
    <link href="https://va-vsrv-github.a.internal/blog/2013/08/27/stochastic-kronecker-natural-graphs/"/>
    <updated>2013-08-27T11:44:00-07:00</updated>
    <id>https://va-vsrv-github.a.internal/blog/2013/08/27/stochastic-kronecker-natural-graphs</id>
    <content type="html"><![CDATA[<p>Why are data scientists so obsessed with graphs? It’s because graphs are the best tools we have for modeling the real world. By analyzing the graph representation of a real-world structure, we can glean a variety of insights about it. Graphs that model real-world phenomena are called “natural” graphs, and a great deal of data science focuses on them. However, obtaining natural graphs is hard; it would be nice if we had a way to generate similar-looking graphs without the data-gathering work. Enter the <strong>stochastic Kronecker graph model</strong>: an easy way to generate almost-natural synthetic graphs. This post will give an overview of natural graphs and describe the stochastic Kronecker model of generating graphs.</p>
<div class="container-fluid">
  <div class="row-fluid">
    <div class="span12">
      <h3 style="text-align:center">
Kronecker Visualization
</h3>
    </div>
  </div>
  <div class="row-fluid">
    <div class="span6">
      <div style="font-size: smaller">
        <p style="width:75%">
The visualizations below demonstrate the contents of this post. Use the interactive controls to easily create almost-natural synthetic graphs from just a few key parameters:
</p>
        <div>
          
Node-edge Visual:
<ul>
            <li>
<strong>linkDistance</strong>: relative distance between nodes
</li>
            <li>
<strong>charge</strong>: repulsion force between nodes
</li>
            <li>
<strong>gravity</strong>: attraction force between nodes
</li>
          </ul>
        </div>
        <div>
          
Kronecker Initiation:
<ul>
            <li>
<strong>iterations</strong>: number of generation iterations
</li>
            <li>
<strong>a1…b2</strong>: probability values for the 2x2 initiator matrix
</li>
          </ul>
        </div>
      </div>
    </div>
    <div class="span6">
      <div id="controller" style="font-size:smaller">
          <div class="row-fluid"> <div>
<strong>Number of iterations:</strong>
<div style="display:inline" id="iterationsvalue"></div></div></div>
          <div class="row-fluid"> <div id="iterationsslider"></div> </div>
          <div class="row-fluid" style="margin-top:5px; margin-bottom:2px"> <div>
<strong>Probabilities in 2x2 initiator matrix:</strong>
</div> </div>
          <div class="row-fluid">
             <div class="span4" style="padding-top:12px"> <div id="a1slider"></div> </div> <div class="span2 matrixval"> <div id="a1value"></div> </div>
             <div style="margin:0" class="span2 matrixval"> <div id="a2value"></div> </div> <div class="span4" style="padding-top:12px"> <div id="a2slider"></div> </div>
          </div>
          <div class="row-fluid">
             <div class="span4" style="padding-top:12px"> <div id="b1slider"></div> </div> <div class="span2 matrixval"> <div id="b1value"></div> </div>
             <div style="margin:0" class="span2 matrixval"> <div id="b2value"></div> </div> <div class="span4" style="padding-top:12px"> <div id="b2slider"></div> </div>
          </div>
          <div class="row-fluid">
            <div style="text-align:center; margin-top:10px" class="span12"> <div id="regenerate"></div> </div>
          </div>
      </div>
    </div>
  </div>


<div class="row-fluid">
    <div class="span6">
      <div id="graph"></div>
    </div>
    <div class="span6">
      <div id="matrix"></div>
    </div>
  </div>
</div>

<h1 id="introduction-to-natural-graphs">Introduction to natural graphs</h1>
<h2 id="background-and-motivation">Background and motivation</h2>
<p>At Lab41, <a href="http://lab41.github.io/blog/2013/06/12/i-see-graphs/">we see graphs everywhere</a>. Much of our work revolves around analyzing and generating natural graphs that have structural properties similar to those found in real-world settings. Such graphs could represent an arrangment of computers in a network, animals in a food chain, or neurons in your brain. Unlike randomly-generated graphs, natural graphs have <em>meaning</em>. For example, characteristics of a system modeled by a graph can be deduced by calculating mathematical metrics such as its nodes’ <em>degree</em> (the number of edges connected to a node in a graph) or the number of triangles formed by its edges.</p>
<p>Working with natural graphs involves a number of challenges:</p>
<ul>
<li><p><strong>Obtaining natural graphs is hard.</strong> One must painstakingly collect a large dataset of real-world observations and connections, find a suitable way to interpret it as a graph, and then actually convert it into a graph - a process that can be tedious and time-consuming.</p></li>
<li><p><strong>Datasets for natural graphs are scarce.</strong> There are only a small number of existing datasets representing natural graphs. In fact, at the recent <a href="http://graphlab.org/graphlab-workshop-2013/">GraphLab workshop</a>, one speaker noted that he was getting tired of every presenter using the same dataset (articles and links between them on <a href="http://www.wikipedia.org/">Wikipedia</a>) for their analyses!</p></li>
<li><p><strong>Synthetic graphs miss the mark.</strong> Graphs randomly generated according to standard models (as my colleague Charlie did in <a href="blog/2013/05/02/zero-to-large/">his previous post</a>, and others have done using the <a href="http://en.wikipedia.org/wiki/Erdos-Renyi_model">Erdos-Renyi graph model</a>) tend to look <em>unnatural</em>, no matter what parameters we use. We can’t just create natural graphs by taking a random number generator and going crazy. Instead, we need to find out what properties make a graph “natural,” and then find a way to effectively and efficiently generate graphs with those properties.</p></li>
</ul>
<h2 id="properties-of-natural-graphs">Properties of natural graphs</h2>
<p>So, what makes a graph “natural”? While there is no hard-and-fast definition, nearly all natural graphs exhibit two simple properties:</p>
<ul>
<li><p><strong>Power-law degree distributions.</strong> A very small number of nodes have a very large number of connections (high degree), while a large number of nodes have a very small number of connections (low degree). Mathematically speaking, this means the degree of any vertex in the graph can be interpreted as a random variable that follows a <a href="http://en.wikipedia.org/wiki/Power-law_distribution">power-law probability distribution.</a></p></li>
<li><p><strong>Self-similarity.</strong> In natural graphs, the large-scale connections between parts of the graph reflect the small-scale connections within these different parts. Such a property also appears within fractals, such as the <a href="http://en.wikipedia.org/wiki/Mandelbrot_set">Mandelbrot</a> or <a href="http://en.wikipedia.org/wiki/Julia_set">Julia</a> sets.</p></li>
</ul>
<p>An accurate mechanism for natural graph generation must preserve these properties. As it turns out, the <a href="http://arxiv.org/abs/0812.4905"><strong>stochastic Kronecker graph model</strong></a> does this. It has a few other advantages as well:</p>
<ul>
<li><p><strong>Parallelism.</strong> The model allows large graphs to be generated at scale via parallel computation.</p></li>
<li><p><strong>Structural summarization.</strong> The model provides a very succinct, yet accurate, way to “summarize” the structural properties of natural graphs. Two Kronecker graphs generated with the same parameters will produce graphs with matching values for common structural metrics, such as degree distribution, diameter, hop number, scree value, and network value.</p></li>
</ul>
<p>The remainder of this blog post will describe the basic Kronecker generation algorithm and how it can be modified to efficiently generate very large graphs via parallel computation, on top of MapReduce and Hadoop.</p>
<h1 id="mathematical-formulation">Mathematical formulation</h1>
<p>The core of the Kronecker generation model is a simple matrix operation called the <em>Kronecker product</em>, an operation on two matrices that “nests” many copies of the second within the first. Since graphs can be represented by adjacency matrices (<a href="http://lab41.github.io/blog/2013/06/12/i-see-graphs/">Karthik’s post</a>), this operation can be generalized to graphs.</p>
<p>Taking the Kronecker product of a graph with itself thus easily produces a new, self-similar graph, as does taking the more general “Kronecker power” of it. In fact, Kronecker powers will have further self-similarity. For example, below you can see an example of a simple three-node graph, its Kronecker cube, and its Kronecker fourth power, with the self-similarity evident in the adjacency matrix.</p>
<p><span class='caption-wrapper'><img class='caption' src='https://va-vsrv-github.a.internal/images/2013-08-27-stochastic-kronecker-natural-graphs/matrix.jpg' width='' height='' title='Fractal patterns visible in the adjecency matrix of a Kronecker graph. Taken from Leskovec et al. (2008).'><span class='caption-text'>Fractal patterns visible in the adjecency matrix of a Kronecker graph. Taken from Leskovec et al. (2008).</span></span></p>
<p>Because the Kronecker power so easily generates self-similar graphs, it’s reasonable to consider that it might be similarly effective at generating <em>random</em> natural graphs. To do this, we simply start with an adjacency matrix, but allow <em>probabilities</em> to occupy the cells of the matrix rather than ones and zeros. This gives us the <em>stochastic</em> Kronecker graph model.</p>
<h1 id="algorithms-for-generating-kronecker-graphs">Algorithms for generating Kronecker graphs</h1>
<h2 id="naive-algorithm">Naive algorithm</h2>
<p>The simplest algorithm for generating Kronecker graphs is to use Kronecker powers to generate a stochastic adjacency matrix, and then step through each cell of the matrix, flipping a coin biased by the probability present in that matrix. In more detail, the algorithm is as follows:</p>
<ol type="1">
<li><p>We start with an <span class="math">\(n\)</span> by <span class="math">\(n\)</span> initiator matrix, <span class="math">\(\theta,\)</span> and the number of iterations <span class="math">\(k\)</span> for which we wish to run the algorithm. We compute the <span class="math">\(k\)</span>-th Kronecker power of the matrix <span class="math">\(\theta,\)</span> giving us a large matrix of probabilities, which we call <span class="math">\(P.\)</span> Each cell in this matrix corresponds to an edge between two nodes in the graph; the formula for the value at the <span class="math">\((u,v)\)</span>th cell of <span class="math">\(P\)</span> is: <span class="math">\[\prod_{i=0}^{k-1} \theta\left[\left\lfloor \frac{u}{n^i}\right\rfloor \bmod{n},
    \left\lfloor \frac{v}{n^i}\right\rfloor \bmod{n} \right].\]</span> (For convenience, we have assumed the matrix is zero-indexed, as is common in computer science.)</p></li>
<li><p>To generate the actual graph, we 1) step through each cell in the matrix, 2) take the probability in the cell, 3) flip a coin biased by that probability, and if the coin “comes up heads,” we 4) place the corresponding edge in the graph.</p></li>
</ol>
<p>If the initiator matrix is an <span class="math">\(n\times n\)</span> square matrix, and we perform <span class="math">\(k\)</span> iterations of the Kronecker power operation, the generated matrix will have dimension <span class="math">\(N=n^k.\)</span> We will need to take a product of <span class="math">\(k\)</span> values to obtain each cell of the final matrix, and there will be <span class="math">\(N^2\)</span> cells, so the runtime of this algorithm will be <span class="math">\(O(kN^2).\)</span></p>
<p>This means that if we want to generate a graph with approximately one billion nodes (a reasonable size for a large natural graph) from an initiator matrix of size 2, our runtime expression tells us we should expect to perform approximately <span class="math">\({(30)(10^9)^2 = 3.0\times 10^{19}}\)</span> operations. That’s 30 <em>quintillion</em> operations. This leads us to wonder whether we could do this with fewer operations. Spoiler alert: it’s possible.</p>
<h2 id="fast-algorithm">Fast algorithm</h2>
<p>If we switch from a node-oriented approach to an edge-oriented approach, there does exist a faster algorithm for generating a Kronecker graph. Most natural graphs are sparse - <span class="math">\(E = O(N).\)</span> Thus, if we can find a way to place each <em>edge</em>, one at a time, in the graph, rather than figuring out if a pair of nodes has an edge between them, we can vastly reduce the on-average running time. To do this, we need to figure out how many edges are in the graph, and we need to figure out which nodes are associated with each edge.</p>
<p>It turns out that the expected number of edges in a stochastically generated Kronecker graph is encoded within the initiator matrix itself - it’s given by: <span class="math">\[E = \left(\sum_{i,j} \theta[i,j]\right)^k.\]</span> In general, this works out to being on the order of the number of nodes.</p>
<p>Next, we need to find a procedure that starts from nothing, and in <span class="math">\(k\)</span> iterations picks a new edge in the graph to add. Thankfully, this operation is already staring us in the face - in the formula presented in the previous section. Here it is again: <span class="math">\[\prod_{i=0}^{k-1} \theta\left[\left\lfloor \frac{u}{n^i}\right\rfloor \bmod{n},
\left\lfloor \frac{v}{n^i}\right\rfloor \bmod{n} \right].\]</span> This formula can be understood in a different way - as a “recursive descent” into the adjacency matrix of the graph, picking smaller and smaller blocks of the matrix until we have finally narrowed our choice to a single cell, which we then “color in” to represent that an edge should be placed there.</p>
<p>Thus, to generate a stochastic Kronecker graph, all we need to do is set up a loop which runs <span class="math">\(E\)</span> times, generating a new edge in the graph on each pass-through. (If we generate the same edge twice, we ignore it and repeat the pass-through as if nothing happened.) This runs in <span class="math">\(O(kE)\)</span> time, which means that for sparse, real-world graphs, it runs in <span class="math">\(O(kN)\)</span>.</p>
<h2 id="parallel-algorithm">Parallel algorithm</h2>
<p>This algorithm allows us to generate every edge in the graph independently of every other edge, allowing us to parallelize the graph’s generation. This means we can leverage the power of Hadoop to generate very large graphs.</p>
<p>The only twist is that this method allows for the creation of duplicate edges, and most of the graphs we’re interested in don’t contain such duplicates. Thus, we need to figure out how to identify and eliminate them. This is hard when generating the graph across multiple machines, because it’s very likely the duplicate edges will be generated on separate machines. Fortunately, with a bit of cleverness, we can leverage the nature of MapReduce to do our duplicate checking. Instead of one MapReduce job, we’ll have three - one to generate edges and eliminate duplicates, one to generate vertices, and one to combine the two together to form a single graph. This gives us the workflow below.</p>
<p><span class='caption-wrapper'><img class='caption' src='https://va-vsrv-github.a.internal/images/2013-08-27-stochastic-kronecker-natural-graphs/workflow.png' width='' height='' title='The workflow for Kronecker graph generation. Datatypes appear above the line, sample data below. For convenience, FaunusVertex objects have been represented in JSON and NodeTuple objects by pairs of values between angle brackets.'><span class='caption-text'>The workflow for Kronecker graph generation. Datatypes appear above the line, sample data below. For convenience, FaunusVertex objects have been represented in JSON and NodeTuple objects by pairs of values between angle brackets.</span></span></p>
<p>The pipeline consists of three stages:</p>
<ol type="1">
<li><p>The first stage of our pipeline is vertex generation. This is the simplest stage - it is a map-only job, utilizing a custom input format representing a range of vertices to be generated. We use as the key a unique <code>Long</code> identifying the vertex, and a <code>FaunusVertex</code> object as the value, giving us a (Long,FaunusVertex`) output sequence file.</p></li>
<li><p>The second stage of our pipeline is edge generation. As with vertex generation, it uses a custom input format representing a quota of edges to place into the graph. For each edge in this quota, we run the fast stochastic Kronecker placement algorithm, yielding a tuple of vertex IDs that represents a directed edge in the graph. This tuple is stored as a custom intermediate key type (called a <code>NodeTuple</code>), with the value as a <code>NullWritable</code>; this allows the shuffling and sorting logic of MapReduce to place identical tuples together, and consequently allows us to easily eliminate duplicate copies of the directed edges before the reduce step. Finally, in our reduce step, we emit a (Long,FaunusVertex<code>) tuple.     The</code>FaunusVertex<code>represents the edge’s source vertex and contains     a</code>FaunusEdge<code>indicating its destination vertex. The</code>Long` key is the source vertex’s ID.</p></li>
<li><p>The third and final stage of our pipeline reads in the vertex objects generated by both the edge and vertex creators and combines them, creating a final list of <code>FaunusVertexes</code> that represents the graph.</p></li>
</ol>
<p>A few details on the pipeline:</p>
<ul>
<li><p><strong>Faunus.</strong> This pipeline uses the same data types as the <a href="http://faunus.thinkaurelius.com">Faunus</a> engine for graph analytics. Faunus provides objects representing edges (<code>FaunusEdge</code>s) and vertices (<code>FaunusVertex</code>es) that can be serialized and utilized by MapReduce jobs but can also serve as a final representation of a graph. Conveniently, <code>FaunusVertex</code>es can store the edges coming off them as well, so we do not need to store edges separately from vertices in the final graph - we need only store the list of vertices with edges added to them.</p></li>
<li><p><strong><code>SequenceFiles</code>.</strong> This pipeline produces <code>SequenceFiles</code> (a native MapReduce serialization format) consisting of <code>FaunusVertex</code>es to serve as intermediate representations of the graph as we construct it.</p></li>
<li><p><strong>Annotations.</strong> In the final stage, we annotate the vertices with several property values (a mixture of floating-points and strings) in order to mimic the data we are interested in.</p></li>
</ul>
<h1 id="references">References</h1>
<ul>
<li>Leskovec, Jure, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani. Kronecker graphs: an approach to modeling networks. <em>ArXiv</em>, <a href="http://arxiv.org/abs/0812.4905">arXiv:0812.4905v2</a></li>
</ul>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I See Graphs]]></title>
    <link href="https://va-vsrv-github.a.internal/blog/2013/06/12/i-see-graphs/"/>
    <updated>2013-06-12T15:17:00-07:00</updated>
    <id>https://va-vsrv-github.a.internal/blog/2013/06/12/i-see-graphs</id>
    <content type="html"><![CDATA[<p>At Lab41 we are obsessed with <a href="http://en.wikipedia.org/wiki/Graph_(mathematics)">graphs</a>. We see graphs everywhere we look, in everything we think about. One of our goals is to better understand and advance techniques for manipulating, storing, and analyzing graphs. In this post, we try and do three things: (1) explain what a graph is, (2) show that it’s an important concept, and (3) discuss a way of working with graphs. More specifically, we talk about how to work with graphs as matrices.</p>
<h3 id="why-are-graphs-important">Why are graphs important?</h3>
<p><img class="right" src="https://va-vsrv-github.a.internal/images/2013-06-12-i-see-graphs/directed_graph.png"> A graph is a mathematical construct that describes things that are linked together. Graphs model networks-systems of “things” connected together in some way. For example, the Internet is a physical network of computers connected together by data connections; the Web is a logical network of web pages connected by hyperlinks; and human societies are networks of people connected together by various social relationships. In the language of mathematics, we say each of the “things” in a network is a node and they are connected by “edges.”</p>
<p>It turns out that you can think of much of the world, both physical and virtual, as a graph. As a mathematical construct, graphs have been around since <a
href="http://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg">Leonhard Euler tried to figure out the best way to get around Konigsberg in 1735</a>. Since then, graph theory has been embraced by a wide array of disciplines including sociology, economics, physics, computer science, biology, and statistics. An excellent resource for understanding how graphs map onto real world systems is the <a href="http://www.santafe.edu/media/workingpapers/90-004.pdf">“Rosetta Stone for Connectionism,”</a> which maps various real world systems onto graph concepts. Graphs really are everywhere.</p>
<p>While graphs are prevalent in many fields, the tools for working with graphs, especially large graphs, are still in their infancy. As graph technologies mature it should become easier to model many different problems, and easier to implement solutions. However, we are still figuring out the best ways to store, query, and compute on graphs. Right now, people use different data structures and technologies for different types of graphs and different use cases. Eventually, we need to figure out how to hide that complexity and let people treat graph data as graphs without thinking about what the right tools are for manipulating that data. Marko Rodriguez, a leading graph technologist, has a great summary of several different types of graphs and graph technologies in his <a href="http://markorodriguez.com/2013/01/09/on-graph-computing/">recent blog post</a>. Lab41 is actively using and working with many of the technologies that Marko describes, including the graph database Titan, which we load tested as noted in <a href="https://github.com/tinkerpop/blueprints/wiki/Property-Graph-Model">our previous blog entry</a>.</p>
<p><img class="center" src="https://va-vsrv-github.a.internal/images/2013-06-12-i-see-graphs/java_graph.png" title="Graph of Java standard class library generated with Gephi 0.8.2" ></p>
<h3 id="graphs-as-matrices">Graphs as Matrices</h3>
<p>The earliest tools for working with graphs were tools for manipulating matrices. In mathematics, graphs are frequently expressed as an <a href="http://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a>. In an adjacency matrix each row/column represents a node, and each entry in the matrix represents the presence of an edge between two nodes. The cool thing about the matrix form of a graph is that once you think of a graph as a matrix, you can apply concepts and methods from linear algebra to your graph analysis. Many common graph metrics and algorithms can easily be expressed in terms of standard matrix operations.</p>
<p><img class="center" src="https://va-vsrv-github.a.internal/images/2013-06-12-i-see-graphs/matrix.png" title="A graph represented as an ajacency matrix" ></p>
<p>The cool thing about the matrix form of a graph is that once you think of a graph as a matrix, you can apply concepts and methods from linear algebra to your graph analysis. Many common graph metrics and algorithms can easily be expressed in terms of standard matrix operations.</p>
<p>While an adjacency matrix is a mathematical abstraction, it’s also a data structure. In this post we are talking primarily about a matrix as a contiguous block of memory. In most programing languages this is an array of arrays:</p>
<div class='bogus-wrapper'>
<notextile>
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//Java
</span><span class='line'>int[][] matrix = {{1,2,3}, {1,2,3}}</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>


<p>From an engineering perspective, there are a number of advantages to storing a graph as a matrix, if the matrix representation of the graph fits in memory:</p>
<ul>
<li>A number of common operations can be performed on matrices quite quickly in comparison to how long they would take on other data structures. (Table 1)
<div class="table-lab41">
    <table>
      <tr>
        <td> 
Operation
</td>
        <td> 
Adjacency Matrix
</td>
        <td> 
Adjacency List
</td>
        <td> 
Adjacency Tree
</td>
      </tr>
      <tr>
        <td> 
Insert
</td>
        <td> 
O(1)
</td>
        <td> 
O(1)
</td>
        <td> 
O(log(m/n))
</td>
      </tr>
      <tr>
        <td>
Delete
</td>
        <td>
O(1)
</td>
        <td>
O(m/n)
</td>
        <td>
O(log(m/n))
</td>
      </tr>
      <tr>
        <td>
Find
</td>
        <td>
O(1)
</td>
        <td>
O(m/n)
</td>
        <td>
O(log(m/n))
</td>
      </tr>
      <tr>
        <td>
Enumerate
</td>
        <td>
O(n)
</td>
        <td>
O(m/n)
</td>
        <td>
O(m/n)
</td>
      </tr>
    </table>
  </div>
</li>
<li>Nearly all programming languages have highly optimized libraries for storing and working with matrices. For example: Python- <a href="http://www.numpy.org/">NumPy</a>; Java-<a href="http://math.nist.gov/javanumerics/jama/">JAMA</a>, <a href="http://acs.lbl.gov/software/colt/">Colt</a>,<a href="https://code.google.com/p/java-matrix-benchmark/">etc..</a>; C++: <a href="http://arma.sourceforge.net/">Armadillo</a>, <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a>, etc.</li>
</ul>
However, matrices are a relatively limited representation of a graph. There are a number of operations that they perform very poorly, and they can be very memory inefficient:
<ul>
<li>
Adjacency matrices only allow you to capture the structure of the graph. They don’t give you a chance to associate information with each node – in other words, you can’t represent <a href="https://github.com/tinkerpop/blueprints/wiki/Property-Graph-Model">property graphs</a> with matrices. For example, if you were modeling the world wide web you might want to store the url, title, and text of each web page in the network, and you can’t do that with an adjacency matrix. Now, if you’re persnickety, you might argue that it’s possible to store edge properties in an adjacency matrix if you think of each entry in the matrix as something other than a numeric value.
</li>

<li> 
You can’t model heterogeneous graphs consisting of different types of entities using an adjacency matrix. For example, if you are trying to implement collaborative filtering, you may want to model a network of users to products. Again, if you are persnickety, you might propose various ways of simulating heterogeneous graphs using a single matrix. However, all of those methods will require you to use information not actually stored in the matrix to differentiate nodes of one type from nodes of another.
</li>

<li> 
Adjacency matrices are really slow at some critical matrix operations, such as running through the neighbors of a particular node on a sparse matrix (O(n)– where n is the number of nodes in the graph).
</li>

<li> <p>
Perhaps worst of all, adjacency matrices are very memory inefficient taking O(n^2) memory. An adjacency matrix has an entry for each possible edge, which means each possible edge is using memory even if it does not exist. This is primarily a problem when working with sparse networks – networks where many of the edges in the network don’t exist. Unfortunately, most real world networks are sparse. For example, if you consider the graph of all people on earth, it is a sparse network because each person knows only a relatively small number of other people. Most of the possible relationships that could exist between people don’t exist. In some ways that is both an engineering problem and an existential problem.
</p>
<p>
To put the memory inefficiency of adjacency matrices into perspective, if each edge in a matrix is stored as a 32 bit integer then the memory requirement for a graph can be calculated by following equation:
</p>
<span class="math">\[\text{memory in gb} = \frac{(\text{number of nodes})^2 * 8}{(1024^3)}\]</span>
<p>
Thus a graph of 50,000 nodes would take about 10GB of memory, which means you can’t store and manipulate large graphs like the graph of the Web, which is estimated to have 4.7 billion nodes, on most desktop computers.
</p>

</li>

<p>While technologies for dealing with small matrices – matrices that can fit in memory – are well developed, technologies for dealing with large matrices in a distributed manner are just emerging. One approach to dealing with extremely large matrices is to use some type of super computer, which has a lot of memory. Another approach is to swap portions of the matrix into and out of memory; there are algorithms that can do this relatively efficiently based on the unique properties of a matrix. A new, and extremely interesting, approach is to distribute a matrix computation across the memory of multiple computers. I think the <a href="http://www.cs.cmu.edu/~pegasus/">Pegasus project</a> is a particularly interesting example of the distributed matrix computation approach.</p>
<h3 id="next-steps">Next Steps</h3>
<p>If you’re interested in learning more about networks and adjacency matrices, I would highly recommend taking a look at M.E.J. Newman’s <a href="http://www.amazon.com/Networks-An-Introduction-Mark-Newman/dp/0199206651/ref=sr_1_1_bnp_1_har?ie=UTF8&amp;qid=1370024687&amp;sr=8-1&amp;keywords=Newman+Networks">Networks</a>. He has an excellent discussion of the adjacency matrix as a mathematical concept in Chapter 6, and discussion of an adjacency matrix as a data structure in Chapter 9.</p>
<p>Also, keep an eye on this blog. I plan to address other data structures for storing graph data, and when they may (or may not be) appropriate in a future post.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zero to Large]]></title>
    <link href="https://va-vsrv-github.a.internal/blog/2013/05/02/zero-to-large/"/>
    <updated>2013-05-02T15:22:00-07:00</updated>
    <id>https://va-vsrv-github.a.internal/blog/2013/05/02/zero-to-large</id>
    <content type="html"><![CDATA[<blockquote>
<p>This first post is intended to introduce the type of work we’ve started at Lab41, which is a unique partnership In-Q-Tel has started with Academia, Industry, and the U.S. Intelligence Community. We’re excited about this venture and look forward to sharing our progress towards collaboratively addressing big data challenges with new technologies.</p>
</blockquote>
<p>Designing scalable systems for the real world requires careful consideration of data – namely, Big Data’s volume, variety, and velocity – to ensure the right pieces are engineered and valuable resources don’t miss the gotchas or edge cases that lead to insight. Basically, when tinkering with different architectures in the Big Data arena, having good data to test against is paramount. One of our projects involves assessing various architectures for working with large-scale graphs, including how to incorporate data that tests the limits of storage, computation, and analytic workflows.</p>
<p>As you might expect, we wanted to use real world data when designing our real-world system. However, getting real data that mimics production data is difficult and time consuming. Oftentimes data only tells a story for that specific dataset, leading a developer to miss the more comprehensive view of the system’s strengths and weaknesses. For those reasons, we developed a method that generates large graphs with the “right” qualities of a system that can scale to one billion nodes.</p>
<h3 id="initial-requirements">Initial Requirements</h3>
<p>Before comparing the leading projects for scaling graphs, we needed a good baseline for assessing the data requirements of the overall system. It quickly became apparent that current offerings such as <a href="http://gephi.org/">Gephi</a>, <a href="http://nwb.cns.iu.edu/">Network WorkBench</a>, <a href="http://networkx.github.com/">NetworkX</a>, and <a href="https://github.com/tinkerpop/furnace">Furnace</a>, all do a good job of following particular distributions and structural constraints. However, most of them are unable to generate graphs at large scale <strong>and</strong> produce the correct format <strong>and</strong> build to completion <strong>and</strong> finish in a reasonable amount of time.</p>
<p>The evaluation and assessment of graph data generators led us down the path of writing a fairly-straightforward script. The <a href="http://lab41.github.io/graph-generators/">code</a> is very young in its development – and has room for a lot of improvement – but it has proven simple to use, moderately good at generating graphs large enough to test claims, and flexible enough to vary characteristics such as directed-ness, out-degrees, and of course numbers of edges, nodes, and attributes. We made sure to add a twist of randomness to avoid creating identical graphs.</p>
<p>The script takes command-line switches to configure the following graph characteristics:</p>
<ul>
<li>Number of nodes</li>
<li>Degree of nodes</li>
<li>In-degree of nodes</li>
<li>Out-degree of nodes</li>
<li>Number of node attributes</li>
<li>Number of edge attributes</li>
<li>Directed-ness</li>
<li>Output type (GraphML and GraphSON so far)</li>
</ul>
<h4 id="practical-considerations">Practical Considerations</h4>
<p>With our script in hand, we moved on to begin the requisite performance testing, but we first discovered an important consideration for anyone wishing to release our script into the wild.</p>
<p><strong>The most important “practical” consideration proved to be enforcement (or not) of strict parameters, which forces the script to scan and verify characteristics of all nodes.</strong> By enforcing strict parameters, we mean that:</p>
<ul>
<li>each node must guarantee
<ul>
<li>that it has no more and no less edges attached to it
<ul>
<li>from both an in and out degree context.</li>
</ul></li>
</ul></li>
</ul>
<p>In order to guarantee this 100% of the time, each time an edge is added, all preexisting edges must be checked to make sure that the chosen random vertices chosen do not go outside the imposed limits. To put it in perspective, the script initially enforces strict parameters, which – as you can probably guess by now – simply become untenable for quickly producing large graph data sets. As the below chart shows, we are able to generate a graph of 100 Million nodes in roughly the same amount of time it took to generate a graph of only 100,000 nodes using strict parameter enforcement:</p>
<p>Click to enlarge: <a class="fancybox-effects-a"  href=/images/post_1/image_1.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_1.png" title="Comparing Graph Generation Time With and Without Strict Parameters" ></a></p>
<p>Since disabling strict enforcement led to a graph three orders of magnitude larger in the same amount of time, you might be asking how the absence of that check – the degree of edges/node – affected the number of edges. Below we show that the difference of edges between checking and not checking is negligible:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_2.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_2.png" title="Difference of \# of Edges Created" ></a></p>
<p>Since we are generating these graphs, it seems reasonable to bend the requirements slightly to treat the minima and maxima simply as guidelines that some nodes may not conform to. While there is still room for improvement, such as leveraging more than a single CPU core, the results are reasonable enough to use.</p>
<h3 id="generating-the-baseline">Generating the Baseline</h3>
<p><strong>The most important point is that seemingly “simple” parameter changes – which represent actual differences in real-world networks – make huge differences to the resulting network and therefore our system design.</strong> We generated three different classes of graphs from a baseline of graph data sets to determine how varying parameters influences such important characteristics as: time to generate, number of edges created, storage footprint, number of node and edge attributes, and average degree of nodes.</p>
<p>Each graph was generated with an increasing value of nodes, while all other settings were static between generations, per graph type. Graph types A, B, and C – described below – will be used in the next couple of charts:</p>
<pre><code>Graph Type: A B C
Magnitude: 1K - 1B 1K - 100M 1K - 100K
Format: graphml* graphml* graphml*
Directed: No No No
Minimum Degree: 1 1 1
Maximum Degree: 10 10 Same number as nodes
Minimum Node Attributes: 2 50 2
Maximum Node Attributes: 2 100 2
Minimum Edge Attributes: 0 5 0
Maximum Edge Attributes: 0 25 0</code></pre>
<blockquote>
<p>GraphML (<a href="http://graphml.graphdrawing.org/">http://graphml.graphdrawing.org/</a>) is a convenient XML format that describes nodes in terms of names and types with labeled edges between nodes.</p>
</blockquote>
<h4 id="number-of-nodes">Number of Nodes</h4>
<p>The first chart illustrates how the number of nodes greatly influences all other characteristics. While Type A generated one billion nodes in approximately 24 hours, the same timeframe yielded graphs of Type B with only 10 Million nodes and Type C with a scant one million nodes:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_3.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_3.png" title="Time to generate graphs with different characteristics" ></a></p>
<h4 id="edges">Edges</h4>
<p>As this is just a first cut, restricting the number of nodes on the graph types seems acceptable for now. The following illustrates that limiting Type C to only 100,000 nodes still produces almost the same number of edges as a one billion node version of graph A:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_4.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_4.png" title="Edges created generating graphs with different characteristics" ></a></p>
<h4 id="storage-footprint">Storage Footprint</h4>
<p>The following chart shows that a one billion node graph of Type B would require approximately 10TB of storage space, while Type C would require 300 Petabytes (!) to reach one billion nodes:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_5.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_5.png" title="Storage footprint of generating graphs with different characteristics" ></a></p>
<p>Sadly, I couldn’t justify buying 300 <a href="http://www.aberdeeninc.com/abcatg/petarack.htm">Petaracks</a> just to generate the world’s most unrealistic graph. Not to mention it would have taken approximately 20,000 <strong>years</strong> to generate, but that’s beside the point.</p>
<h4 id="attributes">Attributes</h4>
<p>When looking at attribute differences, Type B creates about 30-40 times more attributes than the fairly-similar Types A and C:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_6.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_6.png" title="Node attributes created generating graphs with different characteristics" ></a></p>
<h4 id="node-degree">Node Degree</h4>
<p>Finally, this last chart shows how the degree of each node for Type C grows exponentially with the number of nodes, whereas the average degree for the other two graph types remain static:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_7.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_7.png" title="Average degree of nodes generating graphs with different characteristics" ></a></p>
<h3 id="loading-the-data">Loading the Data</h3>
<p>Now that we generated our various graph datasets, we need to load them into a distributed graph data store. For a variety of reasons, we decided to use <a href="http://thinkaurelius.github.com/titan/">Titan</a> with an <a href="http://hbase.apache.org/">HBase</a> backend.</p>
<p>One of the nice things about Titan is that its <a href="https://github.com/tinkerpop/gremlin/wiki">Gremlin console shell</a> enables graph interaction, traversals, and calculations. It also has functions for loading a graph file into the graph data store, which in this case is HBase on top of HDFS. Unfortunately, Gremlin through Titan does not leverage the <a href="http://www.kchodorow.com/blog/2010/03/15/mapreduce-the-fanfiction/">awesomeness of MapReduce</a> that generally goes hand-in-hand with HBase and its Hadoop counterparts. So running the import in parallel is currently impossible. In terms of data formats, Gremlin on Titan can load GraphML; however, the current ID scheme prevents federation of GraphML across multiple machines (or even multiple cores). So as you can see from the chart below, the load times are un-spectacular:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_8.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_8.png" title="Load time of graph types using Gremlin with Titan" ></a></p>
<blockquote>
<p>*Note: ‘Minimal Degree and Attributes’ corresponds to ‘Graph A’ from above, similarly ‘Heavy Attributes’ and ‘Heavy Degree’ correspond to ‘Graph B’ and ‘Graph C’, respectively.</p>
</blockquote>
<p>Fret not! <a href="http://thinkaurelius.github.com/faunus/">Faunus</a> to the rescue! Faunus uses a Gremlin shell, which is similar to Titan’s and one we can use for importing a slightly different data format to gain benefits via MapReduce. The next chart shows the benefits of moving from GraphML to loading the GraphSON* format using a MapReduce job:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_9.png><img src="https://va-vsrv-github.a.internal/images/post_1/image_9.png" title="Load time comparison using Gremlin with Titan versus Gremlin with Faunus" ></a></p>
<blockquote>
<p>*Note: <a href="https://github.com/thinkaurelius/faunus/wiki/GraphSON-Format">GraphSON</a>, which is slightly modified from <a href="https://github.com/tinkerpop/blueprints/wiki/GraphSON-Reader-and-Writer-Library">traditional GraphSON</a>, is a one-record-per-line JSON-style format that describes nodes in terms of names, types, and labeled edges.</p>
</blockquote>
<h3 id="conclusion">Conclusion</h3>
<p><strong>Our <a href="https://github.com/Lab41/graph-generators">Graph-generation Code</a> allows us to generate one-billion node graphs with varying characteristics such as directed-ness, number of edges and nodes, and node degrees. Just as important, we determined how the different characteristics affect real-world considerations such as loading time and storage footprint, also finding an early optimization through MapReduce parallel processing.</strong> As we move to the next phase of designing around the data, we anticipate shortly being able to improve at least a couple orders of magnitude through fairly straightforward tweaks such as parallelizing load computations across a cluster. Of course this is only the first step, and there is a long exciting road ahead of us.</p>
<h4 id="standard-disclaimer">Standard Disclaimer</h4>
<p>As is usually the case, it should be noted that this is not necessarily representative of the technology’s overall performance characteristics, but rather our experience within a specific environment. We used the following environment for our tests:</p>
<p><strong>Environment:</strong></p>
<pre><code>GRAPH GENERATION:

    Run on a MacBook Air 2GHz Intel Core i7, 8GB 1600 MHz DDR3

    Python 2.7.2 (default, Jun 20 2012, 16:23:33)
    [GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin

LOADING GRAPHS:

    Gremlin with Titan:
        Virtual Machine, Ubuntu 12.04 LTS, 4 core processor, 8GB RAM
        Started with the default Java options for Gremlin of:
            JAVA_OPTIONS=&quot;-Xms32m -Xmx512m
        Then bumped that up as the graph file got larger to:
            JAVA_OPTIONS=&quot;-Xms256m -Xmx4096m

    Gremlin with Faunus:
        Virtual Machine, Ubuntu 12.04 LTS, 2 core processor, 4GB RAM

    Hadoop/HBase Cluster:
        12 node cluster - 8 core processors, 64GB RAM, CDH4.2, heap set to 4GB, HBase 0.94.2</code></pre>]]></content>
  </entry>
  
</feed>
