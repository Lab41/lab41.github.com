<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[{ lab41 }]]></title>
  <link href="http://lab41.github.io/atom.xml" rel="self"/>
  <link href="http://lab41.github.io/"/>
  <updated>2015-08-11T20:08:53+00:00</updated>
  <id>http://lab41.github.io/</id>
  <author>
    <name><![CDATA[Lab41]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Triplewide Trailer, Part 1]]></title>
    <link href="http://lab41.github.io/blog/2015/07/28/triplewide-trailer-looking-to-rig-ipython/"/>
    <updated>2015-07-28T12:00:00+00:00</updated>
    <id>http://lab41.github.io/blog/2015/07/28/triplewide-trailer-looking-to-rig-ipython</id>
    <content type="html"><![CDATA[<p>Our <a href="http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker">last post</a> provided a technical overview of how you can deploy an IPython-driven Spark cluster using Docker containers for each component. That setup works well to deploy Spark in standalone mode, but what if you want to run other big data frameworks on the same cluster? With infrastructure tools like <a href="http://mesos.apache.org">Apache Mesos</a>, you can gain fine-grained resource utilization and collapse multiple frameworks – such as Hadoop, Spark, and ElasticSearch – into one cluster. To that end, we’ve been examining how a <a href="https://github.com/Lab41/ipython-spark-docker">Dockerized Spark</a> setup could <a href="https://spark.apache.org/docs/latest/running-on-mesos.html">use Mesos as its task scheduler</a>. What follows is our current thinking on three possible ways that IPython, Spark, Docker, and Mesos can be made to work together.</p>
<div class="margin-bottom-medium">
<img src="http://lab41.github.io/images/post_12_mesos/mesos_resource_sharing.jpg" title="Mesos Resource Sharing" >
</div>
<p>
<small><em>Figure 1: Mesos resource sharing increases throughput and utilization, via <a href="http://www.slideshare.net/caniszczyk/apache-mesos-at-twitter-texas-linuxfest-2014">Apache Mesos at Twitter</a></em></small>
</p>


<h3 id="threes-company">Three’s Company</h3>
<p>Before diving into ways to combine Spark and Mesos with Docker, it will help to give a little background on how Spark <em>typically</em> integrates with Mesos. The interplay between the two systems is important, so bear with me for this quick overview.</p>
<p>Out of the box, it is relatively straightforward for Mesos to distribute Spark tasks to slave nodes. Following the guidance of several <a href="http://www.slideshare.net/pacoid/getting-started-running-apache-spark-on-apache-mesos-30441181">overviews</a> and <a href="http://open.mesosphere.com/tutorials/run-spark-on-mesos">tutorials</a>, the integration begins by first building the Spark binary. Next, you place that binary in an HDFS location each Mesos slave can reach (or alternatively, within the same local directory on each slave). From that point on, when Mesos slaves accept Spark jobs, they retrieve the binary from HDFS (or point to their local path install) to do Spark magic. The following configuration snippet illustrates how to configure Mesos to pull the Spark binary from HDFS:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">MESOS_NATIVE_LIBRARY</span><span class="o">=</span>/usr/local/lib/libmesos.so
</span><span class='line'><span class="nb">export </span><span class="nv">SPARK_EXECUTOR_URI</span><span class="o">=</span>hdfs://hadoop-namenode/tmp/spark-0.8.0-2.0.0-mr1-cdh4.4.0.tgz
</span><span class='line'><span class="nb">export </span><span class="nv">MASTER</span><span class="o">=</span>zk://hadoop-namenode:2181/mesos
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>
<small><em>Figure 2. Mesosphere Spark configuration: First, specify the location of the libmesos.so library. Second, define the URI for the Spark executor, which the Mesos slaves will run. Then define the URIs for the Zookeeper masters.</em></small>
</p>

<p>Once configured, the Spark client can use Mesos as its master, as the <a href="http://people.apache.org/~tdas/spark-1.0-docs/python-programming-guide.html">Spark Python programming guide</a> explicitly states:</p>
<blockquote>
<p>spark-submit supports launching Python applications on standalone, Mesos or YARN clusters, through its <code>--master</code> argument. However, it currently requires the Python driver program to run on the local machine, not the cluster</p>
</blockquote>
<p>This setup should work great for vanilla Spark, but what about our interest in using PySpark with nonstandard Python modules like <a href="http://pandas.pydata.org">Pandas</a> or <a href="http://www.numpy.org">Numpy</a>? As was the impetus for our <a href="https://github.com/Lab41/ipython-spark-docker">ipython-spark-docker</a> project, that would require getting all the right, compatible Python packages to each slave. It looks like using Mesos gets right back to the starting point, where we’d need to do one of the following to make sure each slave has all the right Python packages:</p>
<ul>
<li>Install Python and all required packages on each slave. Luckily, to manage versions and environments, you can <a href="https://mail-archives.apache.org/mod_mbox/spark-user/201409.mbox/%3CCA+2Pv=gmbFNXxPLxL6xu2hNvsWpLtqnQVfvXwhPfUMtC9NzUWw@mail.gmail.com%3E">use Anaconda on each machine</a> if you didn’t want to do system-wide installs.</li>
<li>Use the Spark <code>--py-files</code> option to <a href="http://qnalist.com/questions/4986490/numpy-pyspark">distribute Python packages via egg/zip files</a>. When I attempted this before with Spark standalone mode, I encountered a few difficulties: 1) finding the right packaged module; 2) distributing the modules to all slaves when starting the Spark cluster. I’m sure that has a lot to do with my relative Spark inexperience and interest in having so many Python modules available. Either way, this isn’t a path I plan to revisit.</li>
</ul>
<p>Making a long story short, things aren’t straightforward for getting our desired IPython-driven Spark setup working with Mesos out of the box. So what to do?</p>
<p>Just like before, this desire for portability and repeatability leads to Docker. Since there are several ways to combine these systems, I’ll next walk through three potential architectures for deployment.</p>
<h4 id="option-1-bare-metal-with-a-dash-of-container">Option 1: Bare-metal with a dash of container</h4>
<p>Our first thought for <em>Mesosizing</em> a Dockerized Spark setup was to just install Mesos on bare-metal. Given a vanilla Mesos installation, the Mesos master should be able to accept Spark jobs, send them to the slave nodes, and hosts would then run containerized Spark workers. Voila, distributed analysis! Easy, right?</p>
<p>Not so fast. As near as I can tell, it might not be that straightforward to run Mesos on bare metal as the master for our containerized Spark in cluster mode. Our current Spark worker containers are configured for Spark standalone mode, where the workers register with a Spark master node when launched. Using Mesos as the master would require Mesos to launch the Spark worker containers as task executors. Luckily, Mesos v0.20.1 <a href="https://spark.apache.org/docs/latest/running-on-mesos.html#mesos-docker-support">added the following magic</a>:</p>
<blockquote>
<p>Spark can make use of a Mesos Docker containerizer by setting the property spark.mesos.executor.docker.image in your <a href="https://spark.apache.org/docs/latest/configuration.html#spark-properties">SparkConf</a>. The Docker image used must have an appropriate version of Spark already part of the image, or you can have Mesos download Spark via the usual methods.</p>
</blockquote>
<p>That suggests it might be possible to extend our Spark worker image to include Mesos libraries, point our IPython-Spark client container at Mesos, and configure Spark to launch worker containers to execute Spark tasks. If this option works, it could be a solid way to benefit from Mesos and use our desired Python ecosystem inside Spark worker containers. It could also avoid all the network routing we needed for standalone mode.</p>
<p>Just in case this option has unforeseen issues, or in case we want to consider an alternate architecture in the future, there seem to be two additional options for combining Spark, Mesos, and Docker.</p>
<h4 id="option-2-one-happy-container">Option 2: One happy container</h4>
<p>As outlined above, putting Spark inside Docker provides the ability to quickly spin up and teardown a Spark cluster. To build on that key benefit, it seems like the next step would be to consider a Dockerized Mesos setup similar to our standalone version of Spark-in-Docker containers. But those Mesos containers, of course, also need to include all the requisite Spark and Python libraries, which makes for a pretty beefy container.</p>
<p>Although overloading one Docker image makes me uneasy, I’m wondering if that would deliver a highly portable option for using Spark with Mesos on top of Docker. After all, it should be technically feasible to build a set of master and slave/worker containers that include both Spark and Mesos. Each container would then have all the necessary packages, configurations, and versions. The last step—shuttling communication between hosts and containers—would repeat the network routing work outlined in our <a href="http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker">last post</a>, leading to a similar situation where each host runs one container and “transparently” routes traffic among the cluster hosts and containers.</p>
<p>If we did pursue the <em>Path to Mordor</em> (i.e. “one container to rule them all”), we could build on top of the heavy lifting others have already done to Dockerize Mesos. For example, this <a href="https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586">article</a> and <a href="https://github.com/sekka1/mesosphere-docker">related GitHub repo</a> looks like a solid way to “<em>launch a fault tolerant multi node cluster with about seven Docker commands</em>.” Merging this with our existing repo would take some effort, but we should be able to leverage prior work by adding libraries and configuring host-container routing.</p>
<p>That path is not without its perils, however. Cramming several frameworks into one container could become a slippery slope. As our containerized Spark project demonstrated, network routing (i.e. container1-&gt;host1-&gt;host2-&gt;container2) also isn’t the most straightforward undertaking. Adding Mesos into the mix only complicates matters.</p>
<div class="margin-bottom-medium">
<a class="fancybox-effects-a"  href=/images/post_12_mesos/mesos_docker_stack.png><img src="http://lab41.github.io/images/post_12_mesos/mesos_docker_stack.png" title="ipython-spark-docker Plus Mesos" ></a>
</div>
<p>
<small><em><em>Figure 3</em>: Concept architecture for incorporating Mesos into the existing Docker image</em></small>
</p>


<h4 id="option-3-the-sort-of-standalone-deployment">Option 3: The (sort of) standalone deployment</h4>
<p>Faced with the previous two options (everything in one container or just using Spark worker containers), our team brainstormed a possible third choice. As outlined above, Mesos can <a href="http://mesos.apache.org/documentation/latest/docker-containerizer/">launch tasks that contain Docker images</a>. Further, <a href="https://mesosphere.github.io/marathon/">Marathon</a> is a Mesos framework designed specifically to launch long-running applications using containers. So instead of putting Mesos+Spark into one container, and instead of deploying things on bare metal, could we try <a href="https://mesosphere.github.io/marathon/docs/native-docker.html">Running Docker Containers on Marathon</a>? Therefore, instead of using the Mesos master to distribute Spark jobs to Mesos slaves, Marathon might be able to run the standalone ipython-spark-docker cluster <em>as a service inside of Mesos</em>. I haven’t seen anyone try this specific setup with Spark (and maybe for good reason), but it should be possible for Mesos to spawn Spark containers that would look, feel, and act like a standalone Spark cluster.</p>
<p>One downside of this approach is that we would probably lose some of the efficiencies gained by using Mesos as Spark’s master. Second, it would require the Mesos slaves to redirect a large portion of host ports to the Spark containers, which could break Mesos communication patterns or initiate dominos of errors that could be hard to debug. On that latter point, the standard ports for Mesos (5050/5051), Zookeeper (2181,2888,3888,…) and Marathon (customizable) do not appear to overlap with Spark, giving me hope that network routing might actually be possible. At this point, the only way to know for sure might be to try and see what works and where things break.</p>
<div class="margin-bottom-medium">
<img src="http://lab41.github.io/images/post_12_mesos/mesos_spark_standalone.png" title="Standalone Spark within Mesos" >
</div>
<p>
<small><em><em>Figure 4</em>: Concept architecture for running Dockerized Spark in standalone mode within a Mesos cluster</em></small>
</p>


<h3 id="the-path-forward">The path forward</h3>
<p>The best path forward probably depends on specific needs for using Mesos with Spark with Docker. If Mesos can launch our Spark worker containers, keeping Mesos on metal would position it squarely as a piece of infrastructure and launch Spark jobs as an application (as intended). For those interested in maximizing benefits from Docker, the idea of containerizing Spark, Mesos, and all associated libraries should make it possible to quickly deploy (and/or rebuild) a cluster. On the other hand, despite the efficiencies gained through Mesos, adding several frameworks to one Docker container feels a bit messy. If that is too much, using Marathon to run the Standalone Spark containers as a service might be the option to consider.</p>
<p>Overall, it seems worthwhile to experiment and see where things fall over. We’d be interested in knowing whether anyone else has figured out a way to containerize both Mesos and Spark in a multi-node cluster. As of now, we plan to do the following:</p>
<ol type="1">
<li>Get the <a href="https://github.com/sekka1/mesosphere-docker">multi-node Mesos-in-Docker</a> up and running on a few of our OpenStack nodes. We could just as easily try to install Mesos on metal, but I want to give that project a test drive.</li>
<li>Test how easy it is for Mesos to launch our Spark worker containers to execute Spark jobs.</li>
<li>See if we can use Mesos to spin up the <a href="https://github.com/Lab41/ipython-spark-docker">ipython-spark-docker</a> master and worker images to create a standalone Spark cluster within Mesos.</li>
<li>Poke around to see what it would take to run an IPython-on-Spark-on-Mesos set of containers.</li>
</ol>
<p>Stay tuned for a follow-up post after I finish the above steps. Until then, thanks for reading!</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Docker to Build an IPython-driven Spark Deployment]]></title>
    <link href="http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/"/>
    <updated>2015-04-13T11:11:00+00:00</updated>
    <id>http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker</id>
    <content type="html"><![CDATA[<table class="table-transparent">
  <tr>
    <td width="25%">
<img src="http://lab41.github.io/images/post_11_docker/ipython-spark-docker.png" title="IPython+Spark+Docker Awesomeness" >
</td>
    <td>
<span style="margin-right: 20px">TL;DR:</span> Our <a href="https://github.com/Lab41/ipython-spark-docker">ipython-spark-docker</a> repo is a way to deploy an <a href="https://spark.apache.org">Apache Spark</a> cluster driven by <a href="http://ipython.org">IPython</a> notebooks, running <a href="https://www.docker.com">Docker</a> containers for each component. The project uses Bash scripts to build each node type from a common Docker image that contains all necessary packages, enables data access from a Hadoop cluster, and runs on dedicated hosts. By using IPython as the interface, you can leverage a variety of data processing, machine learning, and visualization tasks using the following tools and libraries:
</td>
  </tr>
</table>

<table class="table-transparent table-centered margin-bottom-large font-size-small">
  <tr>
    <td>
<a href="http://hadoop.apache.org/" target="_blank">HDFS</a>
</td>
    <td>
<a href="http://hbase.apache.org/" target="_blank">Hbase</a>
</td>
    <td>
<a href="https://hive.apache.org/" target="_blank">Hive</a>
</td>
    <td>
<a href="http://oozie.apache.org/" target="_blank">Oozie</a>
</td>
    <td>
<a href="http://pig.apache.org/" target="_blank">Pig</a>
</td>
    <td>
<a href="http://gethue.com/" target="_blank">Hue</a>
</td>
    <td></td>
  </tr>
  <tr>
    <td>
<a href="http://pandas.pydata.org" target="_blank">Pandas</a>
</td>
    <td>
<a href="http://nltk.org" target="_blank">NLTK</a>
</td>
    <td>
<a href="http://www.numpy.org" target="_blank">NumPy</a>
</td>
    <td>
<a href="http://scipy.org" target="_blank">SciPy</a>
</td>
    <td>
<a href="http://sympy.org" target="_blank">SymPy</a>
</td>
    <td>
<a href="http://scikit-learn.org/" target="_blank">Scikit-Learn</a>
</td>
  </tr>
  <tr>
    <td>
<a href="http://cython.org" target="_blank">Cython</a>
</td>
    <td>
<a href="http://numba.pydata.org" target="_blank">Numba</a>
</td>
    <td>
<a href="http://biopython.org" target="_blank">Biopython</a>
</td>
    <td>
<a href="http://zeromq.org/bindings:python" target="_blank">0MQ</a>
</td>
    <td>
<a href="http://www.clips.ua.ac.be/pattern" target="_blank">Pattern</a>
</td>
    <td>
<a href="http://stanford.edu/~mwaskom/software/seaborn/" target="_blank">Seaborn</a>
</td>
  </tr>
  <tr>
    <td>
<a href="http://matplotlib.org/" target="_blank">Matplotlib</a>
</td>
    <td>
<a href="http://statsmodels.sourceforge.net/" target="_blank">Statsmodels</a>
</td>
    <td>
<a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">Beautiful Soup</a>
</td>
    <td>
<a href="https://networkx.github.io/" target="_blank">NetworkX</a>
</td>
    <td>
<a href="http://numba.pydata.org/" target="_blank">LLVM</a>
</td>
    <td>
<a href="http://mdp-toolkit.sourceforge.net/" target="_blank">MDP</a>
</td>
  </tr>
  <tr>
    <td>
<a href="http://bokeh.pydata.org/" target="_blank">Bokeh</a>
</td>
    <td>
<a href="https://github.com/wrobstory/vincent" target="_blank">Vincent</a>
</td>
  </tr>
</table>



<h3 id="generating-a-few-amps">Generating a few amps</h3>
<p>Here at <a href="http://www.lab41.org">Lab41</a>, we build open source prototypes using the latest big data, infrastructure, and data science technologies. In a perfect world, these tools would magically work together. The reality is that they usually require a lot of effort just to install and configure properly. And when someone else comes along to actually use them — especially if they are a newly-minted teammate or someone unfamiliar with the myriad command-line switches and gotchas — the experience can transform these tools into shark repellent for sysadmins and end users alike.</p>
If the above sounds familiar, or if you’re interested in using IPython notebooks to perform non-trivial<span style="color: #C30017">*</span> data analytics with Apache Spark, then please continue…
<div class="margin-bottom-large" style="color: #AAA; text-align: right">
<small><span style="color: #C30017">*</span>defined roughly as being able to use data in our 32-node Hadoop cluster</small>
</div>
<h4 id="sparked-interest">Sparked interest</h4>
<p>This effort started when I became interested in Apache Spark, which has quickly become the heir apparent to MapReduce’s big data throne. By most measures, this data processing engine is living up to claims of <a href="https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html">better performance</a> and usable APIs for <a href="https://spark.apache.org/mllib">powerful algorithmic libraries</a>. If you add in its support for interactive and iterative development, plus use of data-scientist- and developer-friendly languages like Python, it’s no surprise why so many have fallen for this relative newcomer.</p>
<p>Out of the box, Spark includes a number of powerful capabilities, including the ability to write SQL queries, perform streaming analytics, run machine learning algorithms, and even tackle graph-parallel computations. Those features enable Spark to compete with a number of tools from mature ecosystems like Hadoop, but what really stands out is its usability. In short, incorporating interactive shells (in both Scala and Python) presents an approachable way to kick the tires. In my book, that’s a huge win that should help pull in curious developers (like me). After going through Spark’s cut-and-paste <a href="https://spark.apache.org/examples.html">examples</a>, as well as a <a href="http://mbonaci.github.io/mbo-spark">few</a> <a href="http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial">more involved</a> <a href="http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5">tutorials</a>, I had seen enough to want to begin using this platform. Anticipating the rest of our team benefiting from its capabilities, I also became interested in enabling their data analysis needs.</p>
<h3 id="usability-a-driving-need">Usability a driving need</h3>
<p>Within our team, we have developers, data scientists, and analysts with varying skills and experiences. Providing a solution that everyone could use was a key goal, which led to the following objectives:</p>
<ul>
<li>We needed the Spark cluster to handle decent-sized workloads. Our 32-node Hadoop cluster is a representative size.</li>
<li>We needed an “easy-to-use” interface and language bindings that everyone would have a shot at learning. Python would be good. Something with collaboration features that wasn’t driven by the command line would be great.</li>
<li>We needed to run analytics against “non-trivial” data, which I’ll define as being able to access and process data from our Hadoop cluster.</li>
</ul>
<p>After giving it some thought, I realized IPython would address that short list nicely and would be a familiar interface for our team. I decided to try to build something that looks like:</p>
<div class="margin-bottom-medium">
<img src="http://lab41.github.io/images/post_11_docker/architecture-v1-draw.io.png" title="Overview of IPython-Spark-HDFS concept" >
</div>
Definitions
<ul>
  <li>
<strong>Spark Master</strong>: the Spark node that receives jobs, organizes the workflow, and doles out work
</li>
  <li>
<strong>Spark Worker</strong>: <em>N</em> number of Spark nodes that receive work tasks from the master and do the actual analysis
</li>
  <li>
<strong>IPython Driver</strong>: the process running the application’s main function; the Python shell in “client” mode submits from outside the cluster, which is why I refer to it as the <em>remote client</em> and <em>client driver</em>
</li>
</ul>


<h3 id="a-straightforward-path">A straightforward path?</h3>
<p>The first step, deploying the Spark cluster, seemed trivial since Lab41 uses a CDH5 cluster and Cloudera includes Spark in their distribution. However, I also had to develop around the situation where end users won’t be able to login/SSH directly to the Spark cluster for their analytics. Most of our partners are very security-conscious, so adding a client node that can remotely connect and drive analytics on the cluster became the next must-have. “<em>Easy</em>,” I thought. “<em>I’ll just setup a remote node to drive Spark analysis within the cluster</em>.” I assumed the steps would be straightforward (and probably already solved):</p>
<ol type="1">
<li>Start the master</li>
<li>Connect workers to the master node</li>
<li>Configure a remote client connected to the master</li>
<li>Deploy an IPython notebook server on the client</li>
</ol>
<p>Starting the master and worker nodes in our CDH5 cluster via the <a href="http://www.cloudera.com/content/cloudera/en/documentation/cloudera-manager/v4-latest/Cloudera-Manager-Installation-Guide/cmig_spark_installation_standalone.html">Cloudera Manager</a> <em>was</em> straightforward. Building a client node also was easy since the Spark team graciously provides several source and pre-built packages for several recent releases. With a straightforward download and install, my client was ready to drive the cluster.</p>
<p>To initially test the client driver — considering the end goal was to use IPython — I decided to start with a <code>pyspark</code> shell connected to the master (I decided to hold off on IPython integration to isolate any potential misconfigurations). Based on tutorials, connecting the remote client to the cluster initially appeared as easy as specifying <code>./bin/pyspark --master spark://ip:port</code>. However, I immediately ran into a couple errors related to library mismatches:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>incompatible spark driver - java.io.InvalidClassException: org.apache.spark.deploy.ApplicationDescription; <span class="nb">local </span>class incompatible: stream classdesc <span class="nv">serialVersionUID</span> <span class="o">=</span> 583745679236071411, <span class="nb">local </span>class <span class="nv">serialVersionUID</span> <span class="o">=</span> 7674242335164700840
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>java.lang.ClassCastException <span class="o">(</span>cannot assign instance of org.apache.spark.rdd.RDD<span class="nv">$$</span>anonfun<span class="nv">$17</span> to field org.apache.spark.SparkContext<span class="nv">$$</span>anonfun<span class="nv">$runJob$4</span>.func<span class="nv">$1</span> of <span class="nb">type </span>scala.Function1 in instance of org.apache.spark.SparkContext<span class="nv">$$</span>anonfun<span class="nv">$runJob$4</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>After a few rounds of Googling, I found out these errors are caused by using incompatible spark driver libraries. Understandably, the client driver node needs to use libraries compatible with our cluster nodes, whereas the driver’s v1.2.<span style="color: #C30017; font-weight: bold">1</span> was apparently incompatible with our cluster’s v1.2.<span style="color: #C30017; font-weight: bold">0</span>. With that quick reminder to always verify build versions, I downloaded and installed the correct one on the client. Problem fixed!</p>
<h4 id="down-the-rabbit-hole-i-went">Down the rabbit hole I went…</h4>
<p>With those library mismatch errors behind me, I soon encountered another error:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>These kinds of errors scare me more than most since they give just enough to identify the general cause (“registering” the client and/or workers), but not enough to figure out exactly where to look. After poking around the server and worker logs (<code>/var/log/spark/spark-&lt;master|worker&gt;-&lt;hostname&gt;.log</code>), it looked like the client successfully connected to the master, but something after that failed to complete the Spark initialization. Errors like the following highlighted that it had something to do with my network configuration:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>WARN Remoting: Tried to associate with unreachable remote address <span class="o">[</span>akka.tcp://sparkDriver@sandbox:41615<span class="o">]</span>. Address is now gated <span class="k">for </span>60000 ms, all messages to this address will be delivered to dead letters.
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>Since “unreachable addresses” can of course be caused by several factors, I’ll save you the nitty-gritty and jump straight to the important point: connecting a remote client node requires several expected and non-obvious network settings:</p>
<ul>
<li><strong>Expected Node-to-Node Communication</strong>: The master and workers must be reachable. This requirement is satisfied by several <a href="http://spark.apache.org/docs/1.2.1/spark-standalone.html">well-documented</a> configurations:
<ul>
<li>Firewall rules must allow traffic to the Spark service (default: 7077 for master, random for worker)</li>
<li>Firewall rules must allow traffic to the Spark UIs (default: 8080 for master, 8081 for worker)</li>
</ul></li>
<li><strong>Unexpected Cluster-&gt;Driver Communication</strong>: From the <a href="http://spark.apache.org/docs/1.2.1/security.html">Configuring Ports for Network Security</a> page, notice how important things such as communicating state changes and serving files require connections from the Spark nodes <strong>to</strong> the driver on a <strong>random</strong> port. That architecture means the remote client node is opening randomly-selected ports for callback from nodes in the Spark cluster. This design forces two important updates to our network communication:
<ul>
<li>Firewall rules must allow traffic from the Spark master and workers to a range of random ports on the client</li>
<li>The client must be network-addressable by master and worker nodes, which means the <code>tcp://sparkDriver</code> above needed to be a fully-qualified domain name (FQDN) on the network.</li>
</ul></li>
</ul>
<h3 id="compatibility-is-key">Compatibility is key</h3>
<p>Whereas I could easily open the potential range of random ports on master, worker, and client nodes, adding a network-addressable client to the cluster felt like a step too far for this initial test setup. At this point, I decided to stop using our primary Hadoop cluster and instead virtualize a test Spark cluster within our internal instance of <a href="https://www.openstack.org">OpenStack</a>. As before, using the pre-built Spark packages made it easy to create master and worker nodes for a standalone Spark installation. Running the startup scripts <code>./sbin/start-&lt;master|slaves|all&gt;.sh</code> fired up and registered the master and workers, providing me with a throwaway cluster I could use for experimentation more comfortably.</p>
<p>I now had spun up a small virtualized Spark cluster, added a client node on the network, ensured it was reachable with a FQDN, and opened all necessary OpenStack security rules and ports for each node. For good measure I ensured each node’s <code>/etc/hosts</code> contained entries for the cluster’s nodes (i.e. <code>10.1.2.3 spark-node1.internal-domain</code>), leaving me confident all necessary traffic would reach its intended destination.</p>
<p>With the network configurations behind me, the quest led me into another set of library mismatch errors:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>INFO scheduler.TaskSetManager: Lost task 613.3 in stage 0.0 <span class="o">(</span>TID 2261<span class="o">)</span> on executor spark-node1.internal-domain: java.io.IOException <span class="o">(</span>Cannot run program <span class="s2">&quot;python2.7&quot;</span>: <span class="nv">error</span><span class="o">=</span>2, No such file or directory<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>“<em>Hmmm, strange</em>,” I thought. “<em>All of my OpenStack images include Python…what’s the deal?</em>” Well, when I provisioned the OpenStack instances, I used different host images for the cluster nodes and client driver as a way to better mimic that real-world possibility. It turns out the older worker/client nodes had python2.6, whereas the client (and Spark’s default options) explicitly specify python2.7. Updating the client environment to <code>export PYSPARK_PYTHON=python</code> propagated Spark’s configuration and let each node rely on their native python build. This situation clearly won’t work for a production deployment, but I was at the stage of wanting to move past errors and could later re-build environments and configurations.</p>
<p>Next, I ran into the strange situation where my client would accept the examples I had created, but when it submitted jobs to workers, they seemed to be missing things and would fail with messages such as:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>ImportError: No module named numpy
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>and</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>version <span class="s1">&#39;GLIBC_2.14&#39;</span> not found
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>Of course! In my previous rounds of <a href="http://www.hanselman.com/blog/YakShavingDefinedIllGetThatDoneAsSoonAsIShaveThisYak.aspx">yak-shaving</a> fixes, I forgot one obvious requirement: All Spark nodes clearly need the same/compatible environment to effectively distribute analysis (aka “not fail”) across the cluster. It wasn’t sufficient to add things like <code>numpy</code> and <code>GLIBC</code> to my client; every node in the Spark cluster also needed those same modules and libraries.</p>
<h4 id="whale-of-a-pivot">Whale of a pivot</h4>
<p>I made a crucial decision at this point. I did not like the idea of continuing to tweak and tune the configurations, environments, and libraries for each master, worker, and client hosts. While I was beginning to understand things, I knew nobody else would be able (or want) to replicate my work. If only there was a technology focused on transparent repeatability and portability of infrastructure…</p>
<p>Enter <a href="https://www.docker.com">Docker</a>!</p>
<p>Yes, the fantastic “build once, run anywhere” container not only enables development of portable apps, but also can be a godsend to sysadmins in this type of situation. From their website:</p>
<blockquote>
<p>Sysadmins use Docker to provide standardized environments for their development, QA, and production teams, reducing “works on my machine” finger-pointing. By “Dockerizing” the app platform and its dependencies, sysadmins abstract away differences in OS distributions and underlying infrastructure.</p>
</blockquote>
<p>Perfect! This benefit, I knew, would enable me to package all the configuration options within a common environment I could then deploy as master, worker, and client nodes. <em>Caveat emptor</em>, though. I have used Docker enough to know my intended IPythonized-Spark (or is it Sparkified-IPython?) setup would require a decent amount of customization, especially the network configuration pieces. But I also knew it was possible, and since a combination of Dockerfiles and scripting would lead to a repeatable build, I made the call to Dockerize the entire setup.</p>
<h3 id="dockerization">Dockerization</h3>
<p>Since others already figured out how to run Spark in Docker, I first turned to those <a href="https://github.com/sequenceiq/docker-spark">images</a> and <a href="https://github.com/amplab/docker-scripts">tutorials</a>. After using them, I learned a few important things:</p>
<ol type="1">
<li>The images usually run the entire Spark cluster in a single container, which is great for kicking the tires, but not realistic for running actual workloads. I needed to run each master/worker/client as a separate container, ideally with each on a dedicated node to support large workloads.</li>
<li>The images usually include default configurations for accessing Hadoop nodes that spin up within the container. While I could manually specify the longhand <code>hdfs://&lt;hadoop-namenode&gt;/path/to/hdfs/file</code> to access our Hadoop cluster, I’m lazy and wanted our <code>hadoop-namenode</code> to serve as the container’s default HDFS endpoint. To enable that default connectivity, I added our Hadoop configuration to the container. As an added measure for data locality, the ideal deployment would run these containers inside our Hadoop nodes and thereby avoid sending large amounts of data across the network. Keep in mind this setup means you’ll have to ensure library version compatibility between the containers and your Hadoop nodes.</li>
<li>When run, the default Docker option sets each container’s hostname to its container ID, which causes issues related to Spark’s use of FQDN for network traffic. For example, a hostname of <code>d5d3225d06c4</code> would cause Spark workers to attempt sending traffic to that host, which of course wouldn’t exist on the network. By passing the host node’s hostname to the container at runtime, the container effectively “thinks” it is the host and can broadcast the appropriate destination address.</li>
<li>The images provide a very basic Python environment. We need several additional data wrangling, machine learning, and visualization tools.</li>
<li>They’ve all done great work I can build on for our needs.</li>
</ol>
Refining my original architecture, I was looking to build something like:
<div>
<img src="http://lab41.github.io/images/post_11_docker/architecture-v2-draw.io.png" title="Refined concept for IPython-Spark-HDFS using Docker" >
</div>
<p>
<small><em>Refined concept for IPython-Spark-HDFS using Docker</em></small>
</p>

<h4 id="the-base-image">The base image</h4>
<p>Since I’ve used similar bits and pieces in other work, I knew where I wanted to start for building the foundation. I started with the following Docker images:</p>
<ul>
  <li>
<a href="https://registry.hub.docker.com/u/richhaase/cdh5-hadoop/">richaase/cdh5-hadoop</a>: The base image contains CDH5 (Cloudera Distribution for Apache Hadoop 5) installed on Ubuntu 14.04 with Oracle JDK7. I added configuration files to access a remote HDFS cluster. Among other things, they include the following tools and libraries:
<table class="table-transparent table-centered margin-bottom-medium">
      <tr>
        <td>
<a href="http://hadoop.apache.org/">HDFS</a>
</td>
        <td>
<a href="http://hbase.apache.org/">Hbase</a>
</td>
        <td>
<a href="https://hive.apache.org/">Hive</a>
</td>
        <td>
<a href="http://oozie.apache.org/">Oozie</a>
</td>
        <td>
<a href="http://pig.apache.org/">Pig</a>
</td>
        <td>
<a href="http://gethue.com/">Hue</a>
</td>
      </tr>
    </table>
  </li>

<li>
<a href="https://github.com/mingfang/docker-ipython">mingfang/docker-ipython</a>: The base image runs a robust IPython environment inside Docker, which I tweaked by enabling, disabling, and adding a few Python modules. It adds:
<table class="table-transparent table-centered margin-bottom-medium">
      <tr>
        <td>
<a href="http://pandas.pydata.org" target="_blank">Pandas</a>
</td>
        <td>
<a href="http://nltk.org" target="_blank">NLTK</a>
</td>
        <td>
<a href="http://www.numpy.org" target="_blank">NumPy</a>
</td>
        <td>
<a href="http://scipy.org" target="_blank">SciPy</a>
</td>
        <td>
<a href="http://sympy.org" target="_blank">SymPy</a>
</td>
        <td>
<a href="http://scikit-learn.org/" target="_blank">Scikit-Learn</a>
</td>
      </tr>
      <tr>
        <td>
<a href="http://cython.org" target="_blank">Cython</a>
</td>
        <td>
<a href="http://numba.pydata.org" target="_blank">Numba</a>
</td>
        <td>
<a href="http://biopython.org" target="_blank">Biopython</a>
</td>
        <td>
<a href="http://zeromq.org/bindings:python" target="_blank">0MQ</a>
</td>
        <td>
<a href="http://www.clips.ua.ac.be/pattern" target="_blank">Pattern</a>
</td>
        <td>
<a href="http://stanford.edu/~mwaskom/software/seaborn/" target="_blank">Seaborn</a>
</td>
      </tr>
      <tr>
        <td>
<a href="http://matplotlib.org/" target="_blank">Matplotlib</a>
</td>
        <td>
<a href="http://statsmodels.sourceforge.net/" target="_blank">Statsmodels</a>
</td>
        <td>
<a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">Beautiful Soup</a>
</td>
        <td>
<a href="https://networkx.github.io/" target="_blank">NetworkX</a>
</td>
        <td>
<a href="http://numba.pydata.org/" target="_blank">LLVM</a>
</td>
        <td>
<a href="http://mdp-toolkit.sourceforge.net/" target="_blank">MDP</a>
</td>
      </tr>
      <tr>
        <td>
<a href="http://bokeh.pydata.org/" target="_blank">Bokeh</a>
</td>
        <td>
<a href="https://github.com/wrobstory/vincent" target="_blank">Vincent</a>
</td>
      </tr>
    </table>
  </li>
</ul>


<h4 id="enhancing-the-base">Enhancing the base</h4>
<p>Borrowing from those two Docker images to build the common base, I layered a few important changes within the Dockerfile:</p>
<ol type="1">
<li>The image downloads updated Spark libraries to the latest pre-built standalone packages.</li>
<li>The image updates Spark configuration options for <code>PYSPARK_PYTHON</code>, <code>SPARK_SSH_PORT</code> and <code>SPARK_SSH_OPTS</code>. The latter two force Spark to communicate on SSH via a non-standard port (I chose 2122). I made this change to the containers’ SSH daemons so I could still SSH in “normally” via port 22 on the host machines.</li>
<li>I added SSH keys to enable Spark master-worker communication, which I <strong>strongly</strong> recommend re-generating before your build if you decide to try.</li>
<li>I added specific configuration details for HDFS access, which you’ll need to update to connect to your cluster.</li>
</ol>
<h4 id="creating-role-based-images">Creating role-based images</h4>
<p>Building on that base image, I created Docker images for each master, worker, and client node types. Each image uses a <code>bootstrap.sh</code> script to start <code>runit</code>, leaving each node type to implement different startup services:</p>
<ul>
<li>The master image runs an SSH daemon and <code>spark-master</code> process. This setup violates Docker’s “one-process-per-container” philosophy, but is necessary since master and workers communicate via SSH (as noted before, via port 2122)</li>
<li>Similarly, the worker images startup an SSH daemon and <code>spark-worker</code> process</li>
<li>The client image runs an IPython notebook using a custom pyspark profile, which I configured by following guides such as <a href="http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark">How to use IPython notebook with Apache Spark</a></li>
</ul>
<h4 id="running-the-containers">Running the containers</h4>
<p>I wrote a few Bash scripts to startup each container type. If you plan to use these, keep in mind two important details:</p>
<ul>
<li>Since I wanted each container living on a dedicated node, you’ll have to manually startup each container type within a provisioned node. Within OpenStack, I simply startup each instance with an after-build command to run that node type’s startup script (i.e. <code>./3-run-spark-worker.sh spark://master-fqdn:port</code>). If provisioning on bare metal and/or within your HDFS cluster, you could use something heavyweight like <a href="https://puppetlabs.com">puppet</a>, a lighter deployment tool like <a href="http://flask.pocoo.org/docs/0.10/patterns/fabric">fabric</a>, or even a simple series of <code>ssh -e</code> commands.</li>
<li>Given the required network traffic between master, workers, and client, there is a wide range of default ports that each host needs to forward to its respective container. Simply put, each host needs to transparently forward all Spark ports to its container. I could have achieved this by <code>EXPOSE</code>ing ports in the Dockerfile and later publishing each port/range at runtime, but that method can cause <code>iptables</code> to run out of memory. Plus, it makes the container metadata (and <code>docker ps</code> output) unreadable with so many mapped ports. Instead, I made the host create a new <code>iptables</code> chain with custom <code>PREROUTING</code> rules. If you don’t want <code>iptables</code> as a dependency, or if you just want to handle networking <em>The Docker Way</em>, I would suggest explicitly setting the random ports identified in the <a href="http://spark.apache.org/docs/1.2.1/security.html">Configuring Ports for Network Security</a> guide (i.e. <code>SPARK_WORKER_PORT</code> and <code>spark.driver.port</code>).</li>
</ul>
The end result is: <a class="fancybox-effects-a"  href=/images/post_11_docker/architecture-v3-draw.io.png><img src="http://lab41.github.io/images/post_11_docker/architecture-v3-draw.io.png" title="Docker image and networking for ipython-spark-docker deployment" ></a>
<p>
<small><em>Docker image and networking for ipython-spark-docker deployment</em></small>
</p>

<table class="table-transparent margin-bottom-large">
  <tr>
    <td width="50%">
      
<a class="fancybox-effects-a"  href=/images/post_11_docker/screenshot_wordcount.png><img src="http://lab41.github.io/images/post_11_docker/screenshot_wordcount.png" title="Screenshot of Spark word count example using ipython-spark-docker" ></a>
<div>
<small><em>Spark word count example via IPython</em></small>
</div>
<pre><code>&lt;/td&gt;
&lt;td&gt;
  &lt;a class=&quot;fancybox-effects-a&quot;  href=/images/post_11_docker/screenshot_mllib.png&gt;&lt;img src=&quot;/images/post_11_docker/screenshot_mllib.png&quot; title=&quot;Screenshot of Spark MLlib example&quot; &gt;&lt;/a&gt;
  &lt;div&gt;&lt;small&gt;_Spark MLlib example_&lt;/small&gt;&lt;/div&gt;
&lt;/td&gt;</code></pre>
</tr>
</table>


<h3 id="wrap-up">Wrap-up</h3>
<p>As with most big data platforms, setting up Apache Spark was not a simple “double-click installation” process. It required host and network configurations that sometimes were difficult to find and decipher. Adding my goal of driving analytics with a remote client revealed additional gotchas. I managed to troubleshoot these, but it was an effort that I wouldn’t want others to have to reproduce. The extra desire to leverage IPython’s simpler interface, connect to our HDFS cluster, and ensure library compatibility between all nodes led me to Docker’s doorstep.</p>
<p>While the architecture is complex, Docker made it less complicated and more repeatable to develop, test, document, and iterate. Fast forward to today, we now have a working version of IPython-driven Spark analytics on our HDFS data, which is something others might be looking to use. And rather than say, “<em>Email me for help</em>,” or “<em>Google ‘this’ and StackOverflow ‘that’</em>,” I can point you to <a href="https://github.com/Lab41/ipython-spark-docker">ipython-spark-docker</a> for:</p>
<ol type="1">
<li>A base Docker image that contains all necessary Hadoop, Spark, and Python packages.</li>
<li>Skeleton configuration files for HDFS access.</li>
<li>Separate master, worker, and client images.</li>
<li>Bash scripts to build and run each base/master/worker/client.</li>
<li>End users access and use the entire system through the client container’s IPython notebook.</li>
</ol>
<p>If you’ve read this far, thanks for your patience while I walked you through this end-to-end journey. I came across so many questions online where people ran into similar problems that I wanted to document the entire process. Hopefully, this post will save others from wondering where things might have gone wrong.</p>
<p>If you decide to give our <a href="https://github.com/Lab41/ipython-spark-docker">repo</a> a try, let us know. The Lab is interested in knowing if it helps, and is happy to offer a helping hand if something needs a little more work.</p>
<p>Until our next post, thanks for reading!</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing...SKYLINE]]></title>
    <link href="http://lab41.github.io/blog/2015/03/13/skyline/"/>
    <updated>2015-03-13T00:00:00+00:00</updated>
    <id>http://lab41.github.io/blog/2015/03/13/skyline</id>
    <content type="html"><![CDATA[<div class="row-fluid" style="margin-bottom: 20px">
<div class="span3 image_holder_ks" style="padding-top: 5px">
<img src="http://lab41.github.io/images/post_10_skyline/skyline.png" style="border: 0; box-shadow: none; border-radius: 0; -webkit-box-shadow: none;">
</div>
<div class="span9">
<pre><code>Here at Lab41, we’ve recently found ourselves interested in dynamic graphs and we’ve spent the last few months trying to understand what tools we can use to analyze them – we call this effort Project SkyLine. We’re writing this blog to explain why we think dynamic graphs are interesting, and what we’ve found out so far.</code></pre>
</div>
</div>
<h3 id="whats-so-great-about-dynamic-graphs"><strong>What’s so great about dynamic graphs?</strong></h3>
<p>We’ve said it before and we’ll say it again, “Graphs are a great way to model the world around us – from links on the Internet, to the wiring of our brains, to our friendships and relationships.” Graphs naturally represent connections, and connections are central to each of these things. That’s not the whole story though: the world is constantly changing, and so are those connections. Web pages are taken down and links are added every day; our brains constantly rewire themselves as we learn and experience the world.</p>
<p>To understand how the world is changing, we need to be able to analyze graphs that change over time – in other words, <strong>dynamic graphs</strong>. In a dynamic graph, new edges and vertices can be created at any time, old ones can be destroyed, and attributes (things like age or location) can be altered at any moment, updating the graph to reflect changes in the things and relationships it represents.</p>
<p>We can learn a great deal by applying graph analytic techniques to dynamic graphs. For instance, a dynamically connected components algorithm might tell us when someone joins or leaves a particular group of friends; applying PageRank to the web can show us how web pages rise or fall in mindshare. In addition, we can watch for patterns in the graph – like a series of vertices and edges matching a particular query of interest – flag them as they emerge, and track them for as long as they endure. We call this functionality “triggering” because it lets us respond to specific kinds of changes to the graph by using them to “trigger” relevant actions, as in the example below:</p>
<p>Imagine we have a graph where the vertices are web pages and the edges are links connecting them. We might have a trigger such as, “Send a notification whenever a page with ‘dogs’ in the URL is connected to a page with ‘cats’ in the URL which is connected to a page with ‘parakeets’ in the URL.”</p>
<p>The initial graph below doesn’t contain that path and therefore wouldn’t fire that trigger at all:</p>
<p><a class="fancybox-effects-a"  href=/images/post_10_skyline/trigger_ex.png><img src="http://lab41.github.io/images/post_10_skyline/trigger_ex.png" title="Trigger Example" ></a></p>
<p>However, a later update could add an edge between “example.com/cats” and “example.com/parakeets,” which should trigger a notification being sent to the user since it matches the path highlighted by the red arrows [“example.com/dogs”, “example.com/cats”, “example.com/parakeets”]:</p>
<p><a class="fancybox-effects-a"  href=/images/post_10_skyline/trigger_ex_red.png><img src="http://lab41.github.io/images/post_10_skyline/trigger_ex_red.png" title="Trigger Example" ></a></p>
<p>[“example.com/dogs”, “example.com/cats”, “example.com/parakeets”] and a notification should be sent to the user.</p>
<h3 id="so-how-many-open-source-packages-does-it-take-to-analyze-a-dynamic-graph"><strong>So, how many open source packages does it take to analyze a dynamic graph?</strong></h3>
<p>Unfortunately, graph analytics to date have dealt mostly with static graphs – graphs that don’t update change over time – and most of the relevant software is designed for that use case. There are a few exceptions, but even these aren’t very well known, so we decided to figure out what’s possible today.</p>
<p>We started off by looking at all the open source graph analytics packages we could find. Our goal was to find out what functionality each one offers, what use cases are supported, and how these hold up to the stress of real-world dynamics (possibly changing hundreds of thousands of times per second). Below is a summarized version of what we found, and you can see the results for yourself in all their gory detail <a href="http://lab41.github.io/SkyLine/">here</a>.</p>
<p>In the table below, we’ve included a few of the categories that we felt were the important points when making a decision on which tool to use. Here’s what each one means:</p>
<p><strong>ACID Compliance / Eventual Consistency:</strong> Each operation relies on the state of the underlying graph in some way. What guarantees does this platform provide that each operation will see all changes made by its predecessors / will not interrupt or conflict with another operation happening concurrently?</p>
<p><strong>Supports Graphs Larger than Memory:</strong> Pretty self-explanatory, can this platform handle graphs bigger than the memory of the machine it’s running on?</p>
<p><strong>Supports Edge/Vertex Labels:</strong> Can we attach additional information to each edge and vertex besides what vertices/edges it’s connected to?</p>
<p><strong>Supports Dynamic Graphs/Streaming:</strong> Can this platform handle changes to the graph under consideration, without having to reload it altogether?</p>
<p><strong>Supports Triggering:</strong> If the answer to the feature above is yes, is there a way to run some piece of code every time a particular type of change happens (for instance, every time a vertex is added with the “name” attribute set to “Bob”)?</p>
<p><strong>Quality of Documentation:</strong> On a scale from “I can haz cheezburger?” to The Encyclopaedia Britannica, just how approachable and comprehensive is the documentation? And on a scale from Twilight to Shakespeare, how readable is it?</p>
<div>
<div class="img_holder_ks">
<a class="fancybox-effects-a" href="http://lab41.github.io/images/post_10_skyline/chart.png" style="text-decoration: none;"> <img src="http://lab41.github.io/images/post_10_skyline/chart.png" title="Chart" style="border: 0; box-shadow: none; border-radius: 0; -webkit-box-shadow: none;"> </a>
</div>

<p>Summarized Points of Comparison (Full Survey: <a href="http://lab41.github.io/SkyLine" target="_blank" rel="nofollow">http://lab41.github.io/SkyLine</a>)</p>
<p>Looking at this table, it becomes clear that only a few of the packages under consideration attempt to support all of dynamic graphs, streaming updates, and triggering:</p>
<p><strong>Titan</strong>: a leading graph database, created and maintained by the team at Aurelius (now DataStax) and being used at places including Cisco Systems and Los Alamos National Labs.</p>
<p><strong>Stinger</strong>: an open source project started by a team at Georgia Tech based on their work on efficient graph data structures. The goal of Stinger is to support high performance analytics on dynamic graphs!</p>
<p><strong>Weaver</strong>: a new open source, distributed graph store by a team at Cornell’s Systems Group, which shards the graph over multiple servers, and supports highly efficient transactions and updates.</p>
<p>As soon as we saw Weaver, we fell in love with the vision behind it. It looks like a really solid idea with a very smart group of contributors working on it. Unfortunately, it’s very much in its infancy, and the FAQ makes it very clear that Weaver isn’t production-ready, so we’ve had to put a pin in this one for now. Nonetheless, we’ll be following it closely over the near future, and are excited to see what becomes of the project.</p>
<p>That leaves us with Titan and Stinger. Since our first concern is with the platform’s ability to handle updates to the graph efficiently, we decided to benchmark the speed with which each one could process a given stream of changes to a starting graph (actually, a starting collection of disconnected vertices).</p>
<p>We wrote <a href="https://github.com/Lab41/SkyLine">a Python script</a> to create graphs that are representative of interesting workloads for us: lots of nodes and edges, potentially long cycles, vertex attributes, etc. In order to do this efficiently, our script started off by generating a large number of trees, and then randomly adding ancestors to each node from the set of nodes closer than it to the root. Our script then picked and joined random pairs of nodes, and selected sequences of nodes which it joined together to make cycles (all the requisite probabilities and limits were tunable, and the random number generator was given the constant seed of 0xcafebabe, for reproducibility). This random generation of graphs is slightly different than our previous work with stochastic Kronecker natural graphs which, for those who are interested, <a href="http://lab41.github.io/blog/2013/08/27/stochastic-kronecker-natural-graphs/">can be found here</a>.</p>
<p>The next step was to turn this graph into a randomly ordered stream of updates that could be used to generate it. This was slightly more complex than it sounds, since we wanted to ensure that:</p>
<ol style="margin-left: 2em">
    <li>
No edge was created unless the vertices on each end existed.
</li>
    <li>
Every vertex created (after those in the initial set) would have an edge connecting it to an existing vertex no more than one step later in the stream.
</li>
</ol>   
<p>In short, this meant that we had to make sure that at least one predecessor of any given vertex existed before it was itself created. To do this, we started with the standard Graph Traversal Algorithm:</p>
Repeat:
<ol style="margin-left: 2em">
    <li>
Select a path on the frontier. Let’s call the path selected P.
</li>
    <li>
Remove P from the frontier.
</li>
    <li>
For each neighbor of the node at the end of P, extend P to that neighbor and add the extended path to the frontier.
</ol>
<p>Until the frontier is empty. (Adapted from <a href="http://artint.info/tutorials/search/search_1.html" target="_blank" rel="nofollow">http://artint.info/tutorials/search/search_1.html</a>)</p>
<p>Where the frontier is defined as the set of nodes or edges to be explored and is initially set to all nodes that are adjacent to the root (are at the other end of an edge from the root node) or simply all the edges emanating from the root.</p>
<p>We then tweaked this algorithm so that the frontier was randomly ordered. By ensuring no node would ever get into the frontier (and thus be added to the stream) before at least one of its predecessors, we mirrored the random ordering that real streams exhibit.</p>
<p>(After all, in the real world, we can easily predict that a father will exist before his son, but not which father will have a son first!) We then fed the resulting streams to both Titan (using the Berkeley DB backend) and Stinger and measured total time taken to process them. Below are our findings.</p>
<p>We then fed the resulting streams to both Titan (using the Berkeley DB backend) and Stinger and measured total time taken to process them. Below are our findings.</p>
<table class="table-lab41">
  <tbody>
    <tr>
      <td>
Number of Nodes
</td>
      <td>
Number of Edges
</td>
      <td>
Titan Time
</td>
      <td>
Stinger Time
</td></tr>
    <tr>
      <td>
23,236
</td>
      <td>
37,391
</td>
      <td>
8.9 sec
</td>
      <td>
0.05 sec
</td></tr>
    <tr>
      <td>
33,510
</td>
      <td>
65,759
</td>
      <td>
9.0 sec
</td>
      <td>
0.08 sec
</td></tr>
    <tr>
      <td>
52,203
</td>
      <td>
100,712
</td>
      <td>
11.5 sec
</td>
      <td>
0.11 sec
</td></tr>
    <tr>
      <td>
74,724
</td>
      <td>
114,785
</td>
      <td>
11.5 sec
</td>
      <td>
0.13 sec
</td></tr>
    <tr>
      <td>
97,490
</td>
      <td>
150,234
</td>
      <td>
15.2 sec
</td>
      <td>
0.20 sec
</td></tr>
    <tr>
      <td>
109,709
</td>
      <td>
168,898
</td>
      <td>
21.8 sec
</td>
      <td>
0.32 sec
</td></tr>
    <tr>
      <td>
185,705
</td>
      <td>
274,919
</td>
      <td>
19.7 sec
</td>
      <td>
0.31 sec
</td></tr>
    <tr>
      <td>
190,476
</td>
      <td>
292,376
</td>
      <td>
29.2 sec
</td>
      <td>
0.56 sec
</td></tr>
    <tr>
      <td>
376,126
</td>
      <td>
557,933
</td>
      <td>
43.0 sec
</td>
      <td>
1.11 sec
</td></tr>
    <tr>
      <td>
675,017
</td>
      <td>
982,804
</td>
      <td>
58.9 secs
</td>
      <td>
1.19 sec
</td></tr>
    <tr>
      <td>
2,100,312
</td>
      <td>
3,012,488
</td>
      <td>
14.4 min
</td>
      <td>
N/A<sup>a</sup>
</td></tr>
    <tr>
      <td>
22,727,509
</td>
      <td>
40,173,501
</td>
      <td>
12.1 hours
</td>
      <td>
N/A<sup >a,b</sup>
</td></tr>    
  </tbody>
</table>

<p><span style="font-size: 10pt"> <sup>a</sup>No data available<br /> <sup>b</sup>This datapoint is provided only as rough bound, as it was produced on a different, much more powerful machine than all the others. </span></p>
<p>As we can see, Stinger throws down with the best of them. In our tests it was clear that it performed a lot better. Unfortunately, it can only handle graphs of up to a predefined number of vertices (which is very small by default). Titan, on the other hand, while around an order of magnitude slower (using default settings and transaction parameters), was able to handle graphs with no apparent limit on vertex or edge count. Obviously, there are several factors that could explain the performance difference we observed on graphs of comparable size. One reason for this difference is the fact that Stinger operates in memory while Titan is a disk-based transactional database – although there are opportunities to tune those transactions. Other reasons include the fact that Stinger is written in C, whereas Titan uses Java (which could add to the overall performance slowdown).</p>
<p>Regardless, we’ll probably end up choosing to build on Titan, for a variety of reasons apart from performance. First, it’s ACID compliant, whereas Stinger isn’t. Second, of the two projects it’s much more robust and production-ready. It’s also better documented and more actively supported and contributed to. Finally, it has much better support for ad hoc querying and integration with other analytical tools, such as the <a href="http://www.tinkerpop.com/">TinkerPop</a> stack, and the powerful visualization platform <a href="http://gephi.github.io/">Gephi</a>.</p>
<h3 id="so-whats-next"><strong>So what’s next?</strong></h3>
<p>We’re excited about the “triggering” scenario we described above. The ability to spot patterns that emerge as the graph updates, and take actions based on those patterns holds a lot of promise for various application areas. Business rules engines, for instance, do exactly this but with relationally structured data rather than graphs; alternately, being able to annotate models of the human transcriptome with new findings and have a machine notify scientists when patterns emerge that they’re interested in (e.g., This gene that increases connectivity in the fusiform gyrus when knocked out has mutant variations that correlate lower expressions of these other genes that influence height. Yes, I did just make that up.).</p>
<p>Thus far, we’ve only taken a preliminary look at this problem. We’ve simplified our patterns to be fixed length “chains” where each link specifies a predicate that the corresponding node or edge in a potential matching path must satisfy. Even in this case, the problem is pretty tough. Even simple indexing approaches run into problems like high memory requirements – if you’re caching partial paths as they occur – or the substantial time complexity of the <a href="http://www.dis.uniroma1.it/~demetres/docs/dapsp-full.pdf">dynamic all points shortest path problem</a> – if you want to maintain information about how many hops you have to travel to get to the nearest node satisfying the next predicate in the chain (the first approach would be easy if you knew that the paths were always very frequent or very infrequent, but lacking such information we’re stuck with the worst case for now). Some approaches we’re looking into are smarter caching of subpaths, or employing <a href="https://crypto.stanford.edu/pbc/notes/zdd/">zero-suppressed binary decision diagrams</a>, which have previously been used to count paths in graphs. But it’s early days so far.</p>
<p>If we’re lucky, and have a successful outcome, we’re hoping to help one or more open source projects implement and adopt a really efficient engine for doing these sorts of analyses and ideally, dynamic graph analytics in general. We think this is going to be a huge development in analytics, and can’t wait to see what the community builds on top of the ability to see how the world is changing.</p>
<p>Thanks for tuning into another exciting episode of the Lab41 blog. See you next time!</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Circulo: A Community Detection Evaluation Framework]]></title>
    <link href="http://lab41.github.io/blog/2015/02/04/circulo-community-detection/"/>
    <updated>2015-02-04T09:00:00+00:00</updated>
    <id>http://lab41.github.io/blog/2015/02/04/circulo-community-detection</id>
    <content type="html"><![CDATA[<div class="row-fluid" style="margin-bottom: 40px">
<div class="span3" style="padding-top: 5px">
<img src="http://lab41.github.io/images/post_9_circulo/logo_circulo.png" title="Circulo Community Detection Framework" >
</div>
<div class="span9">
<pre><code>Lab41 recently released &lt;a href=&quot;https://github.com/lab41/circulo&quot; target=&quot;_blank&quot;&gt;Circulo&lt;/a&gt;, a python framework that evaluates *Community Detection Algorithms*.  As background, a community detection algorithm partitions a *graph* containing vertices and edges into clusters or communities.  Vertices can be entities such as people, places or things; while the relationships among these entities are the edges.  Behind the scenes, community detection algorithms use these relationships (edges), their strength (edge weights), and their direction (directedness) to conduct the partitioning. The partitioning is significant because it often can provide valuable insight into the underlying raw data, revealing information such as community organizational structure, important relationships, and influential entities.</code></pre>
</div>
</div>
<p>Circulo becomes especially important in circumstances where community detection algorithms fail to present clear and consistent results. One of the more prominent examples of this is the case where algorithms executed against the same dataset produce variable results relative to membership, size, execution time, or number of communities. For example, ten different algorithms can produce ten different results, all at different rates. This level of variation puts a researcher into the difficult position of having to choose among the results, without much guidance as to which of the results most accurately applies to the circumstances. Can varying results combine to form a global best result? Does the type of input data affect which algorithm to use? If an algorithm takes too long to execute, is using a fast algorithm sufficient? Do different definitions of a <em>community</em> determine the algorithm to use? Is there such thing as a best result?</p>
<p><em>Circulo enables researchers to try and answer these questions by giving them an efficient platform to conduct data collection, analysis, and experimentation</em>. The framework calculates a variety of quantitative metrics on each resulting community. It can validate a partitioning against some predefined ground truth or can compare two different partitions to each other. This data can be used to draw conclusions about algorithm performance and efficacy. And best of all, it is completely modular so that a researcher can add new algorithms, metrics, and experiments with ease.</p>
<p>To help explain the Circulo framework, we will use the flight routes data <a href="http://sourceforge.net/p/openflights/code/HEAD/tree/openflights" target="_blank">obtained</a> by <a href="http://openflights.org" target="_blank">openflights.org</a>. This dataset is one of 14 that we use for testing in Circulo. In this example, the airports are nodes, and the routes between airports are the edges. The resulting graph is both directed (since flights travel in a direction from one airport to another) and multi-edged (since numerous routes may exist from one airport to another) and contains 3,255 vertices and 67,614 edges. The reasons to employ community detection against this type of data could range anywhere from an airline debating where to build its next hub, to trying to identify a new route to an underserved region, or to developing a plan to reroute regional airport traffic to a different hub. A clearer understanding of how flight routes can divide airports into clusters could lead to better informed decisions.</p>
<h2 id="the-circulo-data-pipeline">The Circulo Data Pipeline</h2>
<p>Circulo execution can be divided into a three stage pipeline. Generally speaking, the inputs to the pipeline are a collection of algorithms and a collection of datasets. What comes out are JSON encoded files containing numeric metric results. Each algorithm/data pair produces a <em>partitioning</em>, and each <em>partitioning</em> produces a set of <em>metrics</em>. The first stage is the <strong>Data Ingestion</strong> stage, where raw data is extracted, transformed, and loaded (ETL) into a graph and serialized into a GraphML file. The second stage is the <strong>Algorithm Execution</strong> stage, where one or more algorithms are executed against the graph. And finally, the third stage is the <strong>Metric Execution</strong> stage, where the results of the previous stage are evaluated against a variety of metrics.</p>
<h3 id="stage-1-data-ingestion">Stage 1: Data Ingestion</h3>
<p>The primary purpose of the <em>Data Ingestion</em> stage is to provide the remaining two stages with a consistent, known graph input format. In many ways, this stage therefore serves as a tool to convert any raw data format into the expected input format of downstream stages. To accomplish this, a researcher needs to subclass the provided <a href="https://github.com/Lab41/Circulo/blob/master/circulo/data/databot.py" target="_blank">CirculoData base class</a>. We have chosen <em>igraph</em> as the primary framework for representing a graph in memory, and <em>graphML</em> as the primary serialization format for storing the graph to disk. Both <em>igraph</em> and <em>graphML</em> were selected for the following reasons:</p>
<ul style="list-style-type: disc">
  <li>
<em>igraph</em> offered a few key benefits that made it an attractive choice. First and foremost, in addition to the standard vertex and edge manipulation typically provided in a graph framework, <em>igraph</em> also provides a suite of community detection algorithms. That means that not only can we leverage these algorithms out of the box, but also that <em>igraph</em> already has a notion of many of the key data structures and functions one would want when creating an evaluation framework. For example, <em>igraph</em> includes a Dendrogram for divisive results, a VertexClustering for non-overlapping results, and a VertexCover for overlapping results. At a lower level, a vertex <em>membership array</em> maintains which vertices belong to which communities. <em>igraph</em> also provides many evaluation functions such as cohesion, degree distribution, and triad_census to name a few. <em>igraph</em>, which is written in C, also has decent performance, is well-documented, and provides wrappers for both <em>Python</em> and <em>R</em>.
</li>
  <li>
<em>graphML</em> is a widely used graph serialization format, enabling Circulo to be compatible with numerous other graph related technologies. <em>graphML</em> is also written in XML which is a standard serialization format, making it reliable and easy to parse using any number of third party tools.
</li>
</ul>

<p>For the flights data, the <em>Data Ingestion</em> stage begins with the execution of functionality provided by the <a href="https://github.com/Lab41/Circulo/blob/master/circulo/data/flights/run.py" target="_blank">FlightsData class</a>. Each new dataset must subclass the CirculoData class as we do with FlightsData, which provides the base functionality to download the data, convert it into a graph, identify ground truth from labels when available, then serialize the graph as graphML. The raw data for flights includes two CSV files:</p>
<ul style="list-style-type: disc">
  <li>
<code>flights.csv</code> (e.g. “Goroka”,“Goroka”,“Papua New Guinea”,“GKA”,“AYGA”,-6.081689,145.391881,5282,10,“U”,“Pacific/Port_Moresby”)
</li>
  <li>
<code>routes.csv</code> (e.g. “AF,137,ATL,3682,ILM,3845,Y,0,CRJ 319”)
</li>
</ul>

<p>Below are both the vertex (node) and edge representations of the previous CSV lines from flights and routes in <em>graphML</em>:</p>
<h4 id="graphml-airport-example">GraphML (Airport Example)</h4>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>  <span class="nt">&lt;node</span> <span class="na">id=</span><span class="s">&quot;n0&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_ICAO&quot;</span><span class="nt">&gt;</span>&quot;AYGA&quot;<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_airport_name&quot;</span><span class="nt">&gt;</span>&quot;Goroka&quot;<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_DST&quot;</span><span class="nt">&gt;</span>&quot;U&quot;<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_city&quot;</span><span class="nt">&gt;</span>&quot;Goroka&quot;<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_name&quot;</span><span class="nt">&gt;</span>1<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_country&quot;</span><span class="nt">&gt;</span>&quot;Papua New Guinea&quot;<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_latitude&quot;</span><span class="nt">&gt;</span>-6.081689<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_IATA/FAA&quot;</span><span class="nt">&gt;</span>&quot;GKA&quot;<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_timezone&quot;</span><span class="nt">&gt;</span>10<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_altitude&quot;</span><span class="nt">&gt;</span>5282<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;v_longitude&quot;</span><span class="nt">&gt;</span>145.391881<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/node&gt;</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<h4 id="graphml-route-example">GraphML (Route Example)</h4>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>  <span class="nt">&lt;edge</span> <span class="na">source=</span><span class="s">&quot;n1797&quot;</span> <span class="na">target=</span><span class="s">&quot;n1878&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_equipment&quot;</span><span class="nt">&gt;</span>CRJ 319<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_dest_id&quot;</span><span class="nt">&gt;</span>3845<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_source_id&quot;</span><span class="nt">&gt;</span>3682<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_codeshare&quot;</span><span class="nt">&gt;</span>Y<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_source_airport&quot;</span><span class="nt">&gt;</span>ATL<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_airline_id&quot;</span><span class="nt">&gt;</span>3090<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_stops&quot;</span><span class="nt">&gt;</span>0<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_airline&quot;</span><span class="nt">&gt;</span>KL<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>    <span class="nt">&lt;data</span> <span class="na">key=</span><span class="s">&quot;e_dest_airport&quot;</span><span class="nt">&gt;</span>ILM<span class="nt">&lt;/data&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/edge&gt;</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>By default, Circulo includes 14 datasets to enable a researcher to quickly evaluate community detection algorithms out of the box. The project contains <a href="https://github.com/Lab41/Circulo/tree/master/circulo/data" target="_blank">information</a> about these default datasets, including how to add additional datasets.</p>
<table class="table-lab41" style="margin-bottom: 40px">
  <tbody>
    <tr>
      <td>
<strong>Dataset</strong>
</td><td>
<strong>Description</strong>
</td>
    </tr>
    <tr>
      <td>
amazon
</td><td>
Co-purchasing Data – <a href="http://snap.stanford.edu/data/bigdata/communities" target="_blank" rel="nofollow">http://snap.stanford.edu/data/bigdata/communities</a>
</td>
    </tr>
    <tr>
      <td>
house_voting
</td><td>
2014 congress (house) voting data – <a href="https://www.govtrack.us/developers/data" target="_blank" rel="nofollow">https://www.govtrack.us/developers/data</a>
</td>
    </tr>
    <tr>
      <td>
flights
</td><td>
Flight Route Data – <a href="http://openflights.org/data.html" target="_blank" rel="nofollow">http://openflights.org/data.html</a>
</td>
    </tr>
    <tr>
      <td>
football
</td><td>
NCAA D1A games - <a href="http://www-personal.umich.edu/%7Emejn/netdata/football.zip" target="_blank" rel="nofollow">http://www-personal.umich.edu/~mejn/netdata</a>
</td>
    </tr>
    <tr>
      <td>
karate
</td><td>
Famous data set of Zachary’s karate club - <a href="http://www-personal.umich.edu/%7Emejn/netdata/karate.zip" target="_blank" rel="nofollow">http://www-personal.umich.edu/~mejn/netdata/</a>
</td>
    </tr>
    <tr>
      <td>
malaria
</td><td>
Amino acids in malaria parasite – <a href="http://danlarremore.com/bipartiteSBM/malariaData.zip" target="_blank" rel="nofollow">http://danlarremore.com/bipartiteSBM</a>
</td>
    </tr>
    <tr>
      <td>
nba_schedule
</td><td>
Games played in the 2013-2014 NBA season – <a href="https://github.com/davewalk/2013-2014-nba-schedule" target="_blank" rel="nofollow">https://github.com/davewalk/2013-2014-nba-schedule</a>
</td>
    </tr>
    <tr>
      <td>
netscience
</td><td>
Graph of collaborators on papers about network science – <a href="http://www-personal.umich.edu/%7Emejn/netdata/netscience.zip" target="_blank" rel="nofollow">http://www-personal.umich.edu/~mejn/netdata/</a>
</td>
    </tr>
    <tr>
      <td>
pgp
</td><td>
Interactions in pretty good privacy – <a href="http://deim.urv.cat/%7Ealexandre.arenas/data/xarxes/PGP.zip" target="_blank" rel="nofollow">http://deim.urv.cat/~alexandre.arenas/data/xarxes/</a>
</td>
    </tr>
    <tr>
      <td>
revolution
</td><td>
Graph representing colonial American dissidents – <a href="https://github.com/kjhealy/revere.git" target="_blank" rel="nofollow">https://github.com/kjhealy/revere.git</a>
</td>
    </tr>
    <tr>
      <td>
school
</td><td>
Face-to-face interactions in a primary school – <a href="http://www.sociopatterns.org/datasets/primary-school-cumulative-networks/" target="_blank" rel="nofollow">http://www.sociopatterns.org/datasets/primary-school-cumulative-networks/</a>
</td>
    </tr>
    <tr>
      <td>
scotus
</td><td>
Supreme court case citation network – <a href="http://jhfowler.ucsd.edu/judicial.htm" target="_blank" rel="nofollow">http://jhfowler.ucsd.edu/judicial.htm</a>
</td>
    </tr>
    <tr>
      <td>
senate_voting
</td><td>
2014 congress (senate) voting data – <a href="https://www.govtrack.us/developers/data" target="_blank" rel="nofollow">https://www.govtrack.us/developers/data</a>
</td>
    </tr>
    <tr>
      <td>
southern_women
</td><td>
bipartite graph of southern women social groups – <a href="http://nexus.igraph.org/api/dataset_info?id=23&amp;format=html" target="_blank" rel="nofollow">http://nexus.igraph.org/api/dataset_info?id=23&amp;format=html</a>
</td>
    </tr>
  </tbody>
</table>


<h3 id="stage-2-algorithm-execution">Stage 2: Algorithm Execution</h3>
<p><a class="fancybox-effects-a"  href=/images/post_9_circulo/algorithm_execution.png><img src="http://lab41.github.io/images/post_9_circulo/algorithm_execution.png" title="Circulo Algorithm Execution" ></a></p>
<p>The second stage of the Circulo pipeline is running the community detection algorithms against each dataset. To run all algorithms against the <em>flights</em> dataset, a researcher would do the following:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>  <span class="n">python3</span> <span class="n">run_algos</span><span class="o">.</span><span class="n">py</span> <span class="n">flights</span> <span class="n">ALL</span> <span class="err">–</span><span class="n">output</span> <span class="n">algorithm_results</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>Circulo will run each algorithm/dataset pair in parallel, serializing the <em>job name</em>, <em>dataset name</em>, <em>iteration</em> (running algorithm/dataset pair multiple times), <em>VertexCover</em> membership array, total <em>elapsed time of execution</em>, and <em>alterations</em> to the filesystem in JSON as shown in the following example:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'> <span class="nt">&quot;algo&quot;</span><span class="p">:</span> <span class="s2">&quot;bigclam&quot;</span><span class="p">,</span>
</span><span class='line'> <span class="nt">&quot;jobname&quot;</span><span class="p">:</span> <span class="s2">&quot;flights—bigclam—0&quot;</span><span class="p">,</span>
</span><span class='line'> <span class="nt">&quot;alterations&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;weighted&quot;</span><span class="p">,</span> <span class="s2">&quot;undirected&quot;</span><span class="p">,</span><span class="s2">&quot;simple&quot;</span><span class="p">],</span>
</span><span class='line'> <span class="nt">&quot;dataset&quot;</span><span class="p">:</span> <span class="s2">&quot;flights&quot;</span><span class="p">,</span>
</span><span class='line'> <span class="nt">&quot;iteration&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span><span class='line'> <span class="nt">&quot;membership&quot;</span><span class="p">:</span> <span class="p">[[</span><span class="mi">56</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">56</span><span class="p">],</span> <span class="p">[</span><span class="mi">28</span><span class="p">],</span> <span class="p">[</span><span class="mi">28</span><span class="p">],</span> <span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">44</span><span class="p">],</span> <span class="p">[</span><span class="mi">28</span><span class="p">],</span> <span class="p">[</span><span class="mi">23</span><span class="p">],</span> <span class="p">[</span><span class="mi">23</span><span class="p">],</span> <span class="p">[</span><span class="mi">23</span><span class="p">],</span> <span class="p">[</span><span class="mi">102</span><span class="p">],</span> <span class="err">...</span> <span class="p">],</span>
</span><span class='line'> <span class="nt">&quot;elapsed&quot;</span><span class="p">:</span> <span class="mf">17.63929653167724</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>The <em>VertexCover</em> membership array above indicates through 0-based indexing that vertex 0 belongs to communities 56 and 1, vertex 1 belongs to community 56, vertex 2 belongs to community 28, etc.</p>
<p>Though a researcher could add any algorithm for evaluation, the framework by default comes with 15 algorithms, with implementations from Lab41 (Conga, Congo, Radicchi Strong, Radicchi Weak), <em>igraph</em> (infomap, fastgreedy, Girvan Newman, leading eigenvector, label propagation, walktrap, spinglass, multilevel) and SNAP–Stanford Network Analysis Project (BigClam, Coda, Clauset Newman Moore). More information about each algorithm can be found in the <a href="http://lab41.github.io/survey-community-detection/" target="_blank" rel="nofollow">Lab41 Community Detection Survey</a>.</p>
<p>When an algorithm executes against a dataset, the <em>Algorithm Execution</em> stage first attempts to match that dataset as best as possible to the input parameters of that algorithm. Given that algorithms may vary in how they use weighted, multi, and directed edges, it is necessary to conform a dataset to an algorithm to enable proper execution and maximize algorithm efficacy. In this manner, Circulo operates with a <em>Best Chance</em> methodology when executing algorithms –we provide the algorithm with the best circumstances so that it can have the best chance at finding a solution. This transformation process can sometimes be as simple as changing all directed edges to undirected edges, however in other cases, it can be more difficult. <em>igraph</em> provides all the transformation functionality through the functions <em>simplify</em> and <em>to_undirected</em>, which both have the ability to collapse edges, which is necessary when, for example, simplifying a multi-edge graph.</p>
<p>The flights dataset (directed, multigraph, unweighted) is an excellent example of how these transformations can occur. To illustrate this, we highlight how the flights data will change when encountering the following two algorithms:</p>
<ol type="1">
<li>The <strong>fastgreedy</strong> algorithm accepts an undirected, simple (opposite of multigraph), and weighted graph – basically the opposite of the flights data. To make the dataset compatible, Circulo will first convert all directed edges to undirected edges, collapsing those edge pairs between two nodes into a single edge when each goes in an opposite direction. Because we want to preserve the number of edges that are collapsed, Circulo will first add an edge weight of 1 to each edge in the graph. Therefore, two opposite directed edges between two nodes will collapse into a single undirected edge with a weight of 2. Then finally, Circulo will convert any remaining multi-edges into a single edge by collapsing those edges and summing the weights. The result is an undirected, simple, weighted graph.</li>
<li>The <strong>bigclam</strong> algorithm accepts an undirected, simple, and <em>unweighted</em> graph. Unlike fastgreedy, bigclam cannot rely on edge weights to distinguish between collapsed edges. As a result, a decision must be made to prune or not prior to sending the collapsed graph to the algorithm. Because this logic largely depends on the data, we expect the researcher to provide that logic in the implementation of the CirculoData class. The prune function will be called in one of two circumstances: (1) the graph collapses and the algorithm does not support weights or (2) the graph collapses and becomes nearly complete. The default prune function does nothing. The FlightsData class will prune all edges less than the median + 0.0001. Another issue that arises with the bigclam algorithm is that it has a variety of optional parameters for fine-tuning. To deal with this case, Circulo can pass a data <em>context</em> for each dataset, where the data provider can specify parameters for specific algorithms that it might encounter.</li>
</ol>
<p>Once data is transformed and executed against algorithms, a researcher can already start to experiment with some of the results. The following experiment highlights a distinct difference between Label Propagation and Infomap when applied against the flights dataset. We will use the graph visualization tool Gephi to view the results overlaid onto a map using the geo-coordinates of the airports.. Each color in the figures below represents a community as determined by the respective algorithm (the colors vary between the two figures because gephi randomizes the colors).</p>
<p>The visualization confirms that both algorithms are presenting accurate partitions if one assumes that locality is a valid source of ground truth for flight data. However, if one were to look closely, the results do vary in regards to the degree of locality. For example, Label Propagation treats most of the US and Mexico as a single community while Infomap treats them as separate communities. <em>One could surmise that perhaps Infomap presents a more detailed view of communities whereas Label Propagation a more general one–information valuable when using the algorithms in the future, especially when ground truth such as geo-coordinates is not available</em>.</p>
<div class="row-fluid" style="margin-bottom: 40px">
<div class="span6">
<pre><code>&lt;a class=&quot;fancybox-effects-a&quot;  href=/images/post_9_circulo/label_propagation_flights.jpg&gt;&lt;img src=&quot;/images/post_9_circulo/label_propagation_flights.jpg&quot; title=&quot;Label Propagation Community Detection Results for Flight Data&quot; &gt;&lt;/a&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;small&gt;_Label Propagation Results_&lt;/small&gt;&lt;/p&gt;</code></pre>
</div>
<div class="span6">
<pre><code>&lt;a class=&quot;fancybox-effects-a&quot;  href=/images/post_9_circulo/infomap_flights.jpg&gt;&lt;img src=&quot;/images/post_9_circulo/infomap_flights.jpg&quot; title=&quot;Infomap Community Detection Results for Flight Data&quot; &gt;&lt;/a&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;small&gt;_Infomap Results_&lt;/small&gt;&lt;/p&gt;</code></pre>
</div>
</div>
<h3 id="stage-3-metric-execution">Stage 3: Metric Execution</h3>
<p>Before we proceed, it is important to identify the two major data points known at this point in the data pipeline: the <em>vertex membership</em> produced by the algorithm and the algorithm execution time. With this data alone, a researcher could likely come to various conclusions: (1) the effectiveness of algorithms by comparing resulting memberships with ground truth memberships, (2) the variance of membership results by comparing memberships produced from different algorithms, and (3) the accuracy of algorithms as a function of time by including elapsed execution time. So why probe further into the data? Why not just accept that vertex membership and time are enough?</p>
<p>Generally speaking, if more data is available, there are more opportunities to come to better conclusions. When an algorithm draws a boundary, and a community is formed, the graph <em>does</em> actually change. Yes, it has the same vertices, and yes it has the same edges, but it now has a third element: the communities themselves. Communities interact with other communities. Communities have ecosystems within them. It is not just that a boundary sets apart vertices in a graph, it is also that it redefines how relationships among vertices can collectively be viewed. All of these facets of the communities can provide further insight beyond just the vertex membership.</p>
<p>One notable benefit of metrics is that we can now better define what it means to be a good community. For example, a good community might be one that is isolated from the rest of the graph, a metric known as conductance. Now, we can identify those algorithms that minimize the conductance metric or discover other metrics that correlate with conductance. We could even use individual metrics to help distinguish individual communities amongst themselves. What is the most isolated community? What is the most dense community?</p>
<p>In Circulo, we have identified the following community metrics:</p>
<table class="table-lab41">
  <tbody>
    <tr>
      <td>
Cut Ratio
</td>
      <td>
TLU–Local Clustering CoefficientMax
</td>
      <td>
TLU–Local Clustering CoefficientBiased Kurtosis
</td></tr>
    <tr>
      <td>
Degree StatisticsMax
</td>
      <td>
TLU–Local Clustering CoefficientMedian
</td>
      <td>
Average Out Degree Fraction
</td></tr>
    <tr>
      <td>
Internal Number Edges
</td>
      <td>
Transitivity Undirected (Global Clustering Coefficient)
</td>
      <td>
Diameter
</td></tr>
    <tr>
      <td>
Conductance
</td>
      <td>
Degree StatisticsUnbiased Variance
</td>
      <td>
Separability
</td></tr>
    <tr>
      <td>
Triangle Participation Ratio
</td>
      <td>
Cohesiveness
</td>
      <td>
TLU–Local Clustering CoefficientUnbiased Variance
</td></tr>
    <tr>
      <td>
Fraction over a Median Degree
</td>
      <td>
TLU–Local Clustering CoefficientMin
</td>
      <td>
Degree StatisticsMedian
</td></tr>
    <tr>
      <td>
Degree StatisticsMean
</td>
      <td>
Degree StatisticsSize
</td>
      <td>
Degree StatisticsBiased Kurtosis
</td></tr>
    <tr>
      <td>
TLU–Local Clustering CoefficientSize
</td>
      <td>
Internal Number Nodes
</td>
      <td>
Degree StatisticsBiased Skewness
</td></tr>
    <tr>
      <td>
Density
</td>
      <td>
Flake Out Degree Fraction
</td>
      <td>
Expansion
</td></tr>
    <tr>
      <td>
TLU–Local Clustering CoefficientMean
</td>
      <td>
Maximum Out Degree Fraction
</td>
      <td>
Normalized Cut
</td></tr>
    <tr>
      <td>
Degree StatisticsMin
</td>
      <td>
TLU–Local Clustering CoefficientBiased Skewness
</td>
      <td></td></tr>
  </tbody>
</table>

<p>Descriptions of each of these metrics can be found in the <em>igraph</em> documentation and the paper “<a href="http://cs.stanford.edu/people/jure/pubs/comscore-icdm12.pdf" target="_blank" rel="nofollow">Evaluating Network Communities based on Ground-truth</a>,” by Jaewon Yang and Jure Leskovec.</p>
<p>To run the metrics against a given vertex membership, one would do the following:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>  <span class="n">python3</span> <span class="n">run_metrics</span><span class="o">.</span><span class="n">py</span> <span class="p">[</span><span class="n">algorithm_results_path</span><span class="p">]</span> <span class="p">[</span><span class="n">metric_results_output_path</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>The algorithm_results_path is the directory containing the JSON encoded results from stage 2. The metric_results_output_path is the path to the directory where the JSON encoded metrics results will be saved. For example, by running the metrics suite against the infomap/flights result, the following JSON would be created:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;flights--infomap--0&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;omega&quot;</span><span class="p">:</span> <span class="mf">0.45103078087242837</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;metrics_elapsed&quot;</span><span class="p">:</span> <span class="mf">152.93790674209595</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;membership&quot;</span><span class="p">:</span> <span class="p">[</span><span class="err">Membership</span> <span class="err">array</span><span class="p">],</span>
</span><span class='line'>  <span class="nt">&quot;metrics&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;Average Out Degree Fraction&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>            <span class="nt">&quot;results&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01293</span><span class="p">,</span> <span class="mf">0.03441</span><span class="p">,</span> <span class="err">...</span><span class="p">,</span> <span class="err">metric</span> <span class="err">scores</span> <span class="err">for</span> <span class="err">each</span> <span class="err">community</span><span class="p">],</span>
</span><span class='line'>              <span class="nt">&quot;aggregations&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>                  <span class="nt">&quot;Biased Kurtosis&quot;</span><span class="p">:</span> <span class="mf">8.609365543507755</span><span class="p">,</span>
</span><span class='line'>                  <span class="nt">&quot;Biased Skewness&quot;</span><span class="p">:</span> <span class="mf">2.859077427455205</span><span class="p">,</span>
</span><span class='line'>                  <span class="nt">&quot;Max&quot;</span><span class="p">:</span> <span class="mf">0.19886363636363635</span><span class="p">,</span>
</span><span class='line'>                  <span class="nt">&quot;Mean&quot;</span><span class="p">:</span> <span class="mf">0.01995900385645678</span><span class="p">,</span>
</span><span class='line'>                  <span class="nt">&quot;Median&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span class='line'>                  <span class="nt">&quot;Min&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span class='line'>                  <span class="nt">&quot;Size&quot;</span><span class="p">:</span> <span class="mi">157</span><span class="p">,</span>
</span><span class='line'>                  <span class="nt">&quot;Unbiased Variance&quot;</span><span class="p">:</span> <span class="mf">0.0015095135838776254</span>
</span><span class='line'>              <span class="p">}</span>
</span><span class='line'>      <span class="p">},</span>
</span><span class='line'>    <span class="err">#</span> <span class="err">Repeated</span> <span class="err">for</span> <span class="err">each</span> <span class="err">metric</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nt">&quot;elapsed&quot;</span><span class="p">:</span> <span class="mf">17.639296531677246</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>The <em>omega index</em> is a measurement of how similar the partition is to some predefined ground truth, if available. In the case of the flights data, we used the countries of the airports. The <em>metrics_elapsed</em> is the time for the stage to complete. The <em>membership</em> is a carry over of the membership from the <em>Algorithm Execution</em> stage. The <em>metrics</em> is divided into two sub-sections for each metric: (1) <em>results</em>–the actual score for each community indexed by the community id, and (2) <em>aggregations</em>: aggregations of the results.</p>
<p>Though each metric has the potential to provide a valuable perspective on the resulting communities, we will only focus on conductance and density in detail here for the sake of example. Conductance is the ratio of edges leaving the community to the total number of edges. One could consider conductance as a measure of how much a community <em>conducts</em> its energy to the rest of the graph. Density is a measure of the ratio of edges inside a community to the number of possible edges in a community. Vertices belonging to dense communities will have multiple edges connecting them to other vertices in the community.</p>
<p>Using the flights example once again, a researcher for the airline industry might have concluded that the <em>infomap</em> algorithm tends to find communities with low conductance and high density based on previous analysis of the algorithm against numerous datasets. Because the researcher is trying to find the airline new opportunities for expanding into underserved markets, a <em>good</em> community in this case is one that is isolated from other regions (low conductance) and has high internal traffic (high density). When applying the metrics to the results of infomap/flights, we see the following results:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><figcaption>
<span></span>
</figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="s2">&quot;Density&quot;</span><span class="err">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;aggregations&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;Min&quot;</span><span class="p">:</span> <span class="mf">0.036311094358587766</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Max&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Size&quot;</span><span class="p">:</span> <span class="mi">156</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Biased Kurtosis&quot;</span><span class="p">:</span> <span class="mf">-1.008373328338681</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Biased Skewness&quot;</span><span class="p">:</span> <span class="mf">0.7121628304637683</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Median&quot;</span><span class="p">:</span> <span class="mf">0.3333333333333333</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Unbiased Variance&quot;</span><span class="p">:</span> <span class="mf">0.11103269168799376</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Mean&quot;</span><span class="p">:</span> <span class="mf">0.4512116997206803</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;results&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.058736682966007606</span><span class="p">,</span> <span class="mf">0.036311094358587766</span><span class="p">,</span> <span class="err">...</span><span class="p">,</span> <span class="mf">0.11522048364153628</span><span class="p">,</span> <span class="err">...</span><span class="p">],</span>
</span><span class='line'><span class="p">}</span><span class="err">,</span>
</span><span class='line'><span class="s2">&quot;Conductance&quot;</span><span class="err">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;aggregations&quot;</span><span class="p">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;Min&quot;</span><span class="p">:</span> <span class="mf">0.0002958677142575364</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Max&quot;</span><span class="p">:</span> <span class="mf">0.9808383233532935</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Size&quot;</span><span class="p">:</span> <span class="mi">156</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Biased Kurtosis&quot;</span><span class="p">:</span> <span class="mf">-0.9679934944718234</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Biased Skewness&quot;</span><span class="p">:</span> <span class="mf">-0.55152643311979</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Median&quot;</span><span class="p">:</span> <span class="mf">0.6666666666666666</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Unbiased Variance&quot;</span><span class="p">:</span> <span class="mf">0.0767265835269525</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;Mean&quot;</span><span class="p">:</span> <span class="mf">0.6085926165581683</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="nt">&quot;results&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5116088500409725</span><span class="p">,</span> <span class="mf">0.6683854334514986</span><span class="p">,</span> <span class="mf">0.6622996968384582</span><span class="p">,</span> <span class="err">...</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>We can see that the <em>infomap</em> algorithm discovered 156 communities, with an average density of ~0.45 and an average conductance of ~0.61. The individual scores are documented in the <em>results</em> section, where for instance, the <em>conductance</em> of each of the first 3 communities is approximately 0.51, 0.67, and 0.66. Though the moderate average scores of both conductance and density suggest that, overall, many of the existing airports optimally serve their regions; interestingly, there exists at least one community with a conductance of 0.00029 and at least one community with a density of 1.0. Given that in our example, a researcher is trying to identify underserved regions, the results of <em>infomap</em> might still be worth a more detailed analysis.</p>
<h2 id="whats-next">What’s Next</h2>
<p>The big question that remains is, “What’s Next?” Circulo provides the pipeline for efficiently gathering metrics based on community detection algorithm execution, but where is the value?</p>
<p>The value, we would argue, is hidden in the metrics data. Before, when we asked the questions about determining which algorithms to use in which situations, we really had no place to start aside from crude qualitative observations. Now when researchers ask these questions, they can leverage the Circulo framework to produce quantitative metrics to serve as the impetus for further experimentation. We have started this experimentation on our <a href="https://github.com/Lab41/Circulo/tree/master/experiments" target="_blank" rel="nofollow">Circulo experiments page</a>. From here, we hope to add more algorithms, include more metrics, and build a variety of experiments that will drive a better understanding of community detection and the numerous algorithms that encompass it. We also hope that you can help make this happen by contributing to Circulo in the future.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Beyond Community Detection - RolX]]></title>
    <link href="http://lab41.github.io/blog/2014/12/18/rolx-discovering-individuals-roles-in-a-social-network/"/>
    <updated>2014-12-18T00:00:00+00:00</updated>
    <id>http://lab41.github.io/blog/2014/12/18/rolx-discovering-individuals-roles-in-a-social-network</id>
    <content type="html"><![CDATA[In the <a href="https://github.com/lab41/circulo" target="_blank">Circulo</a> project, Lab41 has researched, compared, and implemented several powerful methods for community detection in large graphs. As part of analyzing tightly-connected nodes within a network, we have also evaluated ways to discover the structural roles of nodes. This goal, role detection, is complementary to community detection and can provide additional insight for several applications and industries. A method of particular interest to Lab41 is described by Henderson, Gallagher, et al. (2012) in their paper, “<a href="http://briangallagher.net/pubs/henderson-etal-kdd2012.pdf" target="_blank">RolX: Structural Role Extraction &amp; Mining in Large Graphs</a>,” as it can reveal roles such as central connectors, bridges between groups, or members of a clique. The following post highlights the intuition and general mechanics of the algorithm, which we <a href="https://github.com/Lab41/Circulo/blob/master/circulo/algorithms/rolx.py" target="_blank">implemented</a> in Python as part of our broader community detection effort. <a class="fancybox-effects-a"  href=/images/post_rolx/role_detection.png><img src="http://lab41.github.io/images/post_rolx/role_detection.png" title="Network analysis using community and role detection" ></a>
<p>
<small><em>Conceptual illustration of network analysis using community and role detection</em></small>
</p>

<h3 id="introduction">Introduction</h3>
<p>Network modeling is an extremely powerful analytic method employed across a massive variety of disciplines. As Robbie mentioned in his <a href="http://lab41.github.io/blog/2014/08/22/exploring-the-congo/" target="_blank">recent post</a>, one of the most useful techniques in the network analysis domain is community detection, which breaks down large networks into smaller component parts. The earliest models of community detection viewed these breakdowns as partitions of the network; however, as our collective understanding has matured, we realized that communities make more sense as organic, unrestricted groupings of vertices that could range anywhere from complete overlap to complete exclusion.</p>
<p>This nuanced understanding of communities has several powerful applications in the real world. For example, we can find different clusters of protein interactions within cells, identify how fans of sports teams relate to each other, or understand the influence of different scientists in their collaborations. However, community detection algorithms produce groups of related nodes without distinguishing them relative to each other, leaving several meaningful real-world questions unanswered without further analysis. Fortunately, this field of research has advanced upon the idea that community structure is not the only construct that lends itself to graph analytics.</p>
Going one step further, roles of individual nodes can also be gleaned from the structure of the graph. Combined with community detection, role detection can add crucial insight when looking for key information such as:
<ul style="list-style-type: disc; margin-left: 1.3em">
  <li>
Who are the most important, most connected figures - the “key influencers”?
</li>
  <li>
Which members are highly connected to multiple close-knit communities, forming “bridges” in the network?
</li>
  <li>
What distinguishes communities with almost no connections to the rest of the graph, from communities deeply embedded inside it?
</li>
</ul>

<p>The analysis required to answer these questions is <em>complementary</em> to community detection – it must identify common “roles” across many communities, finding nodes in the graph with similar structure and function. To perform these calculations, it is possible to do them by hand or by examining the auxiliary structure provided by some community detection algorithms. However, those approaches are often ad hoc and do not scale.</p>
We’d hope for a richer system to complement community detection - a system that, given a graph, does the following:
<ol style="margin-left: 1.3em">
  <li>
Identify nodes that play similar structural roles in the graph.
</li>
  <li>
Give the user a quantitative understanding of how similar these nodes are to each other.
</li>
  <li>
Allow the user to “make sense” of the roles by correlating them with well-understood graph-metrics (such as PageRank value, clustering coefficient, structural hole value, or eccentricity). Such information would make analysis much simpler and much easier to automate.
</li>
</ol>

<p>Lucky for us, Henderson et al. proposed such a system in their 2012 paper “<a href="http://briangallagher.net/pubs/henderson-etal-kdd2012.pdf" target="_blank">RolX: Structural Role Extraction &amp; Mining in Large Graphs</a>.” We’ll discuss the paper’s key details below.</p>
<h3 id="overview-of-the-algorithm">Overview of the Algorithm</h3>
<p>The RolX (pronounced “role-ex”) algorithm is simple in conception, but somewhat more complicated in presentation. The core idea of RolX is the observation that if we gather data about a graph in some linear form (such as a matrix), we can use <em>matrix factorization</em> methods to find structure in the data - and possibly use this to discover corresponding structure in the graph itself.</p>
<h4 id="step-1---getting-the-features">Step 1 - Getting the features</h4>
<p>RolX starts by gathering a wide range of information associated with nodes in the graph. To gather details about the elements of the graph, the authors rely on the discussion of ReFeX (Recursive Feature eXtraction) from their earlier paper, “<a href="http://www.cs.cmu.edu/~lakoglu/pubs/ReFeX.pdf" target="_blank">It’s Who You Know: Graph Mining Using Recursive Structural Features</a>” (2011). ReFeX recursively collects a wide range of structural data about the graph, for example, node-centric parameters such as a node’s degree. A key feature of the recursion is that it captures both global and local structure by looking at successively larger regions of the graph.</p>
<p>Quantifying a graph’s structure typically requires computing an ensemble of complicated structural metrics - some focus on local-to-the-node information, such as a node’s degree, and some are more influenced by global structural parameters, such as a node’s PageRank value. These calculations can be time-consuming for large graphs, as many global metrics often require several passes over the graph structure before they converge or stabilize.</p>
ReFeX proposes a new way to do this, using only three basic metrics per node:
<ol style="margin-left: 1.3em">
  <li>
Its degree
</li>
  <li>
The number of edges connecting its neighbors (its “ego-network interconnectivity”)
</li>
  <li>
The number of edges connecting its neighbors to other parts of the graph (its “ego-network outdegree”)
</li>
</ol>

All three of these metrics are local and easy to measure, requiring no more than traversing the neighborhood of each member of the graph. To ascertain more global structural properties, the ReFeX paper proposes a recursive technique, proceeding as follows:
<ol style="margin-left: 1.3em">
  <li>
Initialize a list <span class="math">\(L\)</span> of metrics with <code>[degree, ego-network-inter, ego-network-out]</code>.
</li>
  <li>
For each node <span class="math">\(v\)</span> in the graph <span class="math">\(G\)</span> and each metric <span class="math">\(m\)</span> in <span class="math">\(L,\)</span> define a new metric <span class="math">\(m&#39;\)</span> to be the sum of <span class="math">\(m(u)\)</span> for each neighbor <span class="math">\(u\)</span> of <span class="math">\(v.\)</span>
</li>
  <li>
Append these metrics to the list <span class="math">\(L.\)</span>
</li>
  <li>
Repeat.
</li>
</ol>

<p>Astute minds may notice this process is not guaranteed to terminate, and indeed it could go on forever. We need a stopping condition. Fortunately, there is an obvious one: the process should terminate when the information ceases to yield more knowledge about the graph’s structure. This stopping condition is accomplished by eliminating columns that closely resemble each other, or are in fact duplicates of one another. First, we need to construct an auxiliary graph where each node represents a single column of the matrix. Next, we connect columns where their values are close for all nodes. We can then use connected-component-finding algorithms to “trim the fat” from the matrix. This process ensures that all columns contribute unique information to our structural understanding of the graph.</p>
<h4 id="step-2---finding-hidden-structure">Step 2 - Finding hidden structure</h4>
<p>Now that we have a giant matrix of data about the graph’s structure, we can begin mining it for insights. At this stage, we have a giant matrix – more than <span class="math">\(2^{10}\)</span> elements for an average-sized graph — whose rows represent nodes of the graph, and whose columns represent the values of the recursive structural metrics computed. Each cell associates a given node with a given metric’s value, which makes the matrix rich in value. The challenge is figuring out how to use this information to get a complete, but concise, description of the graph’s structure. It turns out we can do this, using a technique called nonnegative matrix factorization, or NMF.</p>
<p>NMF is a mathematical strategy that has proven popular in the field of unsupervised machine learning. Its goal is straightforward: given a large matrix, create an approximation of that matrix that is much smaller, but mostly equivalent. It is part of a broader class of algorithms that perform the task of <em>dimensionality reduction</em>, which takes complex data and projects it into a smaller-dimensional space for easier analysis. Given it can enable insight into very large datasets, NMF is useful in a wide variety of contexts, such as modeling document topics or building recommender systems.</p>
<p>In mathematical terms, for an <span class="math">\(m\times n\)</span> matrix, <span class="math">\(V\)</span> factors into an <span class="math">\(m\times r\)</span> matrix <span class="math">\(W\)</span> and a <span class="math">\(r\times n\)</span> matrix <span class="math">\(H\)</span>, where <span class="math">\(r\)</span> is much smaller than <span class="math">\(m\)</span> or <span class="math">\(n\)</span>, so that <span class="math">\(WH \approx V.\)</span> Because a perfect solution to this problem is usually not possible, we must approximate it instead. This approximation requires use of several linear-algebra and numerical analysis techniques.</p>
<a class="fancybox-effects-a"  href=/images/post_rolx/NMF.png><img src="http://lab41.github.io/images/post_rolx/NMF.png" title="The matrices generated by NMF" ></a>
<p>
<small><em>The matrices generated by NMF</em></small>
</p>

<p>In this particular instance, NMF can allow us to break down the massive array of graph metrics into a smaller collection of “roles.” Using this, we may be able to extract meaning from the graph’s structure and use this to find common structural motifs in the graph.</p>
<h4 id="step-3---sensemaking">Step 3 - Sensemaking</h4>
<p>Unfortunately, NMF outputs are not always clear to the naked eye. In fact, they are essentially just more matrices of numbers. Since the statistics on nodes output by ReFeX were somewhat obscure, knowing which roles correspond to which combinations of node statistics does not help us actually understand the <em>meaning</em> of the node roles. To do this, we need to understand <em>how</em> each role corresponds to actual graph metrics, such as the PageRank value of a node, or its degree. To figure this out, we need to essentially perform the inverse of this factorization. Now, we have a matrix <span class="math">\(N\)</span> associating nodes in the graph with graph metrics, and a matrix <span class="math">\(W\)</span> associating nodes with roles. We want to generate matrix <span class="math">\(G\)</span> such that <span class="math">\(W\times G\approx N.\)</span> Doing this is a relatively simple optimization problem.</p>
<h3 id="a-quick-example">A Quick Example</h3>
<p>The following example from Henderson’s 2012 paper shows the fundamental difference between community detection and role discovery. Both graphs represent the same community of scientists who have co-authored scholarly papers.</p>
<a class="fancybox-effects-a"  href=/images/post_rolx/community-vs-roles.png><img src="http://lab41.github.io/images/post_rolx/community-vs-roles.png" title="The matrices generated by NMF" ></a>
<p>
<small><em>Figure 2: Henderson and Gallager illustrate the differences between the Fast Modularity community detection algorithm, on left, and RolX after applying each to a collaboration graph (RolX numbering added by Lab41 to help readers identify roles)</em></small>
</p>

<p>Whereas the graph on the left shows 22 communities, the one on the right shows four roles that crosscut those communities (represented as diamonds, squares, triangles, and circles). Some scientists are central members of networks – they reside within tightly connected clusters representing a specific discipline and influence every researcher in that discipline. Others bridge two or more different communities of researchers – these scientists usually focus on interdisciplinary topics. Some scientists are members of cliques – they are members of a small “star” of researchers all connected to each other, and loosely connected to the rest of the graph. Finally, most scientists are connected to some other researchers in one specific field, but not tightly, and are not the central node in that field.</p>
<h3 id="in-conclusion">In Conclusion</h3>
<p>Hopefully, this whirlwind tour of RolX highlights how it can provide valuable insight from graphs. Combined with community detection algorithms, RolX can help you understand not only which groups are tightly connected, but also how certain nodes play key roles within the network. If you’re interested in learning more, be sure to read Henderson and Gallagher’s original paper, as well as take a look at our RolX <a href="https://github.com/Lab41/Circulo/blob/master/circulo/algorithms/rolx.py" target="_blank">implementation</a> and our broader <a href="https://github.com/lab41/circulo" target="_blank">Circulo</a> project. We also welcome contributors to our project, so please checkout our repo on GitHub and submit whatever issues, fixes, or code you think would help.</p>
<p><small style="color: #aaa">Background image <a href="http://upload.wikimedia.org/wikipedia/commons/d/d6/Fugle,_%C3%B8rns%C3%B8_073.jpg">Self-organization</a> used under <a href="https://creativecommons.org">Creative Commons</a> license</small></p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exploring the CONGO]]></title>
    <link href="http://lab41.github.io/blog/2014/08/22/exploring-the-congo/"/>
    <updated>2014-08-22T12:00:00+00:00</updated>
    <id>http://lab41.github.io/blog/2014/08/22/exploring-the-congo</id>
    <content type="html"><![CDATA[<style>
    ul {
        list-style-type: none;
    }
</style>

<p>Here at Lab41, we don’t just <a href="http://lab41.github.io/blog/2013/06/12/i-see-graphs/">see graphs</a>. We’re also investigating the interesting and useful properties of these graphs. Recently, the lab has been evaluating the applicability of <strong>community detection</strong> algorithms to graphs of varying size and structure. These algorithms attempt to find groups of vertices that seem related to one another, and could therefore be considered <strong>communities</strong> in the larger network.</p>
<p>When a graph can be split into groups of nodes that are densely internally connected, we say that those groups are communities and that the graph has <strong>community structure</strong>. Natural graphs often have this property, and as we’ve <a href="http://lab41.github.io/blog/2013/08/27/stochastic-kronecker-natural-graphs/">mentioned before</a>, natural graphs tend to be the most illuminating.</p>
<p>While there has been a fair amount of research focused on community detection since Michelle Girvan and Mark Newman published their <a href="http://arxiv.org/abs/cond-mat/0308217v1">seminal paper</a> in 2002, we still lack a proper understanding of which algorithm(s) work best for a given graph structure and scale.</p>
<p>There are two classes of community detection algorithms. Some algorithms find overlapping communities, while others partition the graph into distinct, non-overlapping communities. The Girvan-Newman algorithm is the canonical example of the latter group. In this post, we’ll discuss an evolution of the Girvan-Newman algorithm into newer algorithms called CONGA and CONGO, and eventually try to find out whether the structure of a graph impacts CONGO’s performance.</p>
<h2 id="girvan-newman-algorithm">Girvan-Newman Algorithm</h2>
<p>It is a good idea to fully digest Girvan-Newman before delving into derivations such as CONGA and CONGO.</p>
<p>Girvan and Newman introduced an idea called edge betweenness centrality, or <strong>edge betweenness</strong>. The edge betweenness of an edge <span class="math">\(e\)</span> is defined as the number of shortest-paths that pass through <span class="math">\(e\)</span>, normalized by the number of shortest-paths between each pair of vertices. Girvan and Newman argue that edges with high betweenness scores tend to be inter-community edges, because a high edge betweenness hints that the popular edge acts as a bottleneck between two communities. An examination of the inter-community edges in the figure below make this intuition obvious.</p>
<a class="fancybox-effects-a"  href=/images/post_8/gn_communities.png><img src="http://lab41.github.io/images/post_8/gn_communities.png" title="Communities are circled. You can see that the light, inter-community edges have high betweenness scores. Figure from [1]." ></a>
<center> 
<em>Hover over the pictures for more information.</em>
</center>
<p><br /></p>
<p>If we accept that the edge with the highest betweenness tends to divide communities, we see that repeatedly removing that edge might be a good way to partition a network. To do so, the Girvan-Newman algorithm takes the following steps:</p>
<ul>
    <li>
        <ol>
            <li>
Calculate betweenness scores for all edges in the network.
</li>
            <li>
Find the edge with the highest score and remove it from the network.
</li>
            <li>
Recalculate betweenness for all remaining edges.
</li>
            <li>
Repeat from step 2 until all edges have been removed.
</li>
        </ol>
    </li>
</ul>


<p>Natural graphs can grow very large. It’s not uncommon to want to find insight about a graph with millions or even billions of nodes. Consequently, it’s important to be able to compare the expected performance of algorithms without looking at the exact number of machine instructions or even writing code. One way to do this is by leveraging asymptotic (also known as <a href="http://en.wikipedia.org/wiki/Big_O_notation">Big-O</a>) notation. An easy way to think of Big-O notation is to imagine an expression once you’ve eliminated all constants and kept only the largest factor. For example, <span class="math">\(.01x^3 + 950x^2\log x + 3 = O(x^3)\)</span>, since even though <span class="math">\(x^3\)</span>’s constant is the smallest, it is still the dominating factor as <span class="math">\(x\)</span> increases.</p>
<p>If some function <span class="math">\(f(x)\)</span> grows no faster than another function <span class="math">\(g(x)\)</span>, we say that <span class="math">\(f(x) \in O(g(x))\)</span> or <span class="math">\(f(x) = O(g(x))\)</span>. A bit more formally, <span class="math">\(f(x) = O(g(x))\)</span> if and only if there exist constants <span class="math">\(C\)</span> and <span class="math">\(x_0\)</span> such that <span class="math">\(f(x) \le C g(x)\)</span> for all <span class="math">\(x &gt; x_0\)</span>. In other words, no matter how much larger <span class="math">\(f(x)\)</span> is for small values of <span class="math">\(x\)</span>, <span class="math">\(g(x)\)</span> will eventually catch up. Big-O notation is an incredibly useful tool to quickly compare algorithms and find out how much performance depends on the size of the input.</p>
<p><a class="fancybox-effects-a"  href=/images/post_8/big_o_1.png><img src="http://lab41.github.io/images/post_8/big_o_1.png" title="Some examples of functions using Big-O notation. Behavior as n approaches infinity is all that matters. Notice that despite constants, O(2^n) grows faster than O(Cn^2) Figure from science.slc.edu" ></a></p>
<p>On a graph with <span class="math">\(|V|\)</span> vertices and <span class="math">\(|E|\)</span> edges, it would seem that calculating all of the betweenness centralities would require <span class="math">\(O(|E||V|^2)\)</span> time, because shortest paths must be found between all <span class="math">\(|V| \times (|V| - 1) / 2 = O(|V|^2)\)</span> pairs of vertices, each using a <a href="http://en.wikipedia.org/wiki/Breadth-first_search">breadth-first search</a> that costs <span class="math">\(O(|E|)\)</span>. Luckily, <a href="http://www-personal.umich.edu/~mejn/papers/016132.pdf">Newman</a> and <a href="http://www.inf.uni-konstanz.de/algo/publications/b-fabc-01.pdf">Brandes</a> independently describe an <span class="math">\(O(|E||V|)\)</span> algorithm for betweenness centrality that requires a single breadth-first search from each vertex. This shortcut method uses a flow algorithm, which yields the edge betweenness without requiring the shortest-paths.</p>
<p>An algorithm like Girvan-Newman’s that repeatedly divides the graph is known as a divisive algorithm. A divisive algorithm on a graph usually returns a <a href="http://en.wikipedia.org/wiki/Dendrogram">dendrogram</a> – a specialized type of tree. A dendrogram is a memory-efficient data structure that describes the history of the algorithm. It stores a list of the ways small communities merge to make larger ones, until the entire graph is one big community. We can even derive the historical list of divisions that the algorithm made by inspecting the list of merges. Furthermore, a dendrogram can be split at any level to find a single clustering (a set of clusters) that contains the desired number of communities. When the number of communities is known, the dendrogram can easily be split at the appropriate level. When the optimal number of communities is unknown, we use a metric like <a href="http://en.wikipedia.org/wiki/Modularity_(networks)">modularity</a> to determine which clustering is the “best” one.</p>
<p><a class="fancybox-effects-a"  href=/images/post_8/dendro.png><img src="http://lab41.github.io/images/post_8/dendro.png" title="The dendrogram generated by running Girvan-Newman on the famous Zachary's Karate Club graph. The merges that happen near the top, like 33-32 and 29-{33-32} describe closely related communities, while merges further down show communities that the algorithm split apart early." ></a></p>
<p>Ostensibly, the Girvan-Newman algorithm runs in <span class="math">\(O(|E|^2|V|)\)</span> time, since the betweennesses must be recalculated for each edge removal. However, since the betweenness only needs to be recalculated in the component in which an edge has just been removed, the algorithm is much more tractable on graphs with strong community structure that split quickly into several disconnected components.</p>
<p>An example of a graph with strong community structure is the graph of character interactions in Victor Hugo’s <em>Les Miserables</em>. The following figure shows how Girvan-Newman partitions that graph.</p>
<p><a class="fancybox-effects-a"  href=/images/post_8/les_mis.png><img src="http://lab41.github.io/images/post_8/les_mis.png" title="A depiction of the community structure that the Girvan-Newman algorithm finds from a graph of the characters in Les Miserables. From [1]." ></a></p>
<h2 id="overlapping-communities">Overlapping Communities</h2>
<p>While Valjean’s cohorts in <em>Les Mis</em> seem to partition nicely into their own communities, actors in real networks are often members of multiple communities. Most of us belong to multiple social groups. In fact, almost any real-world network has at least some overlapping community structure.</p>
<p>Most existing algorithms partition networks into non-overlapping communities, but there has been a recent push to design an effective overlapping community detection algorithm.</p>
<p>Zachary’s Karate Club is a famous network representing feuding factions at a karate club. Non-overlapping community detection algorithms provide a great deal of insight, but at the cost of forcing each student into a single faction, even if he belongs in multiple. The figure on the left is a partitioning performed by a non-overlapping algorithm like Girvan-Newman, and the figure on the right is a clustering that allows for overlap. Of course, this is a toy example in which we’ve limited the number of communities to two, but it’s not hard to imagine a very complex network with many communities and vertices that belong in several.</p>
<p><a class="fancybox-effects-a"  href=/images/post_8/lapping.png><img src="http://lab41.github.io/images/post_8/lapping.png" title="A comparison of non-overlapping and overlapping result structures. The unfilled nodes in the right figure represent students who are in both communities. From [5] and [6]." ></a></p>
<p>For whatever reason, network scientists have been exclusively naming their overlapping algorithms using acronyms. <a href="http://dl.acm.org/citation.cfm?id=1835907">CODA</a>, <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6729613&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6729613">CESNA</a>, <a href="http://dl.acm.org/citation.cfm?id=2433471">BIGCLAM</a>, and <a href="http://link.springer.com/chapter/10.1007/978-3-540-74976-9_12#page-1">CONGA</a> are all algorithms that attempt to discover overlapping communities in a graph. Today, we’ll briefly explore Steve Gregory’s CONGA, or Cluster Overlap Newman Girvan Algorithm.</p>
<h2 id="conga">CONGA</h2>
<p>In CONGA, Gregory defines a concept called <strong>split betweenness</strong>. Imagine splitting a vertex into two parts, such that each part keeps some of the original edges. Then the split betweenness is the edge betweenness of an imaginary edge between the two new vertices (represented as a dashed line in the figure below).</p>
<p><a class="fancybox-effects-a"  href=/images/post_8/split.png><img src="http://lab41.github.io/images/post_8/split.png" title="The red vertex splits such that edges to subgraphs A, B, and C split from subgraphs C and D. The split betweenness of this split is the edge betweenness of the dashed line." ></a></p>
<p>Since split betweenness of this imaginary edge can be calculated the same way as edge betweenness on a real one, comparing the two values is an entirely legitimate operation. CONGA splits the vertex instead of removing an edge when the maximum split betweenness is greater than the maximum edge betweenness. Vertices can be split repeatedly, so a single vertex in the original graph can end up in an arbitrary number of communities. This property gives us the overlapping community structure that we were looking for.</p>
<p>A naive version of CONGA would simply calculate the split betweenness for every possible split of every vertex. The algorithm would then look like this:</p>
<ul>
    <li>
        <ol>
            <li>
Calculate all edge and split betweenness scores.
</li>
            <li>
                <ol>
                    <li>
If the maximum split betweenness is greater than the maximum edge betweenness, split the vertex at the optimal split.
</li>
                    <li>
Otherwise, delete the edge with the maximum edge betweenness.
</li>
                </ol>
            </li>
            <li>
Recalculate all edge and split betweenness scores.
</li>
            <li>
Repeat from step 2 until all edges have been removed.
</li>
        </ol>
    </li>
</ul>

<p>Each time the graph splits, vertices are assigned to one more community than before. Since we don’t know the optimal number of communities, we have to somehow store the historical community assignments before continuing the algorithm. Because CONGA is a divisive algorithm, we would hope to be able to use a dendrogram to store the results. However, the overlapping structure of the result set means that such a data structure wouldn’t make much sense. Instead, our version of CONGA returns a list of all of the community assignments that the algorithm generates.</p>
<p>This version of CONGA is simple, but it’s also intractable with more than a handful of nodes. To see why, assume that each vertex has <span class="math">\(m\)</span> incident edges. Then we can split each vertex <span class="math">\(2^m\)</span> different ways, since we can choose any subset of edges to be split away to the new vertex, and any set with <span class="math">\(m\)</span> elements has <span class="math">\(2^m\)</span> subsets. Since we have <span class="math">\(|V|\)</span> vertices, we need to calculate <span class="math">\(|V|\times 2^m\)</span> split betweenness scores. Calculating a split betweenness costs <span class="math">\(O(|E||V|)\)</span> operations, so each iteration of the algorithm takes <span class="math">\(O(|E||V|^2 2^m)\)</span> time. Finally, we have to recalculate all split betweennesses each time we remove an edge, yielding a total runtime of <span class="math">\(O(|E|^2|V|^2 2^m)\)</span>. In the worst case, on a connected graph, <span class="math">\(m = O(|V|)\)</span> and <span class="math">\(|E| = O(|V|^2)\)</span>, so we have <span class="math">\(O(|E|^2|V|^2 2^m)=O(|V|^6 2^{|V|})\)</span> operations. Even a single densely connected vertex takes this algorithm into exponential time.</p>
<p>Luckily, there are a few significant improvements that we can make. Split betweenness is clearly bounded above by <strong>vertex betweenness</strong>, the normalized number of paths through some vertex <span class="math">\(v\)</span>, because an imaginary path within a vertex cannot possibly be involved in more shortest-paths than the vertex as a whole. Furthermore, we can calculate vertex betweenness from edge betweenness, since any shortest-paths going through an edge incident to some vertex must also contribute to the betweenness of the vertex itself.</p>
We can calculate vertex betweenness from edge betweenness using the following equation:
<div style="text-align:center; margin: 20px 0 20px 0">
<span class="math">\(VertexBetweenness(v) = \frac{1}{2} \left(\sum\limits_{e \in incident(v)} EdgeBetweenness(e) - (n-1)\right)\)</span>
</div>
<p>In practice, filtering the vertices by vertex betweenness makes a big difference. However, calculating all split betweennesses for even a single vertex can still take exponential time. To fix this, Gregory introduces a greedy algorithm that makes use of yet another metric that he calls <strong>pair betweenness</strong>. While pair betweenness is not too useful by itself, it allows us to calculate split betweenness much faster. Pair betweenness is the normalized number of shortest-paths that travel through some triplet <span class="math">\(u \rightarrow v \rightarrow w\)</span>. For every vertex <span class="math">\(v\)</span> with <span class="math">\(m\)</span> incident edges, there are <span class="math">\(\binom{m}{2}\)</span> pair betweennesses that need to be calculated. If we use the original all-pairs-shortest-paths algorithm, we can calculate the pair betweennesses at the same time as the edge betweennesses, in <span class="math">\(O(|V|^2|E|)\)</span> time (though we’re trying to extend the optimized algorithm to do so in <span class="math">\(O(|V||E|)\)</span>).</p>
<p>We can represent the pair betweennesses of a single vertex <span class="math">\(v\)</span> of degree <span class="math">\(k\)</span> by constructing a <a href="http://en.wikipedia.org/wiki/Clique_(graph_theory)">k-clique</a> where each vertex in the clique represents some neighbor of <span class="math">\(v\)</span>. The weight on each edge <span class="math">\(\{u, w\}\)</span> is the pair betweenness of <span class="math">\(v\)</span> for <span class="math">\(\{u, w\}\)</span> (the normalized number of shortest paths through <span class="math">\(u\rightarrow v \rightarrow w\)</span>).</p>
<p>Gregory explains the greedy algorithm using these four steps:</p>
<ul>
    <li>
        <ol>
            <li>
Choose edge <span class="math">\(\{u,w\}\)</span> with minimum score.
</li>
            <li>
Coalesce <span class="math">\(u\)</span> and <span class="math">\(w\)</span> to a single vertex, <span class="math">\(uw\)</span>.
</li>
            <li>
For each vertex <span class="math">\(x\)</span> in the clique, replace edges <span class="math">\(\{u,x\}\)</span>, score <span class="math">\(b_1\)</span>, and <span class="math">\(\{w,x\}\)</span>, score <span class="math">\(b_2\)</span>, by a new edge <span class="math">\(\{uw,x\}\)</span> with score <span class="math">\(b_1+b_2\)</span>.
</li>
            <li>
Repeat from step 1 <span class="math">\(k-2\)</span> times (in total).
</li>
        </ol>
    </li>
</ul>

<p>We are left with two vertices with one edge connecting them, where the edge weight is the split betweenness and the labels on each remaining vertex specify the split.</p>
<p>This procedure does not guarantee an optimal split, but Gregory asserts that it usually ends up close, and the greedy algorithm is much (much) more efficient. Our implementation is <span class="math">\(O(k^3)\)</span>, but a cleverer one that sorts the betweennesses could potentially use <span class="math">\(O(k^2\log k)\)</span> operations. Compared with <span class="math">\(2^k\)</span>, we’re quite pleased.</p>
<p><a class="fancybox-effects-a"  href=/images/post_8/greedy.png><img src="http://lab41.github.io/images/post_8/greedy.png" title="An example of the greedy split betweenness algorithm. This example shows how we would find max split betweenness of some vertex a with neighbors b, c, d, and e and pair betweennesses specified as edge weights. Figure from [3]." ></a></p>
<p>We can modify CONGA to use the greedy algorithm and to filter by vertex betweenness as follows (steps taken from [3]):</p>
<ul>
    <li>
        <ol>
            <li>
Calculate all edge betweenness scores.
</li>
            <li>
Calculate all vertex betweenness scores, using the equation described above.
</li>
            <li>
Find set of vertices such that each member’s vertex betweenness is greater than the maximum edge betweenness. If the set is empty, remove the edge with maximum betweenness and skip to step 7.
</li>
            <li>
For each member of the set, calculate pair betweennesses of each vertex by examining shortest paths.
</li>
            <li>
Calculate the maximum split betweenness and optimal split using the greedy algorithm outlined above.
</li>
            <li>
Remove edge with maximum edge betweenness or split vertex with maximum split betweenness (if greater).
</li>
            <li>
Recalculate edge betweenness for all remaining edges in same component(s) as removed edge or split vertex.
</li>
            <li>
Repeat from step 2 until no edges remain.
</li>
        </ol>
    </li>
</ul>

<p>Given these two optimizations, we now have an algorithm that runs in <span class="math">\(O(|E|^2|V|)\)</span>. In practice, runtime again depends heavily on the community structure of the graph, and how often vertices need to be split.</p>
<p>CONGA is a nice extension to Girvan-Newman, and it even has a cool name. But even with the optimizations, it is still painfully, brain-meltingly slow. What we really need is a significantly faster algorithm with a slightly cooler name. This brings us to Gregory’s next iteration of the algorithm, which fits into the CONG-esque theme: CONGO, or Cluster-Overlap Newman Girvan <em>Optimized</em>.</p>
<h2 id="congo">CONGO</h2>
<p>CONGA spends almost all of its time calculating edge and pair betweennesses, because it has to calculate all shortest-paths each time we want to recalculate a score. Since almost all contributions to betweenness tend to come from very short shortest-paths, Gregory defines yet another class of betweenness: <strong>local betweenness</strong>.</p>
<p>We can calculate both edge betweenness and pair betweenness by only considering paths no longer than length <span class="math">\(h\)</span>. This is a much faster calculation, since any breadth-first search needs only to traverse to <span class="math">\(h\)</span> levels, rather than to the entire graph. This localization is the essence of CONGO.</p>
<p>When we’re only concerned with shortest-paths less than or equal to length <span class="math">\(h\)</span>, betweenness scores aren’t affected by an edge removal or a vertex split <span class="math">\(h + \epsilon\)</span> away, where <span class="math">\(\epsilon\)</span> is some small distance. This means that we only have to calculate all edge and pair betweenness scores once, then adjust the scores in the neighborhood of the modification every time a change is made.</p>
To formalize that notion, Gregory defines the <span class="math">\(h\)</span>-region of a modification to be the smallest possible subgraph containing all shortest-paths that pass through <span class="math">\(v\)</span> (for a vertex split) or <span class="math">\(e\)</span> (for an edge removal) of length no longer than <span class="math">\(h\)</span>. The <span class="math">\(h\)</span>-region of edge <span class="math">\(e\)</span> that traverses <span class="math">\(\{u, v\}\)</span> is the subgraph with the set of vertices:
<div style="text-align:center; margin: 20px 0 20px 0">
<span class="math">\(\{w : d(u, w) \lt h \vee d(v, w) \lt h\}\)</span>
</div>
where <span class="math">\(d(a, b)\)</span> is the length of the shortest path between <span class="math">\(a\)</span> and <span class="math">\(b\)</span>. Similarly, the <span class="math">\(h\)</span>-region of vertex <span class="math">\(v\)</span> is the subgraph with the set of vertices:
<div style="text-align:center; margin: 20px 0 20px 0">
<span class="math">\(\{w : d(v, w) \le h\}\)</span>
</div>
<p>When we remove an edge or split a vertex, we have to update the betweenness scores in its <span class="math">\(h\)</span>-region. Before we modify the graph, we compute all shortest paths no longer than <span class="math">\(h\)</span> that lie entirely within the region, and subtract those scores from the stored values. Then we can make our modification (remove an edge or split a vertex) and recalculate these local shortest paths and betweennesses. Finally, we add these recalculated scores back. This procedure updates the local betweenness scores without requiring a full traversal of the graph.</p>
<!-- <a class="fancybox-effects-a" href="http://lab41.github.io/images/post_8/h_region.png"><img src="http://lab41.github.io/images/post_8/h_region.png" title="a. Edge {h,i} selected for removal. 2-region of {h,i} is shaded. &#10; b. Shortest paths within region are found and subtracted from betweenness. &#10; c. {h,i} is removed. Shortest paths within region are found and added to betweenness. &#10; From [4]." ></a> &#8211;>

<p>Given our new definition of local betweenness, we modify CONGA into CONGO:</p>
<ul>
    <li>
        <ol>
            <li>
Calculate all shortest paths no longer than <span class="math">\(h\)</span>.
</li>
            <li>
From these shortest paths, calculate initial edge and pair betweenness scores.
</li>
            <li>
Calculate all local vertex betweenness scores.
</li>
            <li>
Find set of vertices such that each member’s vertex betweenness is greater than the maximum edge betweenness. If the set is empty, remove the edge with maximum betweenness and skip to step 7.
</li>
            <li>
For each member of the set, calculate the maximum split betweenness and optimal split using the greedy algorithm outlined above.
</li>
            <li>
Find edge with maximum edge betweenness or vertex with maximum split betweenness (if greater).
</li>
            <li>
                <ol>
                    <li>
Calculate local betweenness in the <span class="math">\(h\)</span>-region of the planned split or removal.
</li>
                    <li>
Subtract the local betweenness scores from those in the <span class="math">\(h\)</span>-region.
</li>
                    <li>
Remove edge or split vertex.
</li>
                    <li>
Recalculate local betweennesses in <span class="math">\(h\)</span>-region.
</li>
                    <li>
Add back the local betweenness scores.
</li>
                </ol>
            </li>
            <li>
Repeat from step 3 until no edges remain.
</li>
        </ol>
    </li>
</ul>

<p>In practice, CONGO yields similar results to CONGA with arbitrary <span class="math">\(h &gt; 1\)</span>. As <span class="math">\(h\)</span> increases, accuracy increases, but not nearly as dramatically as one might expect. <span class="math">\(h=2\)</span> performs very well on most networks, and CONGA rarely performs much better than CONGO with <span class="math">\(h=3\)</span>.</p>
<h2 id="performance">Performance</h2>
<p>We implemented all three algorithms using the Python wrapper of the wonderful open-source graph engine <a href="http://igraph.org/redirect.html">igraph</a>. All of the algorithms are much slower than they could be, especially because we repeatedly calculate all shortest paths in CONGO rather than use the optimized algorithm proposed by Newman or Brandes. In addition, a lot of the core functionality is in unoptimized Python, which doesn’t help runtime.</p>
<p>For a detailed analysis of execution time using a more optimized version of CONGO, see Gregory’s paper for some great graphs. He generates random graphs with known community structure to do his analysis.</p>
<p>But what if we’re not sure if the graph has community structure? Let’s find out if CONGO can be used to determine whether a graph can be divided into reasonable communities.</p>
<p>For this analysis, we used a few classic graphs that we know have community structure, and then generated random graphs with the same number of nodes and edges. We generated these graphs using the <a href="http://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős-Réyni model</a>, where each edge is placed between a pair of nodes randomly and independently of all other edges. To get a semi-accurate runtime, we ran the algorithm on each classic data set five times, taking the median value. Since random graphs can – by definition – have random structure, we generated five different random graphs and took the median value for comparison.</p>
<p>This comparison, albeit somewhat unscientific, provides some insight into the algorithm’s performance.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Graph</strong></th>
<th style="text-align: center;"><strong>Number of vertices</strong></th>
<th style="text-align: center;"><strong>Number of edges</strong></th>
<th style="text-align: center;"><strong>Median runtime (h=2)</strong></th>
<th style="text-align: center;"><strong>Median runtime (h=3)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="http://www-personal.umich.edu/~mejn/netdata/">Zachary’s Karate Club</a></td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">.211 seconds</td>
<td style="text-align: center;">.362 seconds</td>
</tr>
<tr class="even">
<td style="text-align: left;">Random Graph</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">.342 seconds</td>
<td style="text-align: center;">.612 seconds</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="http://www-personal.umich.edu/~mejn/netdata/">American College Football</a></td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">616</td>
<td style="text-align: center;">9.8 seconds</td>
<td style="text-align: center;">34.2 seconds</td>
</tr>
<tr class="even">
<td style="text-align: left;">Random Graph</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">616</td>
<td style="text-align: center;">31.2 seconds</td>
<td style="text-align: center;">138 seconds</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="http://www-personal.umich.edu/~mejn/netdata/">Coauthorships in Network Science</a> (largest component)</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">914</td>
<td style="text-align: center;">8.7 seconds</td>
<td style="text-align: center;">12.4 seconds</td>
</tr>
<tr class="even">
<td style="text-align: left;">Random Graph</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">914</td>
<td style="text-align: center;">17.2 seconds</td>
<td style="text-align: center;">87 seconds</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="http://deim.urv.cat/~alexandre.arenas/data/welcome.htm">PGP</a> (largest component)</td>
<td style="text-align: center;">10680</td>
<td style="text-align: center;">24340</td>
<td style="text-align: center;">12431 seconds</td>
<td style="text-align: center;">23598 seconds</td>
</tr>
<tr class="even">
<td style="text-align: left;">Random Graph</td>
<td style="text-align: center;">10680</td>
<td style="text-align: center;">24340</td>
<td style="text-align: center;">69236 seconds</td>
<td style="text-align: center;">88564 seconds (ran once.)</td>
</tr>
</tbody>
</table>
<p><a class="fancybox-effects-a"  href=/images/post_8/runtime.jpg><img src="http://lab41.github.io/images/post_8/runtime.jpg" title="Graph of the table above. The football graph is much denser than the network science graph, explaining the drop in time." ></a></p>
<p>What’s happening here? Why do graphs with the same “stats” have dramatically different runtimes? Well, random graphs tend not to have obvious community structure. This means that there are rarely obvious splits or removals to perform, and a large percentage of the split betweennesses has to be calculated every iteration. Furthermore, CONGO’s logs for the random graphs reveal an abundance of early splits, which can transform the number of vertices to <span class="math">\(O(|E|)\)</span> before any removals are performed. Graphs with community structure, on the other hand, tend to split early and often, so the algorithm runs on much smaller components. This suggests that CONGO is <em>not</em> a good algorithm to use to check if some network can be split into communities. CONGO’s performance depends heavily on removing edges early, so it’s best to be confident in the community structure of a graph before feeding it willy-nilly into an algorithm of the CONG* persuasion. CONGO’s poor performance on randomized graphs reminds us just how important it is to pick the right algorithm to work with each graph structure.</p>
<h2 id="other-thoughts">Other Thoughts</h2>
<p>CONGO and CONGA would both benefit from parallelization. We could perform the breadth-first search from each node in parallel, asymptotically improving runtime. If we decide to eventually optimize our implementation, parallelization would be a very useful step.</p>
<p>In this blog post, we don’t consider the quality of the communities generated. Many communities don’t have straightforward <a href="http://en.wikipedia.org/wiki/Ground_truth">ground truth</a>, so “grading” the results of each algorithm tends to be very difficult. In fact, our current <a href="http://www.lab41.org/our-process/">challenge</a> at the lab, called Circulo, is focused primarily on determining when a result is a good one. We’ve been working on metrics to determine which algorithms outperform others in various situations. Take a look at Circulo’s <a href="https://github.com/Lab41/Circulo">GitHub page</a> and submit a pull request. We’re always looking for more contributors.</p>
<h2 id="resources">Resources</h2>
<p>To really understand the algorithms described, read the papers and check out the source code for</p>
<ul>
<li>Girvan-Newman: <a href="http://www.pnas.org/content/99/12/7821.full">Paper</a><!-- , [source](TODO when we push to public again) --></li>
<li>CONGA: <a href="http://www.cs.bris.ac.uk/Publications/Papers/2000712.pdf">Paper</a><!-- , [source](TODO when we push to public again) --></li>
<li>CONGO: <a href="http://www.cs.bris.ac.uk/Publications/Papers/2000885.pdf">Paper</a><!-- , [source](TODO when we push to public again) --></li>
</ul>
<p>To experiment for yourself, I recommend trying out <a href="http://igraph.org/redirect.html">igraph</a> and looking into some of the community detection algorithms that they’ve already implemented.</p>
<h2 id="references">References</h2>
<p>[1] Newman, M. E., &amp; Girvan, M. (2004). Finding and evaluating community structure in networks. <em>Physical review E</em>, 69(2), 026113.</p>
<p>[2] Brandes, U. (2001). A faster algorithm for betweenness centrality*. <em>Journal of Mathematical Sociology, 25</em>(2), 163-177.</p>
<p>[3] Gregory, S. (2007). An algorithm to find overlapping community structure in networks. <em>Knowledge discovery in databases: PKDD 2007</em>, 91-102.</p>
<p>[4] Gregory, S. (2008). A fast algorithm to find overlapping communities in networks. <em>Machine Learning and Knowledge Discovery in Databases</em>, 408-423.</p>
<p>[5] Nicosia, V., Mangioni, G., Carchiolo, V., &amp; Malgeri, M. (2009). Extending the definition of modularity to directed graphs with overlapping communities. <em>Journal of Statistical Mechanics: Theory and Experiment, 2009</em>(03), P03024.</p>
<p>[6] Zarei, M., Izadi, D., &amp; Samani, K. A. (2009). Detecting overlapping community structure of networks based on vertex–vertex correlations. <em>Journal of Statistical Mechanics: Theory and Experiment, 2009</em>(11), P11013.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Five Steps to Demonstration and Delivery]]></title>
    <link href="http://lab41.github.io/blog/2014/07/22/five-steps-to-demonstration-and-delivery/"/>
    <updated>2014-07-22T18:57:00+00:00</updated>
    <id>http://lab41.github.io/blog/2014/07/22/five-steps-to-demonstration-and-delivery</id>
    <content type="html"><![CDATA[<div class="entry-content">
<p>
Often times the hardest thing about building open source software is conveying what a project really does. The purpose of a project can easily be lost to any given audience due to a variety of factors, including mismatch of technical depth, misinterpreted jargon, or insufficient explanations. Similar to most places, “seeing is believing” in the software world - actually using something is often the only way to solidify points made around the project. However, crafting a demo has remained difficult and time consuming.
</p>
<p>
For too long, software developers have had few options to provide on-demand product demos, leaving many at the mercy of PowerPoint slides and vague discussions. By combining a few easy habits with open source technologies like <a href="http://www.docker.com">Docker</a>, individuals and enterprises alike can automate the creation of simple, intuitive, and reproducible software demos.
</p>
<p>
This week, our team launched <a href="https://try.lab41.org">try.lab41.org</a> which provides instances of our open source projects so users can kick the tires before committing to spinning up their own version. We encourage you to checkout <a href="https://lab41.github.io/try41/">Try41</a> and let us know what you think. In this post, I’m going to walk through five steps we use at Lab41 to easily create repeatable, on-demand demonstrations of our open source projects.
</p>
<h3 id="step-1-document">
Step 1: Document that ####
</h3>
<p>
The first step toward creating useful demonstrations is to craft multiple layers of documentation <em>at the project level</em>. There are many forms of documentation, from commenting on a single line of code to complete and verbose install instructions and everything in between. Two types we consistently use are markup-generated overviews of code, as well as README-style instructions for first-time installations.
</p>
<p>
One such example can be taken from <a href="http://lab41.github.io/Redwood/">Redwood</a>, a framework we’ve been working on at Lab41 to identify anomalous files. Below is a breakdown of languages used in Redwood and how the number of lines of comments compares to the number of lines of code. As you can see, the bulk of the project was written in Python, and there is nearly a 1-to-1 ratio of comments to lines of code. Not bad.
</p>
<p>
Click to enlarge: <a class="fancybox-effects-a"  href=/images/post_7/image_1.png><img src="http://lab41.github.io/images/post_7/image_1.png" title="Spreadsheet comparing languages and lines of code, comments, and blanks" ></a> <a class="fancybox-effects-a"  href=/images/post_7/image_2.png><img src="http://lab41.github.io/images/post_7/image_2.png" title="Graph of comparing lines of code, comments, and blanks" ></a>
</p>
<blockquote>
    <p>
Charts generated by <a href="https://www.ohloh.net/p/Lab41-Redwood/analyses/latest/languages_summary">Ohloh</a>
</p>
</blockquote>
<p>
While comments are great, it can often be tedious to hunt through the code and discern what a particular function does and how it is intended to behave. Most modern languages allow for a markup that can be used to generate beautiful, intuitive code documentation.
</p>
<p>
Looking at <a href="https://lab41.github.io/Hemlock">Hemlock</a>, another project we’ve spent time on at the Lab, we can see how the markup works in practice.
</p>
<pre class="prettyprint"><code class="language-python">
    """
    This module is the main core of Hemlock and interfaces with and controls the 
    majority of other modules in this package.
····
    Created on 19 August 2013
    @author: Charlie Lewis
    """
····
    from clients.hemlock_base import Hemlock_Base
    from clients.hemlock_debugger import Hemlock_Debugger
    from clients.hemlock_runner import Hemlock_Runner
····
    import hemlock_options_parser
····
    import getpass
    import json
    import MySQLdb as mdb
    import os
    import requests
    import sys
    import texttable as tt
    import time
    import uuid
····
    class Hemlock():
        """
        This class is responsible for driving the API and the core functionality of
        Hemlock.
        """
····
        def __init__(self):
            self.log = Hemlock_Debugger()
            self.HELP_COUNTER = 0
····
        def client_add_schedule(self, args, var_d):
            """
            Adds a specific schedule to a specific client.
····
            :param args: arguments to pass in from API
            :param var_d: dictionary of key/values made from the arguments
            :return: returns a list of the arguments supplied
            """
            arg_d = [
                '--uuid',
                '--schedule_id'
            ]
            return self.check_args(args, arg_d, var_d)
············
</code></pre>
<blockquote>
    <p>
Snippet from <a href="https://github.com/Lab41/Hemlock/blob/master/hemlock/hemlock.py">Hemlock</a>
</p>
</blockquote>
<p>
The red comments enclosed in triple quotes are the markup lines for Python that can be used by tools like <a href="http://sphinx-doc.org/">Sphinx</a> to generate HTML documentation, as seen in the screenshot below.
</p>
<p>
<a class="fancybox-effects-a"  href=/images/post_7/image_3.png><img src="http://lab41.github.io/images/post_7/image_3.png" title="Sphinx Documentation for Hemlock" ></a>
<blockquote>
    <p>
Taken from <a href="http://lab41.github.io/Hemlock/docs/_build/html/index.html">Hemlock’s Documentation</a>
</p>
</blockquote>
<p>
That sort of documentation is great for fellow developers of the project, but what about the rest of us that just want to know how to install the project and get it up and running so that we can actually <strong>use</strong> the awesome tool? For those less familiar users, we at Lab41 ensure our projects always have a solid README to guide end-to-end installation from an outsider’s perspective.
</p>
<p>
Having a well-thought-out README goes a long way and should not only explain the project’s intentions, but also include details like installation, dependencies, quick start, known issues, examples, and so on. Here we have the first page of the README for another project we’ve spent a fair amount of time on, called <a href="https://github.com/Lab41/Dendrite">Dendrite</a>, which provides a way to analyze and share graphs.
</p>
<p align="center">
<a class="fancybox-effects-a"  href=/images/post_7/image_4a.png><img src="http://lab41.github.io/images/post_7/image_4a.png" title="Dendrite README" ></a>
<blockquote>
    <p>
Taken from <a href="http://lab41.github.io/Dendrite">Dendrite’s README</a>
</p>
</blockquote></p>
<p>
There are many ways to document a project, and the more up-to-date and consistent the documentation is, the easier it will be to maintain in the future. More importantly, great documentation will help others get a sense of where the project stands and what it is expected to do.
</p>
<p>
We’ve often heard the saying, “<a href="http://en.wikipedia.org/wiki/Undocumented_feature">It’s not a bug; it’s an undocumented feature!</a>”. The truth, however, is that if it’s not documented, it’s a bug. It may be hard at times to fit things like this into a schedule, but this can sometimes be just as valuable if not more so (user experience, etc.) than the product itself.
</p>

<h3 id="step-2-test">
Step 2: Covering-all Tests with Coveralls
</h3>
<p>
Teams often refactor code - restructure the program - to make it cleaner, less complex, and more intuitive as the project evolves. However refactoring a project can potentially create unpredictable and unstable behavior.
</p>
<p>
To avoid unintended consequences during the process of refactoring, good test coverage of the code base can help give you peace of mind as you rework functions, syntax, formatting, or other general cleanup.
</p>
<p>
Testing is another one of those things, like documentation, that often gets left behind, forgotten, or deemed unimportant. To avoid this common target of neglect, we at Lab41 turn to a popular (and automated) testing framework. Beyond the obvious benefits of having tests that ensure a particular project’s code behaves as intended, we’ve found that tests are a great way to craft reproducible demonstrations that behave exactly as intended.
</p>
<p>
Below we can see the code coverage for several of our projects using <a href="https://coveralls.io/">Coveralls</a>, which we have integrated with Travis CI (we will cover this tool in more depth in step <a href="#step-3-build">3</a>) so that every time a build happens, we can not only ensure that the project builds, but also that the tests pass, automatically.
</p>
<a class="fancybox-effects-a"  href=/images/post_7/image_5a.png><img src="http://lab41.github.io/images/post_7/image_5a.png" title="Lab41 Coveralls Repositories" ></a>
<blockquote>
    <p>
Taken from <a href="https://coveralls.io/r/Lab41/">Coveralls</a>
</p>
</blockquote>
<p>
Here we see that specifically for <a href="https://github.com/Lab41/Hemlock-REST">Hemlock-REST</a>, a RESTful server for the Hemlock project, the test coverage adjusts for most of the commits in the history, indicating that tests are being written alongside the code for the project.
</p>
<a class="fancybox-effects-a"  href=/images/post_7/image_6.png><img src="http://lab41.github.io/images/post_7/image_6.png" title="Hemlock-REST coverage by commits" ></a>
<blockquote>
    <p>
Coverage for <a href="https://github.com/Lab41/Hemlock-REST">Hemlock-REST</a> on <a href="https://coveralls.io/r/Lab41/Hemlock-REST">Coveralls</a>
</p>
</blockquote>
<p>
As you can see, automated testing makes it, er &#8230; automatic to march forward with greater peace of mind and less effort. Another specific reason to write tests is to benefit others who want to contribute to a project. Basically, tests are a nice way to show others the way the project is expected to operate - especially for those who haven’t contributed to the project yet or are not familiar with how everything is designed to work.
</p>
<p>
Unit tests are a great way to get started writing tests that will provide code coverage. Most languages have several different unit testing frameworks, including <a href="http://junit.org">JUnit</a> (Java), <a href="http://cunit.sourceforge.net">CUnit</a> (C), and my personal favorite, <a href="http://pytest.org">py.test</a> (Python). Combine these testing frameworks with tools like Cobertura, CodeCover, or Emma to generate reports on how well the unit tests covered the code in the project. Finally, feed those reports to Coveralls, and you’re left with automated code coverage tied to commit history as the project emerges.
</p>
<p>
In concert with documentation, retaining and maintaining traceable testing for a project preps it nicely for the next step toward delivering demonstration: building.
</p>

<h3 id="step-3-build">
Step 3: Travis the Builder
</h3>
<p>
Project builds are important. Being able to build a project consistently, and furthermore, guarantee that it still builds in the expected manner as the project gets updated and evolves, is paramount to ensuring that the community has a positive experience getting the project up and running on their own.
</p>
<p>
One of the ways we ensure the project builds correctly with every change we make is by using a tool called <a href="https://travis-ci.org/">Travis CI</a> (“CI” refers to Continuous Integration). There are lots of CI solutions out there, but this one integrates nicely with GitHub and supports a large number of languages and services to build and test against.
</p>
<a class="fancybox-effects-a"  href=/images/post_7/image_13.png><img src="http://lab41.github.io/images/post_7/image_13.png" title="erickt pull request" ></a> <a class="fancybox-effects-a"  href=/images/post_7/image_12.png><img src="http://lab41.github.io/images/post_7/image_12.png" title="Travis build passed" ></a>
<blockquote>
    <p>
<a href="https://github.com/Lab41/Dendrite/pull/108">Pull Request</a> by <a href="https://github.com/erickt"><span class="citation" data-cites="erickt">@erickt</span></a> for Dendrite showing green check mark of a passing build
</p>
</blockquote>
<p>
Here we have a sample config file for Travis CI that tells Travis what it needs to build and test in order to verify that the new changes made don’t break any tests or intended build executions. We can set multiple targets; this one builds against both OpenJDK7 and OracleJDK7. We can specify which branches get built (or which ones don’t) as well as have before and after installation steps for things like dependencies and test reports.
</p>
<pre class="prettyprint"><code class="language-bash">
    language: java

    jdk:
        - oraclejdk7
        - openjdk7

    before_install:
        - source ./scripts/ci/dependencies.sh

    install: mvn install

    after_success:
        - mvn cobertura:cobertura coveralls:cobertura

    branches:
        except:
            - gh-pages

    notifications:
        email:
            - charliel@lab41.org

</code></pre>
<blockquote>
    <p>
<a href="https://github.com/Lab41/Dendrite/blob/master/.travis.yml">.travis.yml</a> config file for Travis CI for Dendrite
</p>
</blockquote>
<p> 
That simple config file translates into a nice user interface that shows the progress, logs, and history of all builds for each specific project setup with Travis.
</p>
<a class="fancybox-effects-a"  href=/images/post_7/image_7.png><img src="http://lab41.github.io/images/post_7/image_7.png" title="Lab41 Dendrite Travis-CI" ></a>
<blockquote>
    <p>
Travis CI <a href="https://travis-ci.org/Lab41/Dendrite/">status</a> of Dendrite
</p>
</blockquote>
<a class="fancybox-effects-a"  href=/images/post_7/image_8.png><img src="http://lab41.github.io/images/post_7/image_8.png" title="Lab41 Dendrite build history" ></a>
<blockquote>
    <p>
Travis CI <a href="https://travis-ci.org/Lab41/Dendrite/builds">build history</a> of Dendrite
</p>
</blockquote>
<p>
Each PR (Pull Request) is built and tested against Travis before it gets merged, ensuring that no broken builds end up in Master, or whatever specific branch you’re intending the community use to download and try your project. If the build breaks on the PR, it gives the contributor the opporunity to remedy the error before it gets pushed upstream, which keeps things clean and consistent for everyone.
</p>

<h3 id="step-4-deploy">
Step 4: (more) Trusted Deployments with Tags and Docker
</h3>
A lot of groups jump straight to this step, with the popularized war cry: “Ship it!”
<p align="center">
<a class="fancybox-effects-a"  href=/images/post_7/image_16.png><img src="http://lab41.github.io/images/post_7/image_16.png" title="ship it squirrel" ></a>
<blockquote>
    <p>
<a href="http://shipitsquirrel.github.io/">Ship it! squirrel</a>
</p>
</blockquote></p>
<p>
However, jumping the gun before doing due diligence on steps <a href="#step-1-document">1</a>, <a href="#step-2-test">2</a>, and <a href="#step-3-build">3</a> can lead to unstable builds, irreproducible build errors, and next to impossible troubleshooting. For those of us working with open source projects, this can lead to general frustration for all. And there’s no quicker path to unused open source then when something doesn’t work due to lack of documentation, absense of testing, or unchecked build processes.
</p>
<p>
When you are ready to deploy, there are several great options that vary from generic to specific. Since our projects are all hosted on GitHub we get one deployment path for free: tags.
</p>
<a class="fancybox-effects-a"  href=/images/post_7/image_10.png><img src="http://lab41.github.io/images/post_7/image_10.png" title="hemlock github tags" ></a>
<blockquote>
    <p>
GitHub tags for <a href="https://github.com/Lab41/Hemlock/tags">Hemlock</a>
</p>
</blockquote>
<p>
GitHub allows us to create tags associated at any particular point in the commit history to create a downloadable version of the project at that particular point in time. This can be great for pre-releases, or even more official releases.
</p>
<p align="center">
<a class="fancybox-effects-a"  href=/images/post_7/image_11.png><img src="http://lab41.github.io/images/post_7/image_11.png" title="hemlock github tag 0.1.6" ></a>
<blockquote>
    <p>
Release notes for pre-release <a href="https://github.com/Lab41/Hemlock/releases/tag/0.1.6">0.1.6</a> of Hemlock
</p>
</blockquote></p>
<p>
Tags are very generic, letting one create downloadable source of anything at any given time, leaving the details of how to get it installed and running up to you.
</p>
<p>
Another more specific approach that can be used for deployment is <a href="http://pytest.org">PyPI</a>, a Python specific index for packages that can be automatically downloaded and installed via tools like pip and easy_setup. There are many language specific indices for packages, such as <a href="http://www.cpan.org">CPAN</a> (Perl), <a href="https://rubygems.org">RubyGems</a> (Ruby), and <a href="http://central.sonatype.org">Sonatype</a> (Java).
</p>
<p align="center">
<a class="fancybox-effects-a"  href=/images/post_7/image_9.png><img src="http://lab41.github.io/images/post_7/image_9.png" title="hemlock 0.1.6" ></a>
<blockquote>
    <p>
Hemlock package hosted on <a href="https://pypi.python.org/pypi/hemlock/0.1.6">PyPI</a>
</p>
</blockquote></p>
<p>
Sometimes project deployment requires many moving parts, multiple languages, and is more complex than just a single package. Docker, which we’ll go into in more detail in <a href="#step-5-repeat">step 5</a>, is a fantastic new technology for these complex cases. It provides developers with a simple way to create an environment, based on a simple configuration file, for running one or more processes inside a container. In addition to providing fine-grained resource utilization, this capability moves us faster towards the “build once, ship everywhere” Holy Grail for deploying across multiple machines. Using Docker, we are able to deploy trusted builds of each project that remain synced with GitHub as each project matures and evolves. All the end user has to do is issue a few easy commands to pull down the image and run it; the installation and setup is already baked into the container and ready to go.
</p>
<a class="fancybox-effects-a"  href=/images/post_7/image_15.png><img src="http://lab41.github.io/images/post_7/image_15.png" title="project images on Docker index" ></a>
<blockquote>
    <p>
Lab41 projects deployed on the <a href="https://index.docker.io/search?q=lab41">Docker Index</a>
</p>
</blockquote>

<h3 id="step-5-repeat">
Step 5: Rinse and Repeat the Demo Pipeline
</h3>
<p>
Repeatability is the key to tying together demonstration and deployment. Pretty much every developer has run into the opposite (and unfortunate) situation: for example, having a demo that only runs on a specific laptop becuase of undocumented dependencies, untested hacks, an outdated operating system, or unspecific and mismatched build parameters. These all-too-common unreproducible factors really mean you have a weak prototype, not a demo. A demo should be something that can be shared and reproduced, not a <a href="http://en.wikipedia.org/wiki/Rube_Goldberg_machine">Rube Goldberg machine</a>:
</p>
<p align="center">
<a class="fancybox-effects-a"  href=/images/post_7/image_14.png><img src="http://lab41.github.io/images/post_7/image_14.png" title="rube goldberg" ></a>
</p>
<blockquote>
    <p>
<strong><a href="http://www.lizarum.com/assignments/flash/2007/animation/rube_goldberg.html">Simplified pencil-sharpener</a></strong>
</p>
</blockquote>
<p>
Thanks to <a href="https://www.docker.io/">Docker</a>, we can specify the exact environment(s), all of the required dependencies and their versions, and any other setup required for a given project. Through a simple configuration file, we can be assured that the next time someone builds that Docker container, it will do the exact same thing it did before, regardless of the state of the machine - and without any “gotchas” or undocumented hacks! Once that Docker specification - a Dockerfile - is created and deployed as a trusted build, repeatable demonstration of the project (Redwood in this case) comes as simple as a single command:
<p>
<pre class="prettyprint"><code class="language-bash">docker run –d -P lab41/redwood
</code></pre>
<p>
Below is an example of a Dockerfile for another project Lab41 has been working on called <a href="https://github.com/Lab41/try41">try41</a>.
</p>
<pre class="prettyprint"><code class="language-bash">
    from ubuntu
    MAINTAINER Charlie Lewis <charliel@lab41.org>
····
    ENV REFRESHED_AT 2014-02-14
    RUN sed 's/main$/main universe/' -i /etc/apt/sources.list
    RUN apt-get update
····
    # Keep upstart from complaining
    RUN dpkg-divert --local --rename --add /sbin/initctl
    RUN ln -s /bin/true /sbin/initctl
····
    RUN apt-get install -y git
    RUN apt-get install -y python-setuptools
    RUN easy_install pip
    ADD . /try41
    RUN pip install -r /try41/requirements.txt
    ADD patch/auth.py /usr/local/lib/python2.7/dist-packages/docker/auth/auth.py 
    ADD patch/client.py /usr/local/lib/python2.7/dist-packages/docker/client.py
····
    EXPOSE 5000
····
    WORKDIR /try41
    CMD ["python", "api.py"]
····
</code></pre>
<blockquote>
    <p>
Dockerfile for <a href="https://github.com/Lab41/try41/blob/master/Dockerfile">try41</a>
</p>
</blockquote>

<h3 id="conclusion-content">
Conclusion
</h3>
<p>
You’re now ready to follow these five steps for demonstration and delivery:
<ol>
    <li>
Document: Use tools like <a href="http://sphinx-doc.org">Sphinx</a> and leverage services like <a href="https://pages.github.com">GitHub Pages</a> and <a href="https://www.ohloh.net">Ohloh</a> to give the project more clarity and build a valuable foundation towards easier delivery and documented demonstration.
</li>
    <li>
Test: Get code coverage via <a href="https://coveralls.io">Coveralls</a> and discover the benefits of decisive tests, which by extension, can be used to test both demonstration and delivery.
</li>
    <li>
Build: Pave the road for deployment by using GitHub’s Pull Request system in concert with the Continuous Integration system, <a href="https://travis-ci.org">Travis CI</a>.
</li>
    <li>
Deploy: Employ GitHub tags, services like <a href="https://pypi.python.org">PyPI</a>, and the <a href="https://registry.hub.docker.com">Docker Index</a> to prep your projects for delivery and demonstration.
</li>
    <li>
Repeat: Use Dockerfiles (and allow the projects to be built by Docker) to establish repeatable, consistent, and reliable ways for projects to be demonstrated and delivered without special cases or nasty hacks.
</li>
  </ol>
</p>
<p>
So no more excuses. Get out there and deliver demos for your projects.
</p>
</div>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DockerCon Hackathon]]></title>
    <link href="http://lab41.github.io/blog/2014/07/14/dockercon-hackathon/"/>
    <updated>2014-07-14T18:39:00+00:00</updated>
    <id>http://lab41.github.io/blog/2014/07/14/dockercon-hackathon</id>
    <content type="html"><![CDATA[<div class="entry-content"><p>
Recently, <a href="https://github.com/schvin"><span class="citation" data-cites="schvin">@schvin</span></a> and I participated in <a href="http://www.docker.com/">Docker</a>’s first 24-hour hackathon ahead of the first <a href="http://www.dockercon.com/">DockerCon</a>. We were joined by 98 other tech junkies at the Docker offices, each teaming up to build their own awesome projects in the hopes of winning the highly coveted DockerCon tickets as well as a speaking slot at the conference. <a href="https://github.com/schvin"><span class="citation" data-cites="schvin">@schvin</span></a> and I set off with an idea that spawned at the <a href="http://monitorama.com/">Monitorama</a> conference we attended in May.
</p>
<p>
After seeing a presentation on logging by <a href="https://github.com/torkelo"><span class="citation" data-cites="torkelo">@torkelo</span></a> around <a href="http://grafana.org/">Grafana</a> – which gives you all of the benefits one might get from <a href="http://logstash.net/">Logstash</a> and <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a>, but lets you use <a href="http://graphite.wikidot.com/">Graphite</a> and <a href="https://github.com/etsy/statsd/">StatsD</a> to provide a more timeseries-centric view of your logs – we decided that it would be cool if we could automatically ship logs from a Docker host, and the containers running on that Docker host, straight into Grafana without having to run extra services on the containers or enforce global changes on legacy containers.
</p>
<p>
Here is how we built it:
</p>
<p>
5:15 pm - We started the project at the kickoff of the hackathon with an initial commit to <em>Dockerana</em>.
</p>
<p>
<a class="fancybox-effects-a"  href=/images/post_6/initial.png><img src="http://lab41.github.io/images/post_6/initial.png" title="Initial Commit" ></a>
</p>
<blockquote>
    <p>
Initial commit to <a href="https://github.com/dockerana/dockerana">Dockerana</a>
</p>
</blockquote>
<p>
For our idea to work, we had to overcome a number of hurdles related to three core aspects of Docker’s unique environment. First, we needed to ensure we could extract logs from both the host and its containers without having to modify the containers themselves. Second, in order to use Grafana, we had to incorporate several dependencies all within a Docker environment. Finally, we needed to come up with a standard format to wrap the Docker logs in so that the logs could be fed to Graphite and displayed with Grafana.
</p>
<p>
8:45 pm - Three and a half hours later, we had our initial working prototype: a single Docker container running Grafana and all of its dependencies, gleaning data from the Docker host, and shoving it into Graphite so that it was viewable via Grafana.
</p>
<p>
<a class="fancybox-effects-a"  href=/images/post_6/initial_data.png><img src="http://lab41.github.io/images/post_6/initial_data.png" title="Initial Data" ></a>
</p>
<p>
While it was a good start, there was still a lot of work to be done. The following three tasks required significantly more thought and tinkering:
<ul>
 <li>
Getting log data from the containers themselves, and not just the host;
</li>
 <li>
Breaking out the monolithic Grafana container into a series of integrated components, each in their own containers; and
</li>
 <li>
Bringing it all together by automating the process of spinning up Grafana with its dependencies as well as collecting and aggregating the logs.
</li>
</p>
<p>
For our Grafana setup, we required six processes to be running, which in Dockerland translates to six containers that all know how to properly communicate and share data with each other. Below are the six services, which together work as a cohesive Grafana system:
<ul>
    <li>
Carbon
</li>
    <li>
Elasticsearch
</li>
    <li>
Grafana
</li>
    <li>
Graphite
</li>
    <li>
Nginx
</li>
    <li>
StatsD
</li>
  </ul>
Graphite is comprised of two components, an engine, and a backend. The backend is Carbon which gets fed the logs via StatsD. StatsD is a network daemon that listens for statistics, counters, timers, and events sent over UDP and aggregates it to pluggable backend services, such as Carbon. Graphite, the engine, is being served up through Nginx, an HTTP and reverse proxy server, making it available to Grafana. Elasticsearch allows different Grafana dashboards to be saved and loaded.
</p>
<p>
Here’s a roughly drawn diagram of how all of these technologies are wired up for Dockerana:
</p>
<p>
<a class="fancybox-effects-a"  href=/images/post_6/Image.png><img src="http://lab41.github.io/images/post_6/Image.png" title="Dockerana Diagram" ></a>
</p>
<p>
Here in Dockerland, we can see how to spin up some of the necessary containers and how they are interconnected both through data and communication.
</p>
<pre class="prettyprint"><code>
# spin up carbon container with a volume
docker run -d \
           -p 2004:2004 \
           -p 7002:7002 \
           -v /opt/graphite \
           --name dockerana-carbon dockerana/carbon

# spin up a graphite container and connect the volume from carbon to it
docker run -d \
           --volumes-from dockerana-carbon \
           --name dockerana-graphite dockerana/graphite

# spin up an nginx container and link the networking exposed in graphite to it
docker run -d \
           -p 8080:80 \
           --link dockerana-graphite:dockerana-graphite-link \
           --name dockerana-nginx dockerana/nginx

</code></pre>
<p>
An astute eye might notice that when the Graphite container is spun up there does not appear to be any exposed networking specified for the Nginx container to link to. That is because we are not exposing any networking to the outside. Instead, we are using native Docker linking between containers through the Dockerfile. As you can see in the example below, the <code>EXPOSE</code> command allows those ports (in this case, 8000) to communicate between linked containers without being exposed to the outside world.
</p>
<pre class="prettyprint"><code>
FROM ubuntu:trusty
MAINTAINER Charlie Lewis <charliel@lab41.org>

RUN apt-get -y update
RUN apt-get -y install git \
                       python-django \
                       python-django-tagging \
                       python-simplejson \
                       python-memcache \
                       python-ldap \
                       python-cairo \
                       python-twisted \
                       python-pysqlite2 \
                       python-support \
                       python-pip


# graphite, carbon, and whisper
WORKDIR /usr/local/src
RUN git clone https://github.com/graphite-project/graphite-web.git
RUN git clone https://github.com/graphite-project/carbon.git
RUN git clone https://github.com/graphite-project/whisper.git
RUN cd whisper && git checkout master && python setup.py install
RUN cd carbon && git checkout 0.9.x && python setup.py install
RUN cd graphite-web && git checkout 0.9.x && python check-dependencies.py; python setup.py install

# make use of cache from dockerana/carbon
RUN apt-get -y install gunicorn

RUN mkdir -p /opt/graphite/webapp
WORKDIR /opt/graphite/webapp

ENV GRAPHITE_STORAGE_DIR /opt/graphite/storage
ENV GRAPHITE_CONF_DIR /opt/graphite/conf
ENV PYTHONPATH /opt/graphite/webapp

EXPOSE 8000

CMD ["/usr/bin/gunicorn_django", "-b0.0.0.0:8000", "-w2", "graphite/settings.py"]

</code></pre>
<blockquote>
    <p>
Snippet from <a href="https://github.com/dockerana/dockerana/blob/master/components/graphite/Dockerfile">Graphite Dockerfile</a>
</p>
</blockquote>
<p>
If we then look at the Nginx configuration snippet below, we can see how it is using that link to proxy through the Graphite content to Grafana:
</p>
<pre class="prettyprint"><code>
. . .

http {
  . . .

  server {
    listen 80 default_server;
    server_name _;

    open_log_file_cache max=1000 inactive=20s min_uses=2 valid=1m;

    location / {
        proxy_pass                 http://dockerana-graphite-link:8000;
        proxy_set_header           X-Real-IP   $remote_addr;
        proxy_set_header           X-Forwarded-For  $proxy_add_x_forwarded_for;

. . .

</code></pre>
<blockquote>
    <p>
Snippet from <a href="https://github.com/dockerana/dockerana/blob/master/components/nginx/nginx.conf">Nginx configuration file</a>
</p>
</blockquote>
<p>
With all of those components working nicely, we just needed a process to collect and aggregate logs. This sounds like a good candidate for a container - so that’s exactly what we did. We Dockerized that process as well into a simple container that runs a couple scripts which poll various parts of the Docker host to glean logs not only about the host but also about the containers running on the host.
</p>
<p>
Here is our simple Dockerfile to build the container to do the log collection, where the primary driver is <code><a href="https://github.com/dockerana/dockerana/blob/master/scripts/runner.sh">runner.sh</a></code>:
</p>
<pre class="prettyprint"><code>
FROM ubuntu:trusty
MAINTAINER George Lewis <schvin@schvin.net>

RUN apt-get update
RUN apt-get install -y sysstat make

RUN perl -MCPAN -e 'install Net::Statsd'

ADD scripts/ingest.pl /usr/local/bin/
ADD scripts/loop.pl /usr/local/bin/
ADD scripts/periodic-ingest.sh /usr/local/bin/
ADD scripts/runner.sh /usr/local/bin/

CMD /usr/local/bin/runner.sh
</code></pre>
<blockquote>
    <p>
Snippet from <a href="https://github.com/dockerana/dockerana/blob/master/Dockerfile">Main Dockerfile</a>
</p>
</blockquote>

<p>
Now for the fun part: displaying the data. We built a dashboard that is mostly centered around the events on the Docker host. In the future we hope to add automatic dashboards specific to containers, but there’s only so much two people can do in 24 hours.
</p>
<p>
Here are some screenshots of what the dashboard looks like, all dynamically configurable:
</p>
<p>
<a class="fancybox-effects-a"  href=/images/post_6/dashboard1.png><img src="http://lab41.github.io/images/post_6/dashboard1.png" title="Dockerana Dashboard" ></a>
</p>
<p>
Note that here we can see the virtual network interfaces of each container as well as the host:
</p>
<p>
<a class="fancybox-effects-a"  href=/images/post_6/dashboard2.png><img src="http://lab41.github.io/images/post_6/dashboard2.png" title="Dockerana Dashboard" ></a>
</p>

<p>
Finally, we wanted it to be easy to setup and repeatable by anyone running a Docker host. Below is a screencast showing just how simple it is to get Dockerana up and running:
</p>
<div style="width:600px;">
<script type="text/javascript" src="https://asciinema.org/a/10047.js" id="asciicast-10047" async data-speed="2" data-size="small" data-autoplay=0></script>
</div>
<p>
DockerCon was a great event. We learned a lot, and got to see a lot of other interesting ideas around Docker that other groups worked on. You can find our presentation as well as those from the other groups that participated on the <a href="http://blog.docker.com/2014/07/dockercon-video-dockercon-hackathon-winners/">Docker Blog</a>. We are definitely looking forward to the next hackathon.
</p></div>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git-ting Better Teamwork Around Graph Analytics]]></title>
    <link href="http://lab41.github.io/blog/2014/04/24/gitting-better-teamwork/"/>
    <updated>2014-04-24T12:00:00+00:00</updated>
    <id>http://lab41.github.io/blog/2014/04/24/gitting-better-teamwork</id>
    <content type="html"><![CDATA[<p>As my <a href="http://lab41.github.io/blog/2014/04/08/dendrite-overview">last post</a> described, <a href="https://github.com/Lab41/Dendrite">Dendrite</a> is our open source project to prototype of how teams can use graph storage and analytics within a shared environment. From our perspective, collaborative experimentation is a pressing need for graph analytics. Several companies, such as <a href="http://thinkaurelius.com">Aurelius</a> and <a href="http://graphlab.com">GraphLab</a>, are developing robust and scalable technologies for storage and analytics. These tools are extremely powerful, but in our work, we’ve seen several teams that have workflows revolving around a single graph. And nobody wants to mess with the “Master Graph” when they have to manually transfer data or log in to multiple systems, each of which could modify a conclusion their colleague is about to convey to important people.</p>
<p>We think graph analysis can add even more bang for the buck if tailored for a team environment, where colleagues could each experiment with techniques that could alter the structure of the graph. That way, workflows wouldn’t need to revolve around a single graph and colleagues could divide responsibilities, simultaneously test different theories, and follow intuition towards unknown outcomes. Basically, everyone would benefit from all the things that innovative teams do well.</p>
<p><a class="fancybox-effects-a"  href=/images/post_5/checklist.png><img src="http://lab41.github.io/images/post_5/checklist.png" title="Dendrite Goals" ></a></p>
<p>But how could we prototype such a capability, especially since we’d need to link together multiple storage and analytics technologies? And how does it actually work under the hood to support multiple users, each of whom could be doing different or conflicting things?</p>
<p>Just as we can build off existing open source projects, it helps to build from a common workflow paradigm when thinking about collaborative graph experimentation. To work together, many analysts want to create different versions, track changes such as modifications or calculations, and selectively accept or reject those changes back into a shared version. Yes, what I’m describing is the same <em>Track Changes</em> feature everyone has come to know and love from Microsoft<sup>®</sup> Word<sup>®</sup>. Imagine if several people tried to edit a document without that feature. Basically, everyone trips over each other’s edits, making it painful to review and merge changes. That is exactly what happens with graphs, posing a huge problem for analysts.</p>
<p>To prototype collaboration around graphs, Dendrite borrows features from a technology that we developers use on a daily basis: distributed version control systems (such as <a href="http://git-scm.com">Git</a>). These systems enable teams of engineers to independently modify source code, collaboratively review updates, then selectively accept or reject changes. The paradigm fits so well that we even refer to this aspect of the project as <em>Git for Graphs</em> <sup>®(not really)</sup>. The only problem is that such systems are not designed to handle Big Data-esque structures, so we actually pushed down the path of implementing custom Git-style features in something that could handle the scale and data type.</p>
<p>What scalable data type can inherently store relationships between projects, graphs, and versions? After several design and coding sessions, we developed something that we call a Metagraph. The concept is that Dendrite uses <a href="http://thinkaurelius.github.io/titan">Titan</a>, the scalable database behind its graphs, to store different versions and the associated metadata about each graph (let that sink in: Dendrite uses a graph to store data about graphs). Within the context of each project, users can create, modify, and clone different versions of a graph. They can even carve off a query-defined subset into a new graph for tailored analysis. In practice, these collaboration features support the essential actions of selectively incorporating data and experimenting with different hypotheses:</p>
<p><a class="fancybox-effects-a"  href=/images/post_5/branching.png><img src="http://lab41.github.io/images/post_5/branching.png" title="Versions of a Project" ></a></p>
<p>With this baseline of Metagraph services, Dendrite demonstrates how teams can use different graph versions—optionally configured with a Git-backed change log—for experimentation and a better workflow. <a href="https://github.com/erickt">Erick Tryzelaar</a>, Lab41’s resident “Git whisperer” who designed and built the core of this component, rightly deserves credit for drawing these proven concepts into the graph space.</p>
<p>Like most prototypes, the collaboration features within Dendrite would benefit from a few performance optimizations, especially to decrease the storage footprint of multiple versions. Nevertheless, it is a good foundation upon which we will continue building capabilities for better collaboration in the space. If that sounds like a worthy pursuit, we welcome talented engineers to join (perhaps even by <a href="https://www.iqt.org/software-engineer-big-data-and-advanced-analytics">applying to work at Lab41</a>) or simply <a href="http://www.lab41.org/contact">drop us a line</a> if you know of interesting work in this area.</p>
<p>Stay tuned in the coming weeks for additional posts that describe (perhaps even demo) additional facets of Dendrite and other Lab41 projects.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dendrite: A Technology Stack for Collaborative Kevin Bacon-ing]]></title>
    <link href="http://lab41.github.io/blog/2014/04/08/dendrite-overview/"/>
    <updated>2014-04-08T12:00:00+00:00</updated>
    <id>http://lab41.github.io/blog/2014/04/08/dendrite-overview</id>
    <content type="html"><![CDATA[<p>As <a href="http://lab41.github.io/blog/2013/06/12/i-see-graphs/">mentioned before</a>, we like working with graphs because the mathematical construct inherently captures relationships that matter. But to move beyond theorems and proofs—to effectively use graphs in the real world—we need ways to store and analyze them within a team environment. So how does our <a href="https://github.com/lab41/dendrite">Dendrite open source project</a> address that challenge? In short, it ties together modified versions of leading open source technologies, adds a base capability for graph collaboration, and uses a web interface to drive it all.</p>
<p><img src="http://lab41.github.io/images/post_4/collaboration.png" title="Collaboration around graphs" ></p>
<h3 id="the-prelude">The Prelude</h3>
<p>To understand how we got here, it helps to have a notion of how we work. At its core, <a href="http://www.lab41.org/process">Lab41</a> is a venue for collaboration among talented people from the private sector, academia, and government. Together, we develop prototypes for shared challenges in the Big Data space. Tackling points of overlap can be difficult, but in instances like this, it can be a uniquely effective way to advance capabilities.</p>
<p>Since our Lab is <a href="http://www.lab41.org">mission</a>-driven, we started by examining the problem space of analysts using graph technologies. To put it mildly, <a href="https://www.cia.gov/careers/opportunities/analytical/view-jobs.html">some analysts</a> have to answer <a href="https://www.cia.gov/about-cia/cia-vision-mission-values">very difficult questions</a>. But looking at the problem space alone would have been insufficient. We also took time to consider workflow and communication needs, such as how colleagues can team up to tackle the ever-important <a href="http://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon">Six Degrees of Kevin Bacon</a>.</p>
<p>What we learned is that graph-focused analysts, like most teams in pretty much every industry, did not have a problem with technology alone. The market was already providing access to powerful graph databases, many elegant algorithms had been published through academia and open source, and the tailwinds behind Big Data had delivered robust analytic engines. But there was a core need to combine graph storage and analytic technologies into something a team could collaboratively use. Given the overlap among a variety of groups in different industries, we figured these goals were worth pursuing:</p>
<p><a class="fancybox-effects-a"  href=/images/post_4/goals.png><img src="http://lab41.github.io/images/post_4/goals.png" title="Dendrite Goals" ></a></p>
<h3 id="stacking-storage-analytics-collaboration">Stacking Storage + Analytics + Collaboration</h3>
<p>It’s easy to see that combination of goals requires a full-stack approach. So you’re probably wondering, “What open source technologies did you use, already??” Glad you asked:</p>
<ul>
<li><p><strong>Graph Storage</strong>: The <a href="http://thinkaurelius.com">Aurelius</a> team behind the <a href="http://thinkaurelius.github.io/titan">Titan Distributed Graph Database</a> built an impressive suite of capabilities that enables scale-free storage in either Berkeley DB for small datasets or <a href="https://hbase.apache.org">HBase</a> for horizontally scalable needs.</p></li>
<li><p><strong>Graph Analytics</strong>: <a href="http://graphlab.com">GraphLab</a> is a powerful machine learning engine, which my fellow Lab41 contributor <a href="http://github.com/cglewis">Charlie Lewis</a> managed to execute on graphs from Titan by creatively leveraging its sister project <a href="http://thinkaurelius.github.io/faunus">Faunus</a>. Being a <a href="http://hadoop.apache.org">Hadoop</a>-based analytic engine, Faunus was also a natural fit for extra horsepower, which we rounded off with in-memory calculations using Java’s <a href="http://jung.sourceforge.net">JUNG framework</a>.</p></li>
<li><p><strong>Information Retrieval</strong>: Developers of user-facing analytics must figure out a way to combine deep computational power, which takes time, with the interactivity we’ve come to expect from The Internets. We initially limited <a href="http://www.elasticsearch.org">Elasticsearch</a> to its standard search features, but now use it as the primary store for listing and visualizing both vertices and edges.</p></li>
<li><p><strong>User Interface</strong>: A <a href="http://en.wikipedia.org/wiki/Representational_state_transfer">REST</a>ful webserver (using custom <a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/html/mvc.html">SpringMVC</a> controllers paired with endpoints served through the Titan-compatible <a href="https://github.com/thinkaurelius/titan/wiki/Rexster-Graph-Server">Rexster</a>) followed principles of data-driven modularity. This design enabled us to build both <a href="http://angularjs.org">AngularJS</a> and command-line interfaces while also allowing others to swap a different front end if desired.</p></li>
</ul>
<p><a class="fancybox-effects-a"  href=/images/post_4/components.png><img src="http://lab41.github.io/images/post_4/components.png" title="Dendrite open source components" ></a></p>
<h3 id="moving-forward">Moving Forward</h3>
<p>I could go on into deep technical details, lessons learned, and future directions of the project, but my colleagues and I will save those topics for future posts, including one in the near future on the technical underpinnings of Dendrite’s collaboration features. For now, I’ll close out this overview with a few (hopefully) lasting impressions:</p>
<ul>
<li><p>Initial <a href="http://strata.oreilly.com/2014/02/big-data-solutions-through-the-combination-of-tools.html">feedback</a> seems to validate our thoughts that graph technologies could gain wider adoption through co-integration and development of better workflow tools. We welcome contributors to join <a href="https://github.com/Lab41/Dendrite">our project</a>, but would also appreciate pointers to any work in this space.</p></li>
<li><p>If you really want to nerd out on graph technologies, consider attending GraphLab’s <a href="http://graphlab.com/community/events/conference14.html">annual user conference</a> in July. Our team is slated for an in-depth talk on Dendrite.</p></li>
<li><p>There is still a lot of room for collaboration between the brainpower in academic research, talented commercial and open source engineers, and government partners with some very challenging problems. In our second year, <a href="http://www.lab41.org/process">Lab41</a> aims to continue cultivating our space as a venue for that type of participation. <a href="http://www.lab41.org/contact">Contact us</a> if you’re interested in learning more or getting involved.</p></li>
</ul>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stochastic Kronecker natural graphs]]></title>
    <link href="http://lab41.github.io/blog/2013/08/27/stochastic-kronecker-natural-graphs/"/>
    <updated>2013-08-27T11:44:00+00:00</updated>
    <id>http://lab41.github.io/blog/2013/08/27/stochastic-kronecker-natural-graphs</id>
    <content type="html"><![CDATA[<p>Why are data scientists so obsessed with graphs? It’s because graphs are the best tools we have for modeling the real world. By analyzing the graph representation of a real-world structure, we can glean a variety of insights about it. Graphs that model real-world phenomena are called “natural” graphs, and a great deal of data science focuses on them. However, obtaining natural graphs is hard; it would be nice if we had a way to generate similar-looking graphs without the data-gathering work. Enter the <strong>stochastic Kronecker graph model</strong>: an easy way to generate almost-natural synthetic graphs. This post will give an overview of natural graphs and describe the stochastic Kronecker model of generating graphs.</p>
<div class="row-fluid">
<h3 style="text-align:center">
Kronecker Visualization
</h3>
</div>
<div class="row-fluid" style="margin-bottom:20px">
  <div class="span12" style="font-size: smaller">
The visualizations below demonstrate the contents of this post. Use the interactive controls to easily create almost-natural synthetic graphs from just a few key parameters:
</div>
</div>
<div class="row-fluid">
  <div class="span6">
    <div>
      
Kronecker Initiation:
<ul style="font-size: smaller">
        <li>
<strong>Initiator matrix</strong>: a two-by-two matrix of probabilities that is used to generate the random graph.
<li>
<strong>Number of iterations</strong>: The number of times the matrix is “Kronecker multiplied” by itself. The number of nodes in the generated graph is <span class="math">\(2^n,\)</span> where <span class="math">\(n\)</span> is the number of iterations.
</li>
      </ul>
    </div>
  </div>
  <div class="span6">
    <div id="controller" style="font-size:smaller">
        <div class="row-fluid"> <div>
<strong>Number of iterations:</strong>
<div id="iterationsvalue" style="display:inline">

</div>
</div></div>
        <div class="row-fluid"> <div id="iterationsslider"></div> </div>
        <div class="row-fluid" style="margin-top:5px; margin-bottom:2px"> <div>
<strong>Probabilities in 2x2 initiator matrix:</strong>
</div> </div>
        <div class="row-fluid">
           <div class="span4" style="padding-top:12px"> <div id="a1slider"></div> </div> <div class="span2 matrixval"> <div id="a1value"></div> </div>
           <div style="margin:0" class="span2 matrixval"> <div id="a2value"></div> </div> <div class="span4" style="padding-top:12px"> <div id="a2slider"></div> </div>
        </div>
        <div class="row-fluid">
           <div class="span4" style="padding-top:12px"> <div id="b1slider"></div> </div> <div class="span2 matrixval"> <div id="b1value"></div> </div>
           <div style="margin:0" class="span2 matrixval"> <div id="b2value"></div> </div> <div class="span4" style="padding-top:12px"> <div id="b2slider"></div> </div>
        </div>
        <div class="row-fluid">
          <div style="text-align:center; margin-top:10px" class="span12"> <div id="regenerate"></div> </div>
        </div>
    </div>
  </div>
</div>

<div class="row-fluid">
  <div class="span6">
    <div id="graph"></div>
  </div>
  <div class="span6">
    <div id="matrix"></div>
  </div>
</div>

<h1 id="introduction-to-natural-graphs">Introduction to natural graphs</h1>
<h2 id="background-and-motivation">Background and motivation</h2>
<p>At Lab41, <a href="http://lab41.github.io/blog/2013/06/12/i-see-graphs/">we see graphs everywhere</a>. Much of our work revolves around analyzing and generating natural graphs that have structural properties similar to those found in real-world settings. Such graphs could represent an arrangment of computers in a network, animals in a food chain, or neurons in your brain. Unlike randomly-generated graphs, natural graphs have <em>meaning</em>. For example, characteristics of a system modeled by a graph can be deduced by calculating mathematical metrics such as its nodes’ <em>degree</em> (the number of edges connected to a node in a graph) or the number of triangles formed by its edges.</p>
<p>Working with natural graphs involves a number of challenges:</p>
<ul>
<li><p><strong>Obtaining natural graphs is hard.</strong> One must painstakingly collect a large dataset of real-world observations and connections, find a suitable way to interpret it as a graph, and then actually convert it into a graph - a process that can be tedious and time-consuming.</p></li>
<li><p><strong>Datasets for natural graphs are scarce.</strong> There are only a small number of existing datasets representing natural graphs. In fact, at the recent <a href="http://graphlab.org/graphlab-workshop-2013/">GraphLab workshop</a>, one speaker noted that he was getting tired of every presenter using the same dataset (articles and links between them on <a href="http://www.wikipedia.org/">Wikipedia</a>) for their analyses!</p></li>
<li><p><strong>Synthetic graphs miss the mark.</strong> Graphs randomly generated according to standard models (as my colleague Charlie did in <a href="blog/2013/05/02/zero-to-large/">his previous post</a>, and others have done using the <a href="http://en.wikipedia.org/wiki/Erdos-Renyi_model">Erdos-Renyi graph model</a>) tend to look <em>unnatural</em>, no matter what parameters we use. We can’t just create natural graphs by taking a random number generator and going crazy. Instead, we need to find out what properties make a graph “natural,” and then find a way to effectively and efficiently generate graphs with those properties.</p></li>
</ul>
<h2 id="properties-of-natural-graphs">Properties of natural graphs</h2>
<p>So, what makes a graph “natural”? While there is no hard-and-fast definition, nearly all natural graphs exhibit two simple properties:</p>
<ul>
<li><p><strong>Power-law degree distributions.</strong> A very small number of nodes have a very large number of connections (high degree), while a large number of nodes have a very small number of connections (low degree). Mathematically speaking, this means the degree of any vertex in the graph can be interpreted as a random variable that follows a <a href="http://en.wikipedia.org/wiki/Power-law_distribution">power-law probability distribution.</a></p></li>
<li><p><strong>Self-similarity.</strong> In natural graphs, the large-scale connections between parts of the graph reflect the small-scale connections within these different parts. Such a property also appears within fractals, such as the <a href="http://en.wikipedia.org/wiki/Mandelbrot_set">Mandelbrot</a> or <a href="http://en.wikipedia.org/wiki/Julia_set">Julia</a> sets.</p></li>
</ul>
<p>An accurate mechanism for natural graph generation must preserve these properties. As it turns out, the <a href="http://arxiv.org/abs/0812.4905"><strong>stochastic Kronecker graph model</strong></a> does this. It has a few other advantages as well:</p>
<ul>
<li><p><strong>Parallelism.</strong> The model allows large graphs to be generated at scale via parallel computation.</p></li>
<li><p><strong>Structural summarization.</strong> The model provides a very succinct, yet accurate, way to “summarize” the structural properties of natural graphs. Two Kronecker graphs generated with the same parameters will produce graphs with matching values for common structural metrics, such as degree distribution, diameter, hop number, scree value, and network value.</p></li>
</ul>
<p>The remainder of this blog post will describe the basic Kronecker generation algorithm and how it can be modified to efficiently generate very large graphs via parallel computation, on top of MapReduce and Hadoop.</p>
<h1 id="mathematical-formulation">Mathematical formulation</h1>
<p>The core of the Kronecker generation model is a simple matrix operation called the <em>Kronecker product</em>, an operation on two matrices that “nests” many copies of the second within the first. Since graphs can be represented by adjacency matrices (<a href="http://lab41.github.io/blog/2013/06/12/i-see-graphs/">Karthik’s post</a>), this operation can be generalized to graphs.</p>
<p>Taking the Kronecker product of a graph with itself thus easily produces a new, self-similar graph, as does taking the more general “Kronecker power” of it. In fact, Kronecker powers will have further self-similarity. For example, below you can see an example of a simple three-node graph, its Kronecker cube, and its Kronecker fourth power, with the self-similarity evident in the adjacency matrix.</p>
<p><span class="caption-wrapper"><img class='caption' src='http://lab41.github.io/images/2013-08-27-stochastic-kronecker-natural-graphs/matrix.jpg' width='' height='' title='Fractal patterns visible in the adjecency matrix of a Kronecker graph. Taken from Leskovec et al. (2008).'><span class="caption-text">Fractal patterns visible in the adjecency matrix of a Kronecker graph. Taken from Leskovec et al. (2008).</span></span></p>
<p>Because the Kronecker power so easily generates self-similar graphs, it’s reasonable to consider that it might be similarly effective at generating <em>random</em> natural graphs. To do this, we simply start with an adjacency matrix, but allow <em>probabilities</em> to occupy the cells of the matrix rather than ones and zeros. This gives us the <em>stochastic</em> Kronecker graph model.</p>
<h1 id="algorithms-for-generating-kronecker-graphs">Algorithms for generating Kronecker graphs</h1>
<h2 id="naive-algorithm">Naive algorithm</h2>
<p>The simplest algorithm for generating Kronecker graphs is to use Kronecker powers to generate a stochastic adjacency matrix, and then step through each cell of the matrix, flipping a coin biased by the probability present in that matrix. In more detail, the algorithm is as follows:</p>
<ol type="1">
<li><p>We start with an <span class="math">\(n\)</span> by <span class="math">\(n\)</span> initiator matrix, <span class="math">\(\theta,\)</span> and the number of iterations <span class="math">\(k\)</span> for which we wish to run the algorithm. We compute the <span class="math">\(k\)</span>-th Kronecker power of the matrix <span class="math">\(\theta,\)</span> giving us a large matrix of probabilities, which we call <span class="math">\(P.\)</span> Each cell in this matrix corresponds to an edge between two nodes in the graph; the formula for the value at the <span class="math">\((u,v)\)</span>th cell of <span class="math">\(P\)</span> is: <span class="math">\[\prod_{i=0}^{k-1} \theta\left[\left\lfloor \frac{u}{n^i}\right\rfloor \bmod{n},
    \left\lfloor \frac{v}{n^i}\right\rfloor \bmod{n} \right].\]</span> (For convenience, we have assumed the matrix is zero-indexed, as is common in computer science.)</p></li>
<li><p>To generate the actual graph, we 1) step through each cell in the matrix, 2) take the probability in the cell, 3) flip a coin biased by that probability, and if the coin “comes up heads,” we 4) place the corresponding edge in the graph.</p></li>
</ol>
<p>If the initiator matrix is an <span class="math">\(n\times n\)</span> square matrix, and we perform <span class="math">\(k\)</span> iterations of the Kronecker power operation, the generated matrix will have dimension <span class="math">\(N=n^k.\)</span> We will need to take a product of <span class="math">\(k\)</span> values to obtain each cell of the final matrix, and there will be <span class="math">\(N^2\)</span> cells, so the runtime of this algorithm will be <span class="math">\(O(kN^2).\)</span></p>
<p>This means that if we want to generate a graph with approximately one billion nodes (a reasonable size for a large natural graph) from an initiator matrix of size 2, our runtime expression tells us we should expect to perform approximately <span class="math">\({(30)(10^9)^2 = 3.0\times 10^{19}}\)</span> operations. That’s 30 <em>quintillion</em> operations. This leads us to wonder whether we could do this with fewer operations. Spoiler alert: it’s possible.</p>
<h2 id="fast-algorithm">Fast algorithm</h2>
<p>If we switch from a node-oriented approach to an edge-oriented approach, there does exist a faster algorithm for generating a Kronecker graph. Most natural graphs are sparse - <span class="math">\(E = O(N).\)</span> Thus, if we can find a way to place each <em>edge</em>, one at a time, in the graph, rather than figuring out if a pair of nodes has an edge between them, we can vastly reduce the on-average running time. To do this, we need to figure out how many edges are in the graph, and we need to figure out which nodes are associated with each edge.</p>
<p>It turns out that the expected number of edges in a stochastically generated Kronecker graph is encoded within the initiator matrix itself - it’s given by: <span class="math">\[E = \left(\sum_{i,j} \theta[i,j]\right)^k.\]</span> In general, this works out to being on the order of the number of nodes.</p>
<p>Next, we need to find a procedure that starts from nothing, and in <span class="math">\(k\)</span> iterations picks a new edge in the graph to add. Thankfully, this operation is already staring us in the face - in the formula presented in the previous section. Here it is again: <span class="math">\[\prod_{i=0}^{k-1} \theta\left[\left\lfloor \frac{u}{n^i}\right\rfloor \bmod{n},
\left\lfloor \frac{v}{n^i}\right\rfloor \bmod{n} \right].\]</span> This formula can be understood in a different way - as a “recursive descent” into the adjacency matrix of the graph, picking smaller and smaller blocks of the matrix until we have finally narrowed our choice to a single cell, which we then “color in” to represent that an edge should be placed there.</p>
<p>Thus, to generate a stochastic Kronecker graph, all we need to do is set up a loop which runs <span class="math">\(E\)</span> times, generating a new edge in the graph on each pass-through. (If we generate the same edge twice, we ignore it and repeat the pass-through as if nothing happened.) This runs in <span class="math">\(O(kE)\)</span> time, which means that for sparse, real-world graphs, it runs in <span class="math">\(O(kN)\)</span>.</p>
<h2 id="parallel-algorithm">Parallel algorithm</h2>
<p>This algorithm allows us to generate every edge in the graph independently of every other edge, allowing us to parallelize the graph’s generation. This means we can leverage the power of Hadoop to generate very large graphs.</p>
<p>The only twist is that this method allows for the creation of duplicate edges, and most of the graphs we’re interested in don’t contain such duplicates. Thus, we need to figure out how to identify and eliminate them. This is hard when generating the graph across multiple machines, because it’s very likely the duplicate edges will be generated on separate machines. Fortunately, with a bit of cleverness, we can leverage the nature of MapReduce to do our duplicate checking. Instead of one MapReduce job, we’ll have three - one to generate edges and eliminate duplicates, one to generate vertices, and one to combine the two together to form a single graph. This gives us the workflow below.</p>
<p><span class="caption-wrapper"><img class='caption' src='http://lab41.github.io/images/2013-08-27-stochastic-kronecker-natural-graphs/workflow.png' width='' height='' title='The workflow for Kronecker graph generation. Datatypes appear above the line, sample data below. For convenience, FaunusVertex objects have been represented in JSON and NodeTuple objects by pairs of values between angle brackets.'><span class="caption-text">The workflow for Kronecker graph generation. Datatypes appear above the line, sample data below. For convenience, FaunusVertex objects have been represented in JSON and NodeTuple objects by pairs of values between angle brackets.</span></span></p>
<p>The pipeline consists of three stages:</p>
<ol type="1">
<li><p>The first stage of our pipeline is vertex generation. This is the simplest stage - it is a map-only job, utilizing a custom input format representing a range of vertices to be generated. We use as the key a unique <code>Long</code> identifying the vertex, and a <code>FaunusVertex</code> object as the value, giving us a (<code>Long,FaunusVertex</code>) output sequence file.</p></li>
<li><p>The second stage of our pipeline is edge generation. As with vertex generation, it uses a custom input format representing a quota of edges to place into the graph. For each edge in this quota, we run the fast stochastic Kronecker placement algorithm, yielding a tuple of vertex IDs that represents a directed edge in the graph. This tuple is stored as a custom intermediate key type (called a <code>NodeTuple</code>), with the value as a <code>NullWritable</code>; this allows the shuffling and sorting logic of MapReduce to place identical tuples together, and consequently allows us to easily eliminate duplicate copies of the directed edges before the reduce step. Finally, in our reduce step, we emit a <code>Long,FaunusVertex</code> tuple. The <code>FaunusVertex</code> represents the edge’s source vertex and contains a <code>FaunusEdge</code> indicating its destination vertex. The <code>Long</code> key is the source vertex’s ID.</p></li>
<li><p>The third and final stage of our pipeline reads in the vertex objects generated by both the edge and vertex creators and combines them, creating a final list of <code>FaunusVertexes</code> that represents the graph.</p></li>
</ol>
<p>A few details on the pipeline:</p>
<ul>
<li><p><strong>Faunus.</strong> This pipeline uses the same data types as the <a href="http://faunus.thinkaurelius.com">Faunus</a> engine for graph analytics. Faunus provides objects representing edges (<code>FaunusEdge</code>s) and vertices (<code>FaunusVertex</code>es) that can be serialized and utilized by MapReduce jobs but can also serve as a final representation of a graph. Conveniently, <code>FaunusVertex</code>es can store the edges coming off them as well, so we do not need to store edges separately from vertices in the final graph - we need only store the list of vertices with edges added to them.</p></li>
<li><p><strong><code>SequenceFiles</code>.</strong> This pipeline produces <code>SequenceFiles</code> (a native MapReduce serialization format) consisting of <code>FaunusVertex</code>es to serve as intermediate representations of the graph as we construct it.</p></li>
<li><p><strong>Annotations.</strong> In the final stage, we annotate the vertices with several property values (a mixture of floating-points and strings) in order to mimic the data we are interested in.</p></li>
</ul>
<h1 id="further-reading">Further reading</h1>
<p>We have written a version of this blog post up as an informal paper that can be viewed <a href="http://lab41.github.io/assets/post.pdf">here.</a> It contains a more in-depth explanation of the mathematics behind Kronecker graphs.</p>
<h1 id="references">References</h1>
<ul>
<li>Leskovec, Jure, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani. Kronecker graphs: an approach to modeling networks. <em>ArXiv</em>, <a href="http://arxiv.org/abs/0812.4905">arXiv:0812.4905v2</a></li>
</ul>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I See Graphs]]></title>
    <link href="http://lab41.github.io/blog/2013/06/12/i-see-graphs/"/>
    <updated>2013-06-12T15:17:00+00:00</updated>
    <id>http://lab41.github.io/blog/2013/06/12/i-see-graphs</id>
    <content type="html"><![CDATA[<p>At Lab41 we are obsessed with <a href="http://en.wikipedia.org/wiki/Graph_(mathematics)">graphs</a>. We see graphs everywhere we look, in everything we think about. One of our goals is to better understand and advance techniques for manipulating, storing, and analyzing graphs. In this post, we try and do three things: (1) explain what a graph is, (2) show that it’s an important concept, and (3) discuss a way of working with graphs. More specifically, we talk about how to work with graphs as matrices.</p>
<h3 id="why-are-graphs-important">Why are graphs important?</h3>
<p><img class="right" src="http://lab41.github.io/images/2013-06-12-i-see-graphs/directed_graph.png"> A graph is a mathematical construct that describes things that are linked together. Graphs model networks-systems of “things” connected together in some way. For example, the Internet is a physical network of computers connected together by data connections; the Web is a logical network of web pages connected by hyperlinks; and human societies are networks of people connected together by various social relationships. In the language of mathematics, we say each of the “things” in a network is a node and they are connected by “edges.”</p>
<p>It turns out that you can think of much of the world, both physical and virtual, as a graph. As a mathematical construct, graphs have been around since <a
href="http://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg">Leonhard Euler tried to figure out the best way to get around Konigsberg in 1735</a>. Since then, graph theory has been embraced by a wide array of disciplines including sociology, economics, physics, computer science, biology, and statistics. An excellent resource for understanding how graphs map onto real world systems is the <a href="http://www.santafe.edu/media/workingpapers/90-004.pdf">“Rosetta Stone for Connectionism,”</a> which maps various real world systems onto graph concepts. Graphs really are everywhere.</p>
<p>While graphs are prevalent in many fields, the tools for working with graphs, especially large graphs, are still in their infancy. As graph technologies mature it should become easier to model many different problems, and easier to implement solutions. However, we are still figuring out the best ways to store, query, and compute on graphs. Right now, people use different data structures and technologies for different types of graphs and different use cases. Eventually, we need to figure out how to hide that complexity and let people treat graph data as graphs without thinking about what the right tools are for manipulating that data. Marko Rodriguez, a leading graph technologist, has a great summary of several different types of graphs and graph technologies in his <a href="http://markorodriguez.com/2013/01/09/on-graph-computing/">recent blog post</a>. Lab41 is actively using and working with many of the technologies that Marko describes, including the graph database Titan, which we load tested as noted in <a href="https://github.com/tinkerpop/blueprints/wiki/Property-Graph-Model">our previous blog entry</a>.</p>
<p><img class="center" src="http://lab41.github.io/images/2013-06-12-i-see-graphs/java_graph.png" title="Graph of Java standard class library generated with Gephi 0.8.2" ></p>
<h3 id="graphs-as-matrices">Graphs as Matrices</h3>
<p>The earliest tools for working with graphs were tools for manipulating matrices. In mathematics, graphs are frequently expressed as an <a href="http://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a>. In an adjacency matrix each row/column represents a node, and each entry in the matrix represents the presence of an edge between two nodes. The cool thing about the matrix form of a graph is that once you think of a graph as a matrix, you can apply concepts and methods from linear algebra to your graph analysis. Many common graph metrics and algorithms can easily be expressed in terms of standard matrix operations.</p>
<p><img class="center" src="http://lab41.github.io/images/2013-06-12-i-see-graphs/matrix.png" title="A graph represented as an ajacency matrix" ></p>
<p>The cool thing about the matrix form of a graph is that once you think of a graph as a matrix, you can apply concepts and methods from linear algebra to your graph analysis. Many common graph metrics and algorithms can easily be expressed in terms of standard matrix operations.</p>
<p>While an adjacency matrix is a mathematical abstraction, it’s also a data structure. In this post we are talking primarily about a matrix as a contiguous block of memory. In most programing languages this is an array of arrays:</p>
<div class="bogus-wrapper">
<notextile>
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//Java
</span><span class='line'>int[][] matrix = {{1,2,3}, {1,2,3}}</span></code></pre></td></tr></table></div></figure>
</notextile>
</div>
<p>From an engineering perspective, there are a number of advantages to storing a graph as a matrix, if the matrix representation of the graph fits in memory:</p>
<ul>
<li>A number of common operations can be performed on matrices quite quickly in comparison to how long they would take on other data structures. (Table 1)
<div class="table-lab41">
<pre><code>&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt; Operation &lt;/td&gt;
    &lt;td&gt; Adjacency Matrix &lt;/td&gt;
    &lt;td&gt; Adjacency List &lt;/td&gt;
    &lt;td&gt; Adjacency Tree &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt; Insert &lt;/td&gt;
    &lt;td&gt; O(1) &lt;/td&gt;
    &lt;td&gt; O(1) &lt;/td&gt;
    &lt;td&gt; O(log(m/n))&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Delete&lt;/td&gt;
    &lt;td&gt;O(1)&lt;/td&gt;
    &lt;td&gt;O(m/n)&lt;/td&gt;
    &lt;td&gt;O(log(m/n))&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Find&lt;/td&gt;
    &lt;td&gt;O(1)&lt;/td&gt;
    &lt;td&gt;O(m/n)&lt;/td&gt;
    &lt;td&gt;O(log(m/n))&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Enumerate&lt;/td&gt;
    &lt;td&gt;O(n)&lt;/td&gt;
    &lt;td&gt;O(m/n)&lt;/td&gt;
    &lt;td&gt;O(m/n)&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;</code></pre>
</div></li>
<li>Nearly all programming languages have highly optimized libraries for storing and working with matrices. For example: Python- <a href="http://www.numpy.org/">NumPy</a>; Java-<a href="http://math.nist.gov/javanumerics/jama/">JAMA</a>, <a href="http://acs.lbl.gov/software/colt/">Colt</a>,<a href="https://code.google.com/p/java-matrix-benchmark/">etc..</a>; C++: <a href="http://arma.sourceforge.net/">Armadillo</a>, <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a>, etc.</li>
</ul>
However, matrices are a relatively limited representation of a graph. There are a number of operations that they perform very poorly, and they can be very memory inefficient:
<ul>
<li>
Adjacency matrices only allow you to capture the structure of the graph. They don’t give you a chance to associate information with each node – in other words, you can’t represent <a href="https://github.com/tinkerpop/blueprints/wiki/Property-Graph-Model">property graphs</a> with matrices. For example, if you were modeling the world wide web you might want to store the url, title, and text of each web page in the network, and you can’t do that with an adjacency matrix. Now, if you’re persnickety, you might argue that it’s possible to store edge properties in an adjacency matrix if you think of each entry in the matrix as something other than a numeric value.
</li>

<li> 
You can’t model heterogeneous graphs consisting of different types of entities using an adjacency matrix. For example, if you are trying to implement collaborative filtering, you may want to model a network of users to products. Again, if you are persnickety, you might propose various ways of simulating heterogeneous graphs using a single matrix. However, all of those methods will require you to use information not actually stored in the matrix to differentiate nodes of one type from nodes of another.
</li>

<li> 
Adjacency matrices are really slow at some critical matrix operations, such as running through the neighbors of a particular node on a sparse matrix (O(n)– where n is the number of nodes in the graph).
</li>

<li> <p>
Perhaps worst of all, adjacency matrices are very memory inefficient taking O(n^2) memory. An adjacency matrix has an entry for each possible edge, which means each possible edge is using memory even if it does not exist. This is primarily a problem when working with sparse networks – networks where many of the edges in the network don’t exist. Unfortunately, most real world networks are sparse. For example, if you consider the graph of all people on earth, it is a sparse network because each person knows only a relatively small number of other people. Most of the possible relationships that could exist between people don’t exist. In some ways that is both an engineering problem and an existential problem.
</p>
<p>
To put the memory inefficiency of adjacency matrices into perspective, if each edge in a matrix is stored as a 32 bit integer then the memory requirement for a graph can be calculated by following equation:
</p>
<span class="math">\[\text{memory in gb} = \frac{(\text{number of nodes})^2 * 8}{(1024^3)}\]</span>
<p>
Thus a graph of 50,000 nodes would take about 10GB of memory, which means you can’t store and manipulate large graphs like the graph of the Web, which is estimated to have 4.7 billion nodes, on most desktop computers.
</p>

</li>

<p>While technologies for dealing with small matrices – matrices that can fit in memory – are well developed, technologies for dealing with large matrices in a distributed manner are just emerging. One approach to dealing with extremely large matrices is to use some type of super computer, which has a lot of memory. Another approach is to swap portions of the matrix into and out of memory; there are algorithms that can do this relatively efficiently based on the unique properties of a matrix. A new, and extremely interesting, approach is to distribute a matrix computation across the memory of multiple computers. I think the <a href="http://www.cs.cmu.edu/~pegasus/">Pegasus project</a> is a particularly interesting example of the distributed matrix computation approach.</p>
<h3 id="next-steps">Next Steps</h3>
<p>If you’re interested in learning more about networks and adjacency matrices, I would highly recommend taking a look at M.E.J. Newman’s <a href="http://www.amazon.com/Networks-An-Introduction-Mark-Newman/dp/0199206651/ref=sr_1_1_bnp_1_har?ie=UTF8&amp;qid=1370024687&amp;sr=8-1&amp;keywords=Newman+Networks">Networks</a>. He has an excellent discussion of the adjacency matrix as a mathematical concept in Chapter 6, and discussion of an adjacency matrix as a data structure in Chapter 9.</p>
<p>Also, keep an eye on this blog. I plan to address other data structures for storing graph data, and when they may (or may not be) appropriate in a future post.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zero to Large]]></title>
    <link href="http://lab41.github.io/blog/2013/05/02/zero-to-large/"/>
    <updated>2013-05-02T15:22:00+00:00</updated>
    <id>http://lab41.github.io/blog/2013/05/02/zero-to-large</id>
    <content type="html"><![CDATA[<blockquote>
<p>This first post is intended to introduce the type of work we’ve started at Lab41, which is a unique partnership In-Q-Tel has started with Academia, Industry, and the U.S. Intelligence Community. We’re excited about this venture and look forward to sharing our progress towards collaboratively addressing big data challenges with new technologies.</p>
</blockquote>
<p>Designing scalable systems for the real world requires careful consideration of data – namely, Big Data’s volume, variety, and velocity – to ensure the right pieces are engineered and valuable resources don’t miss the gotchas or edge cases that lead to insight. Basically, when tinkering with different architectures in the Big Data arena, having good data to test against is paramount. One of our projects involves assessing various architectures for working with large-scale graphs, including how to incorporate data that tests the limits of storage, computation, and analytic workflows.</p>
<p>As you might expect, we wanted to use real world data when designing our real-world system. However, getting real data that mimics production data is difficult and time consuming. Oftentimes data only tells a story for that specific dataset, leading a developer to miss the more comprehensive view of the system’s strengths and weaknesses. For those reasons, we developed a method that generates large graphs with the “right” qualities of a system that can scale to one billion nodes.</p>
<h3 id="initial-requirements">Initial Requirements</h3>
<p>Before comparing the leading projects for scaling graphs, we needed a good baseline for assessing the data requirements of the overall system. It quickly became apparent that current offerings such as <a href="http://gephi.org/">Gephi</a>, <a href="http://nwb.cns.iu.edu/">Network WorkBench</a>, <a href="http://networkx.github.com/">NetworkX</a>, and <a href="https://github.com/tinkerpop/furnace">Furnace</a>, all do a good job of following particular distributions and structural constraints. However, most of them are unable to generate graphs at large scale <strong>and</strong> produce the correct format <strong>and</strong> build to completion <strong>and</strong> finish in a reasonable amount of time.</p>
<p>The evaluation and assessment of graph data generators led us down the path of writing a fairly-straightforward script. The <a href="http://lab41.github.io/graph-generators/">code</a> is very young in its development – and has room for a lot of improvement – but it has proven simple to use, moderately good at generating graphs large enough to test claims, and flexible enough to vary characteristics such as directed-ness, out-degrees, and of course numbers of edges, nodes, and attributes. We made sure to add a twist of randomness to avoid creating identical graphs.</p>
<p>The script takes command-line switches to configure the following graph characteristics:</p>
<ul>
<li>Number of nodes</li>
<li>Degree of nodes</li>
<li>In-degree of nodes</li>
<li>Out-degree of nodes</li>
<li>Number of node attributes</li>
<li>Number of edge attributes</li>
<li>Directed-ness</li>
<li>Output type (GraphML and GraphSON so far)</li>
</ul>
<h4 id="practical-considerations">Practical Considerations</h4>
<p>With our script in hand, we moved on to begin the requisite performance testing, but we first discovered an important consideration for anyone wishing to release our script into the wild.</p>
<p><strong>The most important “practical” consideration proved to be enforcement (or not) of strict parameters, which forces the script to scan and verify characteristics of all nodes.</strong> By enforcing strict parameters, we mean that:</p>
<ul>
<li>each node must guarantee
<ul>
<li>that it has no more and no less edges attached to it
<ul>
<li>from both an in and out degree context.</li>
</ul></li>
</ul></li>
</ul>
<p>In order to guarantee this 100% of the time, each time an edge is added, all preexisting edges must be checked to make sure that the chosen random vertices chosen do not go outside the imposed limits. To put it in perspective, the script initially enforces strict parameters, which – as you can probably guess by now – simply become untenable for quickly producing large graph data sets. As the below chart shows, we are able to generate a graph of 100 Million nodes in roughly the same amount of time it took to generate a graph of only 100,000 nodes using strict parameter enforcement:</p>
<p>Click to enlarge: <a class="fancybox-effects-a"  href=/images/post_1/image_1.png><img src="http://lab41.github.io/images/post_1/image_1.png" title="Comparing Graph Generation Time With and Without Strict Parameters" ></a></p>
<p>Since disabling strict enforcement led to a graph three orders of magnitude larger in the same amount of time, you might be asking how the absence of that check – the degree of edges/node – affected the number of edges. Below we show that the difference of edges between checking and not checking is negligible:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_2.png><img src="http://lab41.github.io/images/post_1/image_2.png" title="Difference of \# of Edges Created" ></a></p>
<p>Since we are generating these graphs, it seems reasonable to bend the requirements slightly to treat the minima and maxima simply as guidelines that some nodes may not conform to. While there is still room for improvement, such as leveraging more than a single CPU core, the results are reasonable enough to use.</p>
<h3 id="generating-the-baseline">Generating the Baseline</h3>
<p><strong>The most important point is that seemingly “simple” parameter changes – which represent actual differences in real-world networks – make huge differences to the resulting network and therefore our system design.</strong> We generated three different classes of graphs from a baseline of graph data sets to determine how varying parameters influences such important characteristics as: time to generate, number of edges created, storage footprint, number of node and edge attributes, and average degree of nodes.</p>
<p>Each graph was generated with an increasing value of nodes, while all other settings were static between generations, per graph type. Graph types A, B, and C – described below – will be used in the next couple of charts:</p>
<pre><code>Graph Type: A B C
Magnitude: 1K - 1B 1K - 100M 1K - 100K
Format: graphml* graphml* graphml*
Directed: No No No
Minimum Degree: 1 1 1
Maximum Degree: 10 10 Same number as nodes
Minimum Node Attributes: 2 50 2
Maximum Node Attributes: 2 100 2
Minimum Edge Attributes: 0 5 0
Maximum Edge Attributes: 0 25 0</code></pre>
<blockquote>
<p>GraphML (<a href="http://graphml.graphdrawing.org/">http://graphml.graphdrawing.org/</a>) is a convenient XML format that describes nodes in terms of names and types with labeled edges between nodes.</p>
</blockquote>
<h4 id="number-of-nodes">Number of Nodes</h4>
<p>The first chart illustrates how the number of nodes greatly influences all other characteristics. While Type A generated one billion nodes in approximately 24 hours, the same timeframe yielded graphs of Type B with only 10 Million nodes and Type C with a scant one million nodes:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_3.png><img src="http://lab41.github.io/images/post_1/image_3.png" title="Time to generate graphs with different characteristics" ></a></p>
<h4 id="edges">Edges</h4>
<p>As this is just a first cut, restricting the number of nodes on the graph types seems acceptable for now. The following illustrates that limiting Type C to only 100,000 nodes still produces almost the same number of edges as a one billion node version of graph A:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_4.png><img src="http://lab41.github.io/images/post_1/image_4.png" title="Edges created generating graphs with different characteristics" ></a></p>
<h4 id="storage-footprint">Storage Footprint</h4>
<p>The following chart shows that a one billion node graph of Type B would require approximately 10TB of storage space, while Type C would require 300 Petabytes (!) to reach one billion nodes:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_5.png><img src="http://lab41.github.io/images/post_1/image_5.png" title="Storage footprint of generating graphs with different characteristics" ></a></p>
<p>Sadly, I couldn’t justify buying 300 <a href="http://www.aberdeeninc.com/abcatg/petarack.htm">Petaracks</a> just to generate the world’s most unrealistic graph. Not to mention it would have taken approximately 20,000 <strong>years</strong> to generate, but that’s beside the point.</p>
<h4 id="attributes">Attributes</h4>
<p>When looking at attribute differences, Type B creates about 30-40 times more attributes than the fairly-similar Types A and C:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_6.png><img src="http://lab41.github.io/images/post_1/image_6.png" title="Node attributes created generating graphs with different characteristics" ></a></p>
<h4 id="node-degree">Node Degree</h4>
<p>Finally, this last chart shows how the degree of each node for Type C grows exponentially with the number of nodes, whereas the average degree for the other two graph types remain static:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_7.png><img src="http://lab41.github.io/images/post_1/image_7.png" title="Average degree of nodes generating graphs with different characteristics" ></a></p>
<h3 id="loading-the-data">Loading the Data</h3>
<p>Now that we generated our various graph datasets, we need to load them into a distributed graph data store. For a variety of reasons, we decided to use <a href="http://thinkaurelius.github.com/titan/">Titan</a> with an <a href="http://hbase.apache.org/">HBase</a> backend.</p>
<p>One of the nice things about Titan is that its <a href="https://github.com/tinkerpop/gremlin/wiki">Gremlin console shell</a> enables graph interaction, traversals, and calculations. It also has functions for loading a graph file into the graph data store, which in this case is HBase on top of HDFS. Unfortunately, Gremlin through Titan does not leverage the <a href="http://www.kchodorow.com/blog/2010/03/15/mapreduce-the-fanfiction/">awesomeness of MapReduce</a> that generally goes hand-in-hand with HBase and its Hadoop counterparts. So running the import in parallel is currently impossible. In terms of data formats, Gremlin on Titan can load GraphML; however, the current ID scheme prevents federation of GraphML across multiple machines (or even multiple cores). So as you can see from the chart below, the load times are un-spectacular:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_8.png><img src="http://lab41.github.io/images/post_1/image_8.png" title="Load time of graph types using Gremlin with Titan" ></a></p>
<blockquote>
<p>*Note: ‘Minimal Degree and Attributes’ corresponds to ‘Graph A’ from above, similarly ‘Heavy Attributes’ and ‘Heavy Degree’ correspond to ‘Graph B’ and ‘Graph C’, respectively.</p>
</blockquote>
<p>Fret not! <a href="http://thinkaurelius.github.com/faunus/">Faunus</a> to the rescue! Faunus uses a Gremlin shell, which is similar to Titan’s and one we can use for importing a slightly different data format to gain benefits via MapReduce. The next chart shows the benefits of moving from GraphML to loading the GraphSON* format using a MapReduce job:</p>
<p><a class="fancybox-effects-a"  href=/images/post_1/image_9.png><img src="http://lab41.github.io/images/post_1/image_9.png" title="Load time comparison using Gremlin with Titan versus Gremlin with Faunus" ></a></p>
<blockquote>
<p>*Note: <a href="https://github.com/thinkaurelius/faunus/wiki/GraphSON-Format">GraphSON</a>, which is slightly modified from <a href="https://github.com/tinkerpop/blueprints/wiki/GraphSON-Reader-and-Writer-Library">traditional GraphSON</a>, is a one-record-per-line JSON-style format that describes nodes in terms of names, types, and labeled edges.</p>
</blockquote>
<h3 id="conclusion">Conclusion</h3>
<p><strong>Our <a href="https://github.com/Lab41/graph-generators">Graph-generation Code</a> allows us to generate one-billion node graphs with varying characteristics such as directed-ness, number of edges and nodes, and node degrees. Just as important, we determined how the different characteristics affect real-world considerations such as loading time and storage footprint, also finding an early optimization through MapReduce parallel processing.</strong> As we move to the next phase of designing around the data, we anticipate shortly being able to improve at least a couple orders of magnitude through fairly straightforward tweaks such as parallelizing load computations across a cluster. Of course this is only the first step, and there is a long exciting road ahead of us.</p>
<h4 id="standard-disclaimer">Standard Disclaimer</h4>
<p>As is usually the case, it should be noted that this is not necessarily representative of the technology’s overall performance characteristics, but rather our experience within a specific environment. We used the following environment for our tests:</p>
<p><strong>Environment:</strong></p>
<pre><code>GRAPH GENERATION:

    Run on a MacBook Air 2GHz Intel Core i7, 8GB 1600 MHz DDR3

    Python 2.7.2 (default, Jun 20 2012, 16:23:33)
    [GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin

LOADING GRAPHS:

    Gremlin with Titan:
        Virtual Machine, Ubuntu 12.04 LTS, 4 core processor, 8GB RAM
        Started with the default Java options for Gremlin of:
            JAVA_OPTIONS=&quot;-Xms32m -Xmx512m
        Then bumped that up as the graph file got larger to:
            JAVA_OPTIONS=&quot;-Xms256m -Xmx4096m

    Gremlin with Faunus:
        Virtual Machine, Ubuntu 12.04 LTS, 2 core processor, 4GB RAM

    Hadoop/HBase Cluster:
        12 node cluster - 8 core processors, 64GB RAM, CDH4.2, heap set to 4GB, HBase 0.94.2</code></pre>]]></content>
  </entry>
  
</feed>
